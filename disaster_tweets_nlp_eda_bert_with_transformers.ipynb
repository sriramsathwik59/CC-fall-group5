{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ko0-buFE9vr"
      },
      "source": [
        "# DISASTER TWEET ANALYSIS USING NLP IN AWS ENVIRONMENT\n",
        "    \n",
        "\n",
        "## Summary\n",
        "\n",
        "**The scope of this project is to develop and implement a scalable, end-to-end Natural Language Processing (NLP) pipeline using AWS cloud services. The project will involve the ingestion of csv containting tweets into Amazon S3 for centralized storage, followed by data cleaning and preprocessing using AWS Glue for automated ETL workflows. Advanced NLP techniques, such as sentiment analysis will be executed using Amazon SageMaker to build and train machine learning models. The processed data and insights will be visualized using Amazon QuickSight.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8U5O7_WE9vt"
      },
      "source": [
        "<a id=\"Importing_Neccesary_Packages\"></a>\n",
        "\n",
        "## Importing Neccesary Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "-iQ4gUDP6r8E",
        "outputId": "caff2324-61f9-4d53-8eab-f999059b0259"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "c:\\Users\\lenovo\\Desktop\\cloud\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from collections import Counter, defaultdict\n",
        "from PIL import Image\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# For customizing our plots.\n",
        "\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Loading pytorch packages.\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Setting some options for general use.\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set(font_scale=1.5)\n",
        "pd.options.display.max_columns = 250\n",
        "pd.options.display.max_rows = 250\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "#Setting seeds for consistent results.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBCi0gv2E9vu"
      },
      "source": [
        "<a id=\"Loading_the_Data\"></a>\n",
        "## Loading the Data\n",
        "\n",
        "### Installing boto3 and connecting to s3 buecket using acess key and secret key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSohgJQgE9vv",
        "outputId": "645d9a7d-ad06-4e0a-ef7f-e95780e2a04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First file preview:\n",
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   target                                         text_clean  \n",
            "0       1  Our Deeds are the Reason of this earthquake Ma...  \n",
            "1       1              Forest fire near La Ronge Sask Canada  \n",
            "2       1  All residents asked to shelter in place are be...  \n",
            "3       1  13000 people receive wildfires evacuation orde...  \n",
            "4       1  Just got sent this photo from Ruby Alaska as s...  \n",
            "\n",
            "Second file preview:\n",
            "   id keyword location                                               text\n",
            "0   0     NaN      NaN                 Just happened a terrible car crash\n",
            "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
            "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
            "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
            "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "\n",
        "# Replace these with your actual credentials and bucket details\n",
        "ACCESS_KEY = \"AKIA23WHTXI77U57G4PZ\"\n",
        "SECRET_KEY = \"W6431oA8fON0vVLHVnFJGyx+RG/woAIBVdi5l5vd\"\n",
        "BUCKET_NAME = \"cloudfinalprojectgroup5\"\n",
        "\n",
        "# File keys for the two files\n",
        "FILE_KEY_1 = \"Cleaned_data_final/part-00000-fe496805-1ed6-4b59-8920-593fc93d2757-c000.csv\"\n",
        "FILE_KEY_2 = \"test.csv\"  # Replace with the actual second file key\n",
        "\n",
        "# Create an S3 client\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=ACCESS_KEY,\n",
        "    aws_secret_access_key=SECRET_KEY\n",
        ")\n",
        "\n",
        "# Load the first file directly into a Pandas DataFrame\n",
        "response_1 = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY_1)\n",
        "data1 = pd.read_csv(response_1['Body'])\n",
        "print(\"First file preview:\")\n",
        "print(data1.head())\n",
        "\n",
        "# Load the second file directly into a Pandas DataFrame\n",
        "response_2 = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY_2)\n",
        "data2 = pd.read_csv(response_2['Body'])\n",
        "print(\"\\nSecond file preview:\")\n",
        "print(data2.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LmIYxux6r8O"
      },
      "outputs": [],
      "source": [
        "# Loading the train and test data for visualization & exploration.\n",
        "\n",
        "trainv = data1\n",
        "testv = data2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrmSzzdyE9vv",
        "outputId": "4db4a6d1-3ada-4867-e4c8-5f600b81e211"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3329</th>\n",
              "      <td>4769</td>\n",
              "      <td>evacuated</td>\n",
              "      <td>Portland, Ore.</td>\n",
              "      <td>New evacuation ordered for 25 homes in danger ...</td>\n",
              "      <td>1</td>\n",
              "      <td>New evacuation ordered for 25 homes in danger ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>1251</td>\n",
              "      <td>blood</td>\n",
              "      <td>canberra</td>\n",
              "      <td>another day another excellent @_dangerousbeans...</td>\n",
              "      <td>0</td>\n",
              "      <td>another day another excellent dangerousbeans p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3992</th>\n",
              "      <td>5671</td>\n",
              "      <td>floods</td>\n",
              "      <td>canada</td>\n",
              "      <td>@Cyberdemon531 i hope that mountain dew erodes...</td>\n",
              "      <td>1</td>\n",
              "      <td>Cyberdemon531 i hope that mountain dew erodes ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6971</th>\n",
              "      <td>10000</td>\n",
              "      <td>tsunami</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I liked a @YouTube video http://t.co/0h7OUa1pn...</td>\n",
              "      <td>0</td>\n",
              "      <td>I liked a YouTube video  Call of Duty Ghosts  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2517</th>\n",
              "      <td>3617</td>\n",
              "      <td>desolation</td>\n",
              "      <td>St. Louis Mo.</td>\n",
              "      <td>Wow! I just won this for free The Hobbit: Deso...</td>\n",
              "      <td>0</td>\n",
              "      <td>Wow I just won this for free The Hobbit Desola...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id     keyword        location  \\\n",
              "3329   4769   evacuated  Portland, Ore.   \n",
              "866    1251       blood        canberra   \n",
              "3992   5671      floods          canada   \n",
              "6971  10000     tsunami             NaN   \n",
              "2517   3617  desolation   St. Louis Mo.   \n",
              "\n",
              "                                                   text  target  \\\n",
              "3329  New evacuation ordered for 25 homes in danger ...       1   \n",
              "866   another day another excellent @_dangerousbeans...       0   \n",
              "3992  @Cyberdemon531 i hope that mountain dew erodes...       1   \n",
              "6971  I liked a @YouTube video http://t.co/0h7OUa1pn...       0   \n",
              "2517  Wow! I just won this for free The Hobbit: Deso...       0   \n",
              "\n",
              "                                             text_clean  \n",
              "3329  New evacuation ordered for 25 homes in danger ...  \n",
              "866   another day another excellent dangerousbeans p...  \n",
              "3992  Cyberdemon531 i hope that mountain dew erodes ...  \n",
              "6971  I liked a YouTube video  Call of Duty Ghosts  ...  \n",
              "2517  Wow I just won this for free The Hobbit Desola...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Taking general look at the both datasets.\n",
        "\n",
        "display(trainv.sample(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KLD2T7uE9vv",
        "outputId": "7fcabd04-8764-446e-d95f-3d1156d20179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7613, 6)\n",
            "(3263, 4)\n"
          ]
        }
      ],
      "source": [
        "# Checking observation and feature numbers for train and test data.\n",
        "\n",
        "print(trainv.shape)\n",
        "print(testv.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs8YIxEj6r8Z",
        "outputId": "9055f4b7-223a-4b8f-a1e1-44ff455bff51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to shelter in place are be...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target                                         text_clean  \\\n",
              "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
              "1       1              Forest fire near La Ronge Sask Canada   \n",
              "2       1  All residents asked to shelter in place are be...   \n",
              "3       1  13000 people receive wildfires evacuation orde...   \n",
              "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
              "\n",
              "                                           tokenized  \n",
              "0  [Our, Deeds, are, the, Reason, of, this, earth...  \n",
              "1      [Forest, fire, near, La, Ronge, Sask, Canada]  \n",
              "2  [All, residents, asked, to, shelter, in, place...  \n",
              "3  [13000, people, receive, wildfires, evacuation...  \n",
              "4  [Just, got, sent, this, photo, from, Ruby, Ala...  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenizing the tweet base texts.\n",
        "nltk.download('punkt_tab')\n",
        "trainv['tokenized'] = trainv['text_clean'].apply(word_tokenize)\n",
        "\n",
        "trainv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "S38f8KU_6r8g",
        "outputId": "6b0bf982-79b9-4366-9d1b-7fbcf1435851"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to shelter in place are be...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target                                         text_clean  \\\n",
              "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
              "1       1              Forest fire near La Ronge Sask Canada   \n",
              "2       1  All residents asked to shelter in place are be...   \n",
              "3       1  13000 people receive wildfires evacuation orde...   \n",
              "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
              "\n",
              "                                           tokenized  \\\n",
              "0  [Our, Deeds, are, the, Reason, of, this, earth...   \n",
              "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
              "2  [All, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
              "\n",
              "                                               lower  \n",
              "0  [our, deeds, are, the, reason, of, this, earth...  \n",
              "1      [forest, fire, near, la, ronge, sask, canada]  \n",
              "2  [all, residents, asked, to, shelter, in, place...  \n",
              "3  [13000, people, receive, wildfires, evacuation...  \n",
              "4  [just, got, sent, this, photo, from, ruby, ala...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lower casing clean text.\n",
        "\n",
        "trainv['lower'] = trainv['tokenized'].apply(\n",
        "    lambda x: [word.lower() for word in x])\n",
        "\n",
        "trainv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "uYHThET16r8k",
        "outputId": "80a487df-b0d2-4cdc-9310-e6784ec98fde"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>stopwords_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to shelter in place are be...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target                                         text_clean  \\\n",
              "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
              "1       1              Forest fire near La Ronge Sask Canada   \n",
              "2       1  All residents asked to shelter in place are be...   \n",
              "3       1  13000 people receive wildfires evacuation orde...   \n",
              "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
              "\n",
              "                                           tokenized  \\\n",
              "0  [Our, Deeds, are, the, Reason, of, this, earth...   \n",
              "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
              "2  [All, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
              "\n",
              "                                               lower  \\\n",
              "0  [our, deeds, are, the, reason, of, this, earth...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [all, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
              "\n",
              "                                   stopwords_removed  \n",
              "0  [deeds, reason, earthquake, may, allah, forgiv...  \n",
              "1      [forest, fire, near, la, ronge, sask, canada]  \n",
              "2  [residents, asked, shelter, place, notified, o...  \n",
              "3  [13000, people, receive, wildfires, evacuation...  \n",
              "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing stopwords.\n",
        "\n",
        "trainv['stopwords_removed'] = trainv['lower'].apply(\n",
        "    lambda x: [word for word in x if word not in stop])\n",
        "\n",
        "trainv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "y0YxhSvd6r8p",
        "outputId": "16bf87db-b856-44f3-9a78-fe23574d0695"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>stopwords_removed</th>\n",
              "      <th>pos_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
              "      <td>[(deeds, NNS), (reason, NN), (earthquake, NN),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[(forest, JJS), (fire, NN), (near, IN), (la, J...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to shelter in place are be...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
              "      <td>[(residents, NNS), (asked, VBD), (shelter, JJ)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[(13000, CD), (people, NNS), (receive, JJ), (w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
              "      <td>[(got, VBD), (sent, JJ), (photo, NN), (ruby, N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target                                         text_clean  \\\n",
              "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
              "1       1              Forest fire near La Ronge Sask Canada   \n",
              "2       1  All residents asked to shelter in place are be...   \n",
              "3       1  13000 people receive wildfires evacuation orde...   \n",
              "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
              "\n",
              "                                           tokenized  \\\n",
              "0  [Our, Deeds, are, the, Reason, of, this, earth...   \n",
              "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
              "2  [All, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
              "\n",
              "                                               lower  \\\n",
              "0  [our, deeds, are, the, reason, of, this, earth...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [all, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
              "\n",
              "                                   stopwords_removed  \\\n",
              "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [residents, asked, shelter, place, notified, o...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
              "\n",
              "                                            pos_tags  \n",
              "0  [(deeds, NNS), (reason, NN), (earthquake, NN),...  \n",
              "1  [(forest, JJS), (fire, NN), (near, IN), (la, J...  \n",
              "2  [(residents, NNS), (asked, VBD), (shelter, JJ)...  \n",
              "3  [(13000, CD), (people, NNS), (receive, JJ), (w...  \n",
              "4  [(got, VBD), (sent, JJ), (photo, NN), (ruby, N...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Applying part of speech tags.\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "trainv['pos_tags'] = trainv['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
        "\n",
        "trainv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oe8KxNR6r8u",
        "outputId": "46abc640-c0e1-4c91-ed4a-c55c558d9b3d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>stopwords_removed</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
              "      <td>[(deeds, NNS), (reason, NN), (earthquake, NN),...</td>\n",
              "      <td>[(deeds, n), (reason, n), (earthquake, n), (ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[(forest, JJS), (fire, NN), (near, IN), (la, J...</td>\n",
              "      <td>[(forest, a), (fire, n), (near, n), (la, a), (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to shelter in place are be...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
              "      <td>[(residents, NNS), (asked, VBD), (shelter, JJ)...</td>\n",
              "      <td>[(residents, n), (asked, v), (shelter, a), (pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[(13000, CD), (people, NNS), (receive, JJ), (w...</td>\n",
              "      <td>[(13000, n), (people, n), (receive, a), (wildf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
              "      <td>[(got, VBD), (sent, JJ), (photo, NN), (ruby, N...</td>\n",
              "      <td>[(got, v), (sent, a), (photo, n), (ruby, n), (...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target                                         text_clean  \\\n",
              "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
              "1       1              Forest fire near La Ronge Sask Canada   \n",
              "2       1  All residents asked to shelter in place are be...   \n",
              "3       1  13000 people receive wildfires evacuation orde...   \n",
              "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
              "\n",
              "                                           tokenized  \\\n",
              "0  [Our, Deeds, are, the, Reason, of, this, earth...   \n",
              "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
              "2  [All, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
              "\n",
              "                                               lower  \\\n",
              "0  [our, deeds, are, the, reason, of, this, earth...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [all, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
              "\n",
              "                                   stopwords_removed  \\\n",
              "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [residents, asked, shelter, place, notified, o...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
              "\n",
              "                                            pos_tags  \\\n",
              "0  [(deeds, NNS), (reason, NN), (earthquake, NN),...   \n",
              "1  [(forest, JJS), (fire, NN), (near, IN), (la, J...   \n",
              "2  [(residents, NNS), (asked, VBD), (shelter, JJ)...   \n",
              "3  [(13000, CD), (people, NNS), (receive, JJ), (w...   \n",
              "4  [(got, VBD), (sent, JJ), (photo, NN), (ruby, N...   \n",
              "\n",
              "                                         wordnet_pos  \n",
              "0  [(deeds, n), (reason, n), (earthquake, n), (ma...  \n",
              "1  [(forest, a), (fire, n), (near, n), (la, a), (...  \n",
              "2  [(residents, n), (asked, v), (shelter, a), (pl...  \n",
              "3  [(13000, n), (people, n), (receive, a), (wildf...  \n",
              "4  [(got, v), (sent, a), (photo, n), (ruby, n), (...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting part of speeches to wordnet format.\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "\n",
        "trainv['wordnet_pos'] = trainv['pos_tags'].apply(\n",
        "    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
        "\n",
        "trainv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBkJo3E-6r8x",
        "outputId": "5c907893-f035-4600-c615-110facf6befa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>stopwords_removed</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "      <th>lemmatized</th>\n",
              "      <th>lemma_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
              "      <td>[(deeds, NNS), (reason, NN), (earthquake, NN),...</td>\n",
              "      <td>[(deeds, n), (reason, n), (earthquake, n), (ma...</td>\n",
              "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
              "      <td>deed reason earthquake may allah forgive u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[(forest, JJS), (fire, NN), (near, IN), (la, J...</td>\n",
              "      <td>[(forest, a), (fire, n), (near, n), (la, a), (...</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to shelter in place are be...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
              "      <td>[(residents, NNS), (asked, VBD), (shelter, JJ)...</td>\n",
              "      <td>[(residents, n), (asked, v), (shelter, a), (pl...</td>\n",
              "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
              "      <td>resident ask shelter place notify officer evac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
              "      <td>[(13000, CD), (people, NNS), (receive, JJ), (w...</td>\n",
              "      <td>[(13000, n), (people, n), (receive, a), (wildf...</td>\n",
              "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
              "      <td>13000 people receive wildfire evacuation order...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
              "      <td>[(got, VBD), (sent, JJ), (photo, NN), (ruby, N...</td>\n",
              "      <td>[(got, v), (sent, a), (photo, n), (ruby, n), (...</td>\n",
              "      <td>[get, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
              "      <td>get sent photo ruby alaska smoke wildfires pou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target                                         text_clean  \\\n",
              "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
              "1       1              Forest fire near La Ronge Sask Canada   \n",
              "2       1  All residents asked to shelter in place are be...   \n",
              "3       1  13000 people receive wildfires evacuation orde...   \n",
              "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
              "\n",
              "                                           tokenized  \\\n",
              "0  [Our, Deeds, are, the, Reason, of, this, earth...   \n",
              "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
              "2  [All, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
              "\n",
              "                                               lower  \\\n",
              "0  [our, deeds, are, the, reason, of, this, earth...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [all, residents, asked, to, shelter, in, place...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
              "\n",
              "                                   stopwords_removed  \\\n",
              "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [residents, asked, shelter, place, notified, o...   \n",
              "3  [13000, people, receive, wildfires, evacuation...   \n",
              "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
              "\n",
              "                                            pos_tags  \\\n",
              "0  [(deeds, NNS), (reason, NN), (earthquake, NN),...   \n",
              "1  [(forest, JJS), (fire, NN), (near, IN), (la, J...   \n",
              "2  [(residents, NNS), (asked, VBD), (shelter, JJ)...   \n",
              "3  [(13000, CD), (people, NNS), (receive, JJ), (w...   \n",
              "4  [(got, VBD), (sent, JJ), (photo, NN), (ruby, N...   \n",
              "\n",
              "                                         wordnet_pos  \\\n",
              "0  [(deeds, n), (reason, n), (earthquake, n), (ma...   \n",
              "1  [(forest, a), (fire, n), (near, n), (la, a), (...   \n",
              "2  [(residents, n), (asked, v), (shelter, a), (pl...   \n",
              "3  [(13000, n), (people, n), (receive, a), (wildf...   \n",
              "4  [(got, v), (sent, a), (photo, n), (ruby, n), (...   \n",
              "\n",
              "                                          lemmatized  \\\n",
              "0  [deed, reason, earthquake, may, allah, forgive...   \n",
              "1      [forest, fire, near, la, ronge, sask, canada]   \n",
              "2  [resident, ask, shelter, place, notify, office...   \n",
              "3  [13000, people, receive, wildfire, evacuation,...   \n",
              "4  [get, sent, photo, ruby, alaska, smoke, wildfi...   \n",
              "\n",
              "                                           lemma_str  \n",
              "0         deed reason earthquake may allah forgive u  \n",
              "1              forest fire near la ronge sask canada  \n",
              "2  resident ask shelter place notify officer evac...  \n",
              "3  13000 people receive wildfire evacuation order...  \n",
              "4  get sent photo ruby alaska smoke wildfires pou...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Applying word lemmatizer.\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "trainv['lemmatized'] = trainv['wordnet_pos'].apply(\n",
        "    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
        "\n",
        "trainv['lemmatized'] = trainv['lemmatized'].apply(\n",
        "    lambda x: [word for word in x if word not in stop])\n",
        "\n",
        "trainv['lemma_str'] = [' '.join(map(str, l)) for l in trainv['lemmatized']]\n",
        "\n",
        "trainv.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXv9uLGYE9vy"
      },
      "source": [
        "<a id=\"Visualizing_the_Data\"></a>\n",
        "# Visualizing the Data\n",
        "\n",
        "### Finding relations between attributes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh9DIL5T6r9i"
      },
      "outputs": [],
      "source": [
        "lis = [\n",
        "    trainv[trainv['target'] == 0]['lemma_str'],\n",
        "    trainv[trainv['target'] == 1]['lemma_str']\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAAWpTVG6r9l",
        "outputId": "e6c015f2-0144-4d82-e1ca-fea52f43297f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No stopwords left in texts.\n"
          ]
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, j in zip(lis, axes):\n",
        "    try:\n",
        "        new = i.str.split()\n",
        "        new = new.values.tolist()\n",
        "        corpus = [word.lower() for i in new for word in i]\n",
        "        dic = defaultdict(int)\n",
        "        for word in corpus:\n",
        "            if word in stop:\n",
        "                dic[word] += 1\n",
        "\n",
        "        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "        x, y = zip(*top)\n",
        "        df = pd.DataFrame([x, y]).T\n",
        "        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n",
        "        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n",
        "        plt.tight_layout()\n",
        "    except:\n",
        "        plt.close()\n",
        "        print('No stopwords left in texts.')\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Waep9BG9E9vz"
      },
      "source": [
        "<a id=\"Most_Common_Words\"></a>\n",
        "## Most Common Words\n",
        "\n",
        "#### \"Analyzing word frequency, disaster tweets often include terms like fire, kill, or bomb, clearly indicating disasters. Non-disaster tweets, however, use more generic vocabulary.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "id": "k6ATaXOA6r9s",
        "outputId": "44fb4bc9-3ed6-4c80-aa4d-135fca3e50bf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Displaying most common words.\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, j in zip(lis, axes):\n",
        "\n",
        "    new = i.str.split()\n",
        "    new = new.values.tolist()\n",
        "    corpus = [word for i in new for word in i]\n",
        "\n",
        "    counter = Counter(corpus)\n",
        "    most = counter.most_common()\n",
        "    x, y = [], []\n",
        "    for word, count in most[:30]:\n",
        "        if (word not in stop):\n",
        "            x.append(word)\n",
        "            y.append(count)\n",
        "\n",
        "    sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
        "axes[0].set_title('Non Disaster Tweets')\n",
        "\n",
        "axes[1].set_title('Disaster Tweets')\n",
        "axes[0].set_xlabel('Count')\n",
        "axes[0].set_ylabel('Word')\n",
        "axes[1].set_xlabel('Count')\n",
        "axes[1].set_ylabel('Word')\n",
        "\n",
        "fig.suptitle('Most Common Unigrams', fontsize=24, va='baseline')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHo6CEJR6r9x",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def ngrams(n, title):\n",
        "    \"\"\"A Function to plot most common ngrams\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "    axes = axes.flatten()\n",
        "    for i, j in zip(lis, axes):\n",
        "\n",
        "        new = i.str.split()\n",
        "        new = new.values.tolist()\n",
        "        corpus = [word for i in new for word in i]\n",
        "\n",
        "        def _get_top_ngram(corpus, n=None):\n",
        "            #getting top ngrams\n",
        "            vec = CountVectorizer(ngram_range=(n, n),\n",
        "                                  max_df=0.9,\n",
        "                                  stop_words='english').fit(corpus)\n",
        "            bag_of_words = vec.transform(corpus)\n",
        "            sum_words = bag_of_words.sum(axis=0)\n",
        "            words_freq = [(word, sum_words[0, idx])\n",
        "                          for word, idx in vec.vocabulary_.items()]\n",
        "            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "            return words_freq[:15]\n",
        "\n",
        "        top_n_bigrams = _get_top_ngram(i, n)[:15]\n",
        "        x, y = map(list, zip(*top_n_bigrams))\n",
        "        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
        "\n",
        "        axes[0].set_title('Non Disaster Tweets')\n",
        "        axes[1].set_title('Disaster Tweets')\n",
        "        axes[0].set_xlabel('Count')\n",
        "        axes[0].set_ylabel('Words')\n",
        "        axes[1].set_xlabel('Count')\n",
        "        axes[1].set_ylabel('Words')\n",
        "        fig.suptitle(title, fontsize=24, va='baseline')\n",
        "        plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE4KqcJIE9vz"
      },
      "source": [
        "<a id=\"Most_Common_Bigrams\"></a>\n",
        "## Most Common Bigrams\n",
        "\n",
        "#### \"Exploring bigrams (pairs of adjacent words), disaster-related tweets are distinct, though non-disaster ones sometimes include terms like body bag or emergency service, requiring deeper analysis. Overall, the distinction is clear.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-U71sUb6r91",
        "outputId": "b45bf732-570c-4be6-fcca-4f81ae26a75d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ngrams(2, 'Most Common Bigrams')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35gVwr0OE9v0"
      },
      "source": [
        "<a id=\"Most_Common_Trigrams\"></a>\n",
        "## Most Common Trigrams\n",
        "\n",
        "#### \"The next code analyzes trigrams (three-word sequences). This resolves earlier confusion, such as body bags referring to cross body bags. Disaster-related tweets are now clearly distinguishable from non-disaster ones.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vunRj2r46r94",
        "outputId": "50adb84a-87a7-4ad8-fefe-42f726fd4f2a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ngrams(3, 'Most Common Trigrams')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihzr7kCME9v0"
      },
      "source": [
        "<a id=\"Building_the_Model\"></a>\n",
        "# Building the Bert Model\n",
        "\n",
        "####  \"For this task, we will use BERT, leveraging the Transformers library to streamline implementation.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znxx_dAJE9v0"
      },
      "source": [
        "<a id=\"Getting_Things_Ready\"></a>\n",
        "## Getting Things Ready\n",
        "\n",
        "#### \"The code selects GPU if available, defaulting to CPU (GPU recommended for speed). Training and test datasets are loaded, using raw text as BERT handles it effectively.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyRYMKRQ6r-s",
        "outputId": "ff9b2087-0848-42b5-ebe7-17fc0e729783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA GeForce GTX 1660 Ti\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDH7qw3O6r-v",
        "outputId": "c35c82bd-0857-4962-9da9-d1e9bd32ddb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training tweets: 7613\n",
            "\n",
            "Number of training tweets: 3263\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>761</td>\n",
              "      <td>avalanche</td>\n",
              "      <td>London, Kent &amp; SE England.</td>\n",
              "      <td>Beautiful Sweet Avalanche Faith and Akito rose...</td>\n",
              "      <td>0</td>\n",
              "      <td>Beautiful Sweet Avalanche Faith and Akito rose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2759</th>\n",
              "      <td>3962</td>\n",
              "      <td>devastation</td>\n",
              "      <td>contactSimpleNews@gmail.com</td>\n",
              "      <td>70 Years After Atomic Bombs Japan Still Strugg...</td>\n",
              "      <td>1</td>\n",
              "      <td>70 Years After Atomic Bombs Japan Still Strugg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6878</th>\n",
              "      <td>9861</td>\n",
              "      <td>traumatised</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm so traumatised.</td>\n",
              "      <td>0</td>\n",
              "      <td>Im so traumatised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4970</th>\n",
              "      <td>7086</td>\n",
              "      <td>meltdown</td>\n",
              "      <td>Two Up Two Down</td>\n",
              "      <td>@LeMaireLee @danharmon People Near Meltdown Co...</td>\n",
              "      <td>0</td>\n",
              "      <td>LeMaireLee danharmon People Near Meltdown Comi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4771</th>\n",
              "      <td>6789</td>\n",
              "      <td>lightning</td>\n",
              "      <td>Reddit</td>\n",
              "      <td>Lightning strike in the distance via /r/pics h...</td>\n",
              "      <td>1</td>\n",
              "      <td>Lightning strike in the distance via rpics  pics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4311</th>\n",
              "      <td>6120</td>\n",
              "      <td>hellfire</td>\n",
              "      <td>Denver, Colorado</td>\n",
              "      <td>@gg_keeponrockin @StrawberrySoryu Oh okay I ju...</td>\n",
              "      <td>1</td>\n",
              "      <td>ggkeeponrockin StrawberrySoryu Oh okay I just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3178</th>\n",
              "      <td>4563</td>\n",
              "      <td>emergency%20plan</td>\n",
              "      <td>Somewhere in the Canada</td>\n",
              "      <td>City of Calgary activates Municipal Emergency ...</td>\n",
              "      <td>1</td>\n",
              "      <td>City of Calgary activates Municipal Emergency ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7555</th>\n",
              "      <td>10800</td>\n",
              "      <td>wrecked</td>\n",
              "      <td>Milwaukee County</td>\n",
              "      <td>http://t.co/DeQQOpSP4f: Iger's 3 words that wr...</td>\n",
              "      <td>0</td>\n",
              "      <td>Igers 3 words that wrecked Disneys stock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3445</th>\n",
              "      <td>4923</td>\n",
              "      <td>exploded</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My little heart just exploded #OTRAMETLIFE #MT...</td>\n",
              "      <td>0</td>\n",
              "      <td>My little heart just exploded OTRAMETLIFE MTVH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1588</th>\n",
              "      <td>2294</td>\n",
              "      <td>cliff%20fall</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>One day I want someone to run for the ferry fa...</td>\n",
              "      <td>0</td>\n",
              "      <td>One day I want someone to run for the ferry fa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id           keyword                     location  \\\n",
              "526     761         avalanche   London, Kent & SE England.   \n",
              "2759   3962       devastation  contactSimpleNews@gmail.com   \n",
              "6878   9861       traumatised                          NaN   \n",
              "4970   7086          meltdown              Two Up Two Down   \n",
              "4771   6789         lightning                       Reddit   \n",
              "4311   6120          hellfire             Denver, Colorado   \n",
              "3178   4563  emergency%20plan      Somewhere in the Canada   \n",
              "7555  10800           wrecked             Milwaukee County   \n",
              "3445   4923          exploded                          NaN   \n",
              "1588   2294      cliff%20fall                 New York, NY   \n",
              "\n",
              "                                                   text  target  \\\n",
              "526   Beautiful Sweet Avalanche Faith and Akito rose...       0   \n",
              "2759  70 Years After Atomic Bombs Japan Still Strugg...       1   \n",
              "6878                                I'm so traumatised.       0   \n",
              "4970  @LeMaireLee @danharmon People Near Meltdown Co...       0   \n",
              "4771  Lightning strike in the distance via /r/pics h...       1   \n",
              "4311  @gg_keeponrockin @StrawberrySoryu Oh okay I ju...       1   \n",
              "3178  City of Calgary activates Municipal Emergency ...       1   \n",
              "7555  http://t.co/DeQQOpSP4f: Iger's 3 words that wr...       0   \n",
              "3445  My little heart just exploded #OTRAMETLIFE #MT...       0   \n",
              "1588  One day I want someone to run for the ferry fa...       0   \n",
              "\n",
              "                                             text_clean  \n",
              "526   Beautiful Sweet Avalanche Faith and Akito rose...  \n",
              "2759  70 Years After Atomic Bombs Japan Still Strugg...  \n",
              "6878                                  Im so traumatised  \n",
              "4970  LeMaireLee danharmon People Near Meltdown Comi...  \n",
              "4771   Lightning strike in the distance via rpics  pics  \n",
              "4311  ggkeeponrockin StrawberrySoryu Oh okay I just ...  \n",
              "3178  City of Calgary activates Municipal Emergency ...  \n",
              "7555           Igers 3 words that wrecked Disneys stock  \n",
              "3445  My little heart just exploded OTRAMETLIFE MTVH...  \n",
              "1588  One day I want someone to run for the ferry fa...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Loading the data for modelling.\n",
        "\n",
        "train = data1\n",
        "test = data2\n",
        "\n",
        "print(f'Number of training tweets: {train.shape[0]}\\n')\n",
        "print(f'Number of training tweets: {test.shape[0]}\\n')\n",
        "display(train.sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmobtZpn6r-y"
      },
      "outputs": [],
      "source": [
        "# Setting target variables, creating combined data and saving index for dividing combined data later.\n",
        "\n",
        "labels = train['target'].values\n",
        "idx = len(labels)\n",
        "combined = pd.concat([train, test])\n",
        "combined = combined.text.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TV7OXTZE9v1"
      },
      "source": [
        "<a id=\"Tokenization_and_Formatting_the_Inputs\"></a>\n",
        "## Tokenization and Formatting the Inputs\n",
        "\n",
        "#### tokenization using BERT's tokenizer (or any transformer-based model tokenizer) is necessary because BERT requires a specific type of tokenization. BERT uses subword tokenization (e.g., WordPiece in BERT), which breaks words into smaller subword units to handle out-of-vocabulary words and capture more nuanced meanings in context. This is crucial for transformer models to understand the semantic context of words in sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFP3p2aA6r-1"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the combined text data using bert tokenizer.\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0_JPi_r6r-3",
        "outputId": "0aa56d6d-dab8-4de4-98dd-073548e3ffd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Original:  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
            "Tokenized:  ['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', '#', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n",
            "Token IDs:  [2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035]\n"
          ]
        }
      ],
      "source": [
        "# Print the original tweet.\n",
        "\n",
        "print(' Original: ', combined[0])\n",
        "\n",
        "# Print the tweet split into tokens.\n",
        "\n",
        "print('Tokenized: ', tokenizer.tokenize(combined[0]))\n",
        "\n",
        "# Print the sentence mapped to token ID's.\n",
        "\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap4apYT16r-4",
        "outputId": "ff808191-9a93-458f-e82f-a6f24eef749f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  84\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "\n",
        "for text in combined:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26gsBcV9E9v2",
        "outputId": "efd450a7-15f7-4a77-ba38-6c5ed8c955ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ],
      "source": [
        "# Making list of sentence lenghts:\n",
        "\n",
        "token_lens = []\n",
        "\n",
        "for text in combined:\n",
        "    tokens = tokenizer.encode(text, max_length = 512)\n",
        "    token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2YG3YFiE9v2",
        "outputId": "6169ee1e-4232-474e-cad9-eb5bbe20e5dc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Displaying sentence length dist.\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(14, 6))\n",
        "sns.distplot(token_lens, color='#e74c3c')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rFT4jbKE9v2"
      },
      "source": [
        "### I choose max len of 84 since it's the longest sentence we have here, playing with this number might get different results but the bigger you choose the slower the model will be!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5nQqKaO6r-6",
        "outputId": "6bedb450-7dfb-4132-eda9-2283f0a27fb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7613,)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Splitting the train test data after tokenizing.\n",
        "\n",
        "train= combined[:idx]\n",
        "test = combined[idx:]\n",
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqbud3--6r-8"
      },
      "outputs": [],
      "source": [
        "def tokenize_map(sentences, labs=None):\n",
        "    \"\"\"A function to tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for text in sentences:\n",
        "        # \"encode_plus\" will:\n",
        "        # (1) Tokenize the sentence.\n",
        "        # (2) Prepend the [CLS] token to the start.\n",
        "        # (3) Append the [SEP] token to the end.\n",
        "        # (4) Map tokens to their IDs.\n",
        "        # (5) Pad or truncate the sentence to max_length\n",
        "        # (6) Create attention masks for [PAD] tokens.\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            text,                      # Sentence to encode.\n",
        "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
        "            truncation='longest_first', # Activate and control truncation\n",
        "            max_length=84,            # Max length according to your text data.\n",
        "            padding='max_length',     # Pad & truncate all sentences.\n",
        "            return_attention_mask=True, # Construct attention masks.\n",
        "            return_tensors='pt',      # Return PyTorch tensors.\n",
        "        )\n",
        "\n",
        "        # Add the encoded sentence to the ID list.\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # And its attention mask.\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    # Check if labs is provided and process labels accordingly\n",
        "    if labs is not None:  # labs should be checked for None, empty, or valid data\n",
        "        labels = torch.tensor(labs)\n",
        "        return input_ids, attention_masks, labels\n",
        "    else:\n",
        "        return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzUBnO5y6r--"
      },
      "outputs": [],
      "source": [
        "# Tokenizing all of the train test sentences and mapping the tokens to their word IDs.\n",
        "\n",
        "input_ids, attention_masks, labels = tokenize_map(train, labels)\n",
        "test_input_ids, test_attention_masks= tokenize_map(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF-cP9956r-_",
        "outputId": "5a1e09ff-b85c-433a-a380-2146c38f30fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6,090 training samples\n",
            "1,523 validation samples\n"
          ]
        }
      ],
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZL9--546r_C"
      },
      "outputs": [],
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order.\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rogeV1Nj6r_E"
      },
      "outputs": [],
      "source": [
        "prediction_data = TensorDataset(test_input_ids, test_attention_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ZEhr9IE9v3"
      },
      "source": [
        "\n",
        "## Setting the Bert Classification Model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "BityYQ8J6r_G",
        "outputId": "04838d37-af3e-4b47-82ef-be6e6a0a6719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI49270K6r_I",
        "outputId": "d90f98b0-f706-423f-8189-c9e625edfb61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples:\n",
        "\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7Qy4SHS6r_K"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch).\n",
        "\n",
        "# The 'W' stands for 'Weight Decay fix' probably...\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 6e-6, # args.learning_rate\n",
        "                  eps = 1e-8 # args.adam_epsilon\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2_EBwu96r_N"
      },
      "outputs": [],
      "source": [
        "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
        "\n",
        "# We chose to run for 3, but we'll see later that this may be over-fitting the training data.\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs] (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEuQUD8UE9v4"
      },
      "source": [
        "\n",
        "## Training and Evaluating\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBO_0cQf6r_P"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "\n",
        "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
        "\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return accuracy_score(labels_flat, pred_flat)\n",
        "\n",
        "def flat_f1(preds, labels):\n",
        "\n",
        "    \"\"\"A function for calculating f1 scores\"\"\"\n",
        "\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return f1_score(labels_flat, pred_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZc7gqT76r_R"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "\n",
        "    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n",
        "\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNBD02-66r_T",
        "outputId": "31ddb2d9-9f0e-4deb-97df-0e9ca81a1ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3322,  0.0512],\n",
            "        [ 0.1451, -0.0631],\n",
            "        [ 0.2560, -0.1472],\n",
            "        [ 0.2094, -0.1306],\n",
            "        [ 0.2646,  0.0174],\n",
            "        [ 0.2680, -0.2089],\n",
            "        [ 0.2325, -0.1381],\n",
            "        [ 0.0829, -0.2585],\n",
            "        [ 0.5214, -0.1103],\n",
            "        [ 0.1814,  0.0516],\n",
            "        [ 0.3359, -0.0305],\n",
            "        [ 0.4716, -0.2267],\n",
            "        [ 0.1376, -0.0972],\n",
            "        [ 0.1828,  0.0374],\n",
            "        [ 0.1553, -0.2799],\n",
            "        [-0.1166,  0.0743],\n",
            "        [ 0.0108,  0.0992],\n",
            "        [ 0.3963, -0.3307],\n",
            "        [ 0.4242, -0.3275],\n",
            "        [ 0.3671, -0.1157],\n",
            "        [ 0.3964,  0.0342],\n",
            "        [ 0.3116, -0.0704],\n",
            "        [ 0.0630, -0.1733],\n",
            "        [ 0.2358, -0.0261],\n",
            "        [ 0.4244,  0.0216],\n",
            "        [ 0.2683, -0.1145],\n",
            "        [ 0.1471, -0.0826],\n",
            "        [ 0.4499, -0.1227],\n",
            "        [ 0.1771, -0.0598],\n",
            "        [ 0.2178, -0.0761],\n",
            "        [ 0.3181, -0.1883],\n",
            "        [ 0.1911,  0.0834]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3131, -0.3135],\n",
            "        [ 0.2620, -0.0094],\n",
            "        [ 0.0415,  0.1473],\n",
            "        [ 0.1544, -0.1090],\n",
            "        [ 0.4492, -0.0144],\n",
            "        [ 0.1866,  0.2143],\n",
            "        [ 0.2986, -0.2868],\n",
            "        [ 0.3067, -0.0553],\n",
            "        [ 0.1246,  0.2305],\n",
            "        [ 0.3937, -0.1634],\n",
            "        [ 0.0251,  0.3383],\n",
            "        [ 0.4117, -0.1566],\n",
            "        [ 0.3888, -0.3287],\n",
            "        [ 0.2857,  0.1056],\n",
            "        [ 0.3692,  0.0675],\n",
            "        [ 0.0858, -0.0179],\n",
            "        [ 0.1973,  0.0377],\n",
            "        [ 0.1097, -0.0342],\n",
            "        [ 0.3205, -0.1646],\n",
            "        [ 0.1111, -0.1672],\n",
            "        [ 0.1546,  0.2302],\n",
            "        [ 0.1026,  0.0213],\n",
            "        [ 0.3006, -0.2097],\n",
            "        [ 0.0822, -0.0606],\n",
            "        [ 0.5461, -0.4760],\n",
            "        [ 0.3243, -0.0368],\n",
            "        [ 0.3005, -0.1182],\n",
            "        [ 0.0753,  0.1134],\n",
            "        [ 0.3287, -0.1268],\n",
            "        [ 0.3913, -0.1490],\n",
            "        [ 0.2346,  0.0161],\n",
            "        [ 0.2237,  0.0117]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3874, -0.1659],\n",
            "        [ 0.3083, -0.0712],\n",
            "        [ 0.5558, -0.1127],\n",
            "        [ 0.1042,  0.1423],\n",
            "        [ 0.2612,  0.1966],\n",
            "        [ 0.3877, -0.0315],\n",
            "        [ 0.4305, -0.3055],\n",
            "        [ 0.0863,  0.0012],\n",
            "        [ 0.5041, -0.2879],\n",
            "        [ 0.5164,  0.0771],\n",
            "        [ 0.1680,  0.1138],\n",
            "        [ 0.3815, -0.0936],\n",
            "        [ 0.1616,  0.0535],\n",
            "        [ 0.1201,  0.1644],\n",
            "        [ 0.3792, -0.0104],\n",
            "        [ 0.1434,  0.1301],\n",
            "        [ 0.3313, -0.1627],\n",
            "        [ 0.0357,  0.0322],\n",
            "        [ 0.3083,  0.0972],\n",
            "        [ 0.0704, -0.0296],\n",
            "        [ 0.3868, -0.0496],\n",
            "        [ 0.3241, -0.1838],\n",
            "        [ 0.2372,  0.1356],\n",
            "        [ 0.2342, -0.0878],\n",
            "        [ 0.1410, -0.1426],\n",
            "        [ 0.1904,  0.0673],\n",
            "        [ 0.1944,  0.0266],\n",
            "        [ 0.4077, -0.0959],\n",
            "        [ 0.1320,  0.0841],\n",
            "        [ 0.2976, -0.4499],\n",
            "        [ 0.0927,  0.0804],\n",
            "        [ 0.3429, -0.3258]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2420,  0.0112],\n",
            "        [ 0.0914,  0.1000],\n",
            "        [ 0.1176,  0.1845],\n",
            "        [ 0.2616,  0.0852],\n",
            "        [ 0.2413, -0.0758],\n",
            "        [ 0.2926, -0.2081],\n",
            "        [ 0.3106,  0.0860],\n",
            "        [ 0.1429, -0.0898],\n",
            "        [-0.0397, -0.0783],\n",
            "        [ 0.1642,  0.0557],\n",
            "        [ 0.1161,  0.0217],\n",
            "        [ 0.2928, -0.0874],\n",
            "        [ 0.1909,  0.1187],\n",
            "        [ 0.4296, -0.0992],\n",
            "        [ 0.1287, -0.1286],\n",
            "        [ 0.2439, -0.0722],\n",
            "        [ 0.2016,  0.1088],\n",
            "        [ 0.6240, -0.1421],\n",
            "        [ 0.3756,  0.2460],\n",
            "        [ 0.2021, -0.1234],\n",
            "        [ 0.1421, -0.2375],\n",
            "        [ 0.1508,  0.0647],\n",
            "        [ 0.1894,  0.0440],\n",
            "        [ 0.1021,  0.0083],\n",
            "        [ 0.2688, -0.0469],\n",
            "        [ 0.4259, -0.1007],\n",
            "        [ 0.5147,  0.0261],\n",
            "        [ 0.1712, -0.2613],\n",
            "        [ 0.2437, -0.0996],\n",
            "        [ 0.0055,  0.0542],\n",
            "        [ 0.3836, -0.2322],\n",
            "        [ 0.1029, -0.0289]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2008, -0.1252],\n",
            "        [ 0.1760,  0.1029],\n",
            "        [ 0.3966, -0.0124],\n",
            "        [ 0.4577, -0.1364],\n",
            "        [ 0.1526,  0.1512],\n",
            "        [ 0.1225,  0.1969],\n",
            "        [ 0.1773, -0.1886],\n",
            "        [ 0.1229,  0.2392],\n",
            "        [ 0.0639,  0.2251],\n",
            "        [ 0.0216,  0.0283],\n",
            "        [ 0.0527, -0.0403],\n",
            "        [ 0.2358,  0.0254],\n",
            "        [ 0.2447, -0.0198],\n",
            "        [ 0.0546, -0.0139],\n",
            "        [ 0.3150,  0.1222],\n",
            "        [ 0.3321, -0.0415],\n",
            "        [ 0.1689, -0.2199],\n",
            "        [ 0.3179, -0.0385],\n",
            "        [ 0.5807, -0.3990],\n",
            "        [ 0.4794, -0.0795],\n",
            "        [ 0.3341,  0.1960],\n",
            "        [ 0.2960,  0.1029],\n",
            "        [ 0.2488, -0.2681],\n",
            "        [ 0.1651, -0.1369],\n",
            "        [ 0.0720,  0.1357],\n",
            "        [ 0.2259,  0.0998],\n",
            "        [ 0.2622,  0.0723],\n",
            "        [ 0.1781,  0.0426],\n",
            "        [ 0.2424,  0.2888],\n",
            "        [ 0.1195, -0.2018],\n",
            "        [ 0.1225,  0.1340],\n",
            "        [ 0.5702,  0.1959]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1496,  0.1034],\n",
            "        [ 0.1975,  0.0454],\n",
            "        [ 0.3425, -0.1069],\n",
            "        [ 0.1716, -0.0280],\n",
            "        [ 0.0972,  0.0353],\n",
            "        [ 0.2907, -0.2223],\n",
            "        [-0.0095,  0.3142],\n",
            "        [ 0.3089,  0.0228],\n",
            "        [ 0.3417, -0.0027],\n",
            "        [ 0.3954,  0.1394],\n",
            "        [ 0.4163, -0.2894],\n",
            "        [ 0.1017,  0.2027],\n",
            "        [ 0.3324, -0.1277],\n",
            "        [ 0.1005,  0.2270],\n",
            "        [ 0.2812, -0.0054],\n",
            "        [ 0.2954,  0.0056],\n",
            "        [ 0.0713, -0.1701],\n",
            "        [ 0.3769,  0.0185],\n",
            "        [ 0.2668, -0.1967],\n",
            "        [ 0.3637, -0.2653],\n",
            "        [ 0.4483, -0.2918],\n",
            "        [ 0.2057, -0.0656],\n",
            "        [ 0.3582, -0.0519],\n",
            "        [ 0.4294, -0.2889],\n",
            "        [ 0.0990,  0.2421],\n",
            "        [ 0.1576,  0.0686],\n",
            "        [ 0.3522, -0.1655],\n",
            "        [ 0.3388,  0.0977],\n",
            "        [ 0.2535, -0.0872],\n",
            "        [ 0.0347, -0.0679],\n",
            "        [ 0.2610,  0.0615],\n",
            "        [ 0.3806, -0.2037]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3310,  0.2014],\n",
            "        [ 0.2012, -0.1572],\n",
            "        [ 0.3303, -0.0231],\n",
            "        [ 0.2794, -0.0826],\n",
            "        [ 0.3013,  0.1591],\n",
            "        [ 0.3494, -0.1868],\n",
            "        [ 0.0758,  0.1684],\n",
            "        [-0.0707, -0.0445],\n",
            "        [ 0.3834, -0.2224],\n",
            "        [ 0.4923, -0.0942],\n",
            "        [ 0.2722, -0.2239],\n",
            "        [ 0.2129, -0.1409],\n",
            "        [ 0.2708,  0.0212],\n",
            "        [ 0.4046,  0.1143],\n",
            "        [ 0.2942, -0.0980],\n",
            "        [ 0.2846,  0.0238],\n",
            "        [ 0.3678, -0.0312],\n",
            "        [ 0.2744,  0.1130],\n",
            "        [ 0.3878, -0.1849],\n",
            "        [ 0.0936, -0.0677],\n",
            "        [ 0.0685,  0.0678],\n",
            "        [ 0.2354, -0.0450],\n",
            "        [ 0.2515, -0.0483],\n",
            "        [ 0.2032,  0.0724],\n",
            "        [ 0.1311,  0.1147],\n",
            "        [ 0.2771, -0.1050],\n",
            "        [ 0.1023,  0.2032],\n",
            "        [ 0.1320, -0.0131],\n",
            "        [ 0.1885,  0.1685],\n",
            "        [ 0.4171, -0.3736],\n",
            "        [ 0.3561,  0.1019],\n",
            "        [ 0.2254,  0.2360]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3242, -0.0511],\n",
            "        [ 0.4356, -0.2895],\n",
            "        [ 0.4033, -0.1634],\n",
            "        [ 0.2711,  0.1031],\n",
            "        [ 0.4522, -0.1207],\n",
            "        [ 0.3730, -0.2314],\n",
            "        [ 0.4275, -0.2310],\n",
            "        [ 0.1954, -0.1579],\n",
            "        [ 0.2343, -0.1582],\n",
            "        [ 0.3094, -0.0996],\n",
            "        [ 0.1887, -0.1114],\n",
            "        [ 0.3254, -0.2136],\n",
            "        [ 0.0347, -0.0626],\n",
            "        [ 0.1979, -0.0748],\n",
            "        [ 0.2269, -0.1110],\n",
            "        [ 0.1587,  0.0456],\n",
            "        [ 0.3167, -0.1590],\n",
            "        [ 0.3495, -0.0512],\n",
            "        [ 0.3521, -0.2124],\n",
            "        [ 0.4042, -0.1171],\n",
            "        [ 0.2453, -0.1552],\n",
            "        [ 0.2197,  0.0306],\n",
            "        [ 0.3267, -0.1107],\n",
            "        [ 0.2751,  0.1334],\n",
            "        [ 0.1079,  0.2063],\n",
            "        [ 0.4983, -0.0653],\n",
            "        [ 0.3060,  0.2201],\n",
            "        [ 0.2831,  0.0493],\n",
            "        [ 0.1845, -0.1125],\n",
            "        [ 0.1946,  0.0729],\n",
            "        [ 0.2931, -0.0967],\n",
            "        [ 0.2038,  0.1568]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2324, -0.3194],\n",
            "        [ 0.2766,  0.3020],\n",
            "        [ 0.2974,  0.1023],\n",
            "        [ 0.3168,  0.0236],\n",
            "        [ 0.1227, -0.1718],\n",
            "        [ 0.2513,  0.1424],\n",
            "        [ 0.2243,  0.2013],\n",
            "        [ 0.2307, -0.1477],\n",
            "        [ 0.2749,  0.0189],\n",
            "        [ 0.1182, -0.2130],\n",
            "        [ 0.1974,  0.1713],\n",
            "        [ 0.0851,  0.2077],\n",
            "        [ 0.0694,  0.1561],\n",
            "        [ 0.2140, -0.0445],\n",
            "        [ 0.2999, -0.0896],\n",
            "        [ 0.4729, -0.3480],\n",
            "        [ 0.1522, -0.0440],\n",
            "        [ 0.2819, -0.0731],\n",
            "        [ 0.2495,  0.2194],\n",
            "        [ 0.0600,  0.0835],\n",
            "        [ 0.1968, -0.0394],\n",
            "        [ 0.3811, -0.1497],\n",
            "        [ 0.3245,  0.0589],\n",
            "        [ 0.2913,  0.0373],\n",
            "        [ 0.5143,  0.0456],\n",
            "        [ 0.2166,  0.2209],\n",
            "        [ 0.3871,  0.2172],\n",
            "        [ 0.2312, -0.0919],\n",
            "        [ 0.2162,  0.1296],\n",
            "        [ 0.3004,  0.0405],\n",
            "        [ 0.1823, -0.3510],\n",
            "        [ 0.2557, -0.3638]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2353, -0.0543],\n",
            "        [ 0.3550, -0.0990],\n",
            "        [ 0.3267, -0.0092],\n",
            "        [ 0.2368,  0.2523],\n",
            "        [ 0.1281, -0.1984],\n",
            "        [ 0.2419, -0.0007],\n",
            "        [ 0.4780, -0.1297],\n",
            "        [ 0.1737, -0.0783],\n",
            "        [ 0.1560,  0.1408],\n",
            "        [-0.0629,  0.0011],\n",
            "        [ 0.2810,  0.0952],\n",
            "        [ 0.2846, -0.0287],\n",
            "        [ 0.2442,  0.1883],\n",
            "        [ 0.2481, -0.2541],\n",
            "        [ 0.1406,  0.1786],\n",
            "        [ 0.4600, -0.2290],\n",
            "        [ 0.2198, -0.0580],\n",
            "        [-0.0758,  0.2413],\n",
            "        [ 0.4772, -0.3257],\n",
            "        [ 0.1886,  0.3782],\n",
            "        [ 0.3291,  0.1258],\n",
            "        [ 0.3796, -0.0638],\n",
            "        [ 0.0914,  0.0738],\n",
            "        [ 0.2825,  0.2303],\n",
            "        [ 0.2981, -0.1242],\n",
            "        [ 0.3098, -0.3516],\n",
            "        [ 0.3325, -0.0217],\n",
            "        [ 0.4491, -0.0620],\n",
            "        [ 0.2344,  0.2320],\n",
            "        [ 0.3094, -0.2397],\n",
            "        [ 0.2328, -0.2372],\n",
            "        [ 0.1186, -0.2937]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1894, -0.2464],\n",
            "        [ 0.0421, -0.0979],\n",
            "        [ 0.0092,  0.0861],\n",
            "        [ 0.2552, -0.0782],\n",
            "        [ 0.4984, -0.4802],\n",
            "        [ 0.0149,  0.2373],\n",
            "        [-0.0737,  0.3639],\n",
            "        [ 0.2818,  0.2534],\n",
            "        [ 0.5015, -0.3629],\n",
            "        [ 0.0307,  0.2733],\n",
            "        [ 0.1239,  0.2689],\n",
            "        [ 0.3710,  0.0018],\n",
            "        [ 0.3075, -0.2202],\n",
            "        [ 0.4432,  0.0201],\n",
            "        [ 0.0679,  0.0829],\n",
            "        [ 0.0108,  0.2550],\n",
            "        [ 0.1339,  0.0261],\n",
            "        [ 0.1647,  0.0571],\n",
            "        [ 0.3800, -0.3375],\n",
            "        [ 0.0587, -0.0951],\n",
            "        [ 0.1891,  0.2498],\n",
            "        [ 0.1324,  0.0715],\n",
            "        [ 0.1370, -0.0770],\n",
            "        [ 0.3255, -0.0278],\n",
            "        [ 0.2961, -0.1909],\n",
            "        [ 0.4304,  0.0403],\n",
            "        [-0.0832,  0.1597],\n",
            "        [ 0.3572, -0.2889],\n",
            "        [ 0.1802,  0.1044],\n",
            "        [-0.0942, -0.0219],\n",
            "        [ 0.1529,  0.0110],\n",
            "        [ 0.0407,  0.3388]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3052, -0.0764],\n",
            "        [ 0.3835, -0.1984],\n",
            "        [-0.0520,  0.1914],\n",
            "        [ 0.4379, -0.4264],\n",
            "        [ 0.2952, -0.0447],\n",
            "        [ 0.1915,  0.4033],\n",
            "        [ 0.1470,  0.2066],\n",
            "        [-0.1282,  0.3681],\n",
            "        [ 0.2823, -0.2530],\n",
            "        [ 0.1505,  0.0594],\n",
            "        [ 0.4240, -0.1719],\n",
            "        [ 0.1757, -0.0427],\n",
            "        [ 0.1160, -0.0125],\n",
            "        [ 0.2796, -0.0386],\n",
            "        [ 0.2720, -0.0079],\n",
            "        [ 0.5013, -0.1039],\n",
            "        [ 0.0194,  0.4970],\n",
            "        [ 0.3389, -0.1118],\n",
            "        [ 0.0710,  0.0060],\n",
            "        [ 0.2466, -0.0045],\n",
            "        [ 0.0308,  0.0910],\n",
            "        [ 0.2249,  0.2187],\n",
            "        [ 0.2004, -0.2877],\n",
            "        [ 0.0099,  0.2592],\n",
            "        [ 0.2191,  0.6241],\n",
            "        [ 0.3943, -0.4064],\n",
            "        [ 0.0961, -0.0900],\n",
            "        [ 0.1809, -0.1230],\n",
            "        [ 0.0998,  0.0864],\n",
            "        [ 0.1826,  0.0072],\n",
            "        [ 0.2027,  0.0565],\n",
            "        [ 0.1165,  0.1678]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1800,  0.1452],\n",
            "        [ 0.0542,  0.1896],\n",
            "        [ 0.0943,  0.2314],\n",
            "        [ 0.0820, -0.0826],\n",
            "        [ 0.2726,  0.0212],\n",
            "        [ 0.2431,  0.1903],\n",
            "        [ 0.3200, -0.2992],\n",
            "        [-0.0610,  0.2752],\n",
            "        [ 0.4682, -0.2128],\n",
            "        [ 0.1407, -0.0301],\n",
            "        [ 0.2878, -0.2906],\n",
            "        [ 0.1658,  0.2272],\n",
            "        [ 0.1674,  0.2320],\n",
            "        [ 0.2436,  0.0627],\n",
            "        [ 0.3939, -0.1546],\n",
            "        [ 0.0432,  0.2942],\n",
            "        [ 0.5114, -0.3432],\n",
            "        [ 0.2823, -0.1196],\n",
            "        [ 0.0320,  0.2259],\n",
            "        [ 0.3151, -0.2815],\n",
            "        [ 0.4602, -0.2751],\n",
            "        [ 0.1778, -0.1010],\n",
            "        [ 0.0446, -0.0532],\n",
            "        [ 0.3779,  0.0688],\n",
            "        [ 0.2266,  0.2484],\n",
            "        [ 0.3681, -0.2805],\n",
            "        [ 0.3782, -0.1450],\n",
            "        [ 0.1129, -0.0508],\n",
            "        [ 0.3848,  0.0582],\n",
            "        [ 0.3663, -0.2997],\n",
            "        [ 0.2168,  0.2030],\n",
            "        [-0.0884,  0.3758]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1753,  0.3684],\n",
            "        [ 0.1927, -0.0345],\n",
            "        [ 0.1830, -0.2276],\n",
            "        [ 0.0759,  0.1415],\n",
            "        [ 0.1094,  0.1545],\n",
            "        [ 0.3180, -0.2432],\n",
            "        [ 0.4082, -0.1817],\n",
            "        [ 0.4211, -0.2846],\n",
            "        [ 0.1295, -0.1469],\n",
            "        [-0.0026,  0.1776],\n",
            "        [ 0.2702, -0.0519],\n",
            "        [ 0.2308, -0.0309],\n",
            "        [ 0.0413,  0.0324],\n",
            "        [ 0.1508,  0.2509],\n",
            "        [ 0.3703, -0.1642],\n",
            "        [ 0.3426, -0.0195],\n",
            "        [ 0.1266, -0.1336],\n",
            "        [ 0.3108,  0.0454],\n",
            "        [ 0.1770, -0.0389],\n",
            "        [ 0.2080, -0.0703],\n",
            "        [ 0.2121, -0.1614],\n",
            "        [ 0.2772, -0.0288],\n",
            "        [ 0.3160,  0.2300],\n",
            "        [ 0.4133, -0.1119],\n",
            "        [ 0.4123,  0.2559],\n",
            "        [ 0.2591, -0.2150],\n",
            "        [ 0.2541, -0.1789],\n",
            "        [ 0.1798,  0.0262],\n",
            "        [ 0.3327, -0.6564],\n",
            "        [ 0.2730,  0.0721],\n",
            "        [ 0.3316, -0.1544],\n",
            "        [ 0.2546,  0.3582]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1793, -0.1449],\n",
            "        [ 0.1586,  0.1298],\n",
            "        [ 0.0405,  0.1576],\n",
            "        [-0.1950,  0.2822],\n",
            "        [ 0.4457, -0.2036],\n",
            "        [ 0.4462, -0.3027],\n",
            "        [ 0.3297, -0.3236],\n",
            "        [ 0.4389, -0.3997],\n",
            "        [ 0.0746,  0.0614],\n",
            "        [ 0.2765, -0.1551],\n",
            "        [ 0.0269,  0.2365],\n",
            "        [ 0.3333,  0.1828],\n",
            "        [-0.0729, -0.0414],\n",
            "        [ 0.3013, -0.1604],\n",
            "        [ 0.1547,  0.2629],\n",
            "        [ 0.3747, -0.0305],\n",
            "        [ 0.4880, -0.5597],\n",
            "        [ 0.1929, -0.4219],\n",
            "        [-0.0028,  0.2187],\n",
            "        [ 0.1400,  0.1348],\n",
            "        [ 0.1894, -0.2140],\n",
            "        [ 0.2874, -0.2675],\n",
            "        [ 0.0882,  0.2517],\n",
            "        [ 0.2162,  0.1413],\n",
            "        [ 0.0343,  0.3536],\n",
            "        [ 0.2079, -0.2502],\n",
            "        [ 0.2042, -0.0938],\n",
            "        [ 0.2058, -0.0341],\n",
            "        [ 0.1671,  0.0300],\n",
            "        [ 0.3408, -0.4896],\n",
            "        [ 0.1455, -0.0733],\n",
            "        [ 0.1073,  0.2025]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0462,  0.2880],\n",
            "        [ 0.2857, -0.6725],\n",
            "        [ 0.1907,  0.0292],\n",
            "        [ 0.4166,  0.0041],\n",
            "        [ 0.1537,  0.1915],\n",
            "        [ 0.1215, -0.1032],\n",
            "        [ 0.2402,  0.0070],\n",
            "        [ 0.1948, -0.2107],\n",
            "        [-0.1235,  0.3319],\n",
            "        [ 0.1032,  0.4392],\n",
            "        [-0.0099,  0.2654],\n",
            "        [-0.1395,  0.4129],\n",
            "        [ 0.0717,  0.0152],\n",
            "        [ 0.1579,  0.0486],\n",
            "        [ 0.2262,  0.3107],\n",
            "        [ 0.5242, -0.2894],\n",
            "        [ 0.3372, -0.0161],\n",
            "        [-0.0704,  0.2537],\n",
            "        [ 0.0215,  0.0449],\n",
            "        [ 0.4252, -0.1463],\n",
            "        [ 0.2087, -0.1062],\n",
            "        [-0.0323,  0.3315],\n",
            "        [ 0.1322, -0.0473],\n",
            "        [ 0.3230,  0.0203],\n",
            "        [ 0.1805,  0.2449],\n",
            "        [ 0.3996, -0.3698],\n",
            "        [ 0.3469, -0.4691],\n",
            "        [ 0.1858,  0.2317],\n",
            "        [ 0.3766,  0.1848],\n",
            "        [ 0.0929,  0.1529],\n",
            "        [ 0.2158,  0.1080],\n",
            "        [ 0.1729, -0.1113]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0483,  0.1156],\n",
            "        [ 0.2348, -0.1887],\n",
            "        [ 0.0569,  0.0914],\n",
            "        [ 0.4156, -0.4089],\n",
            "        [ 0.4528, -0.2372],\n",
            "        [-0.0311,  0.2212],\n",
            "        [ 0.2255, -0.0330],\n",
            "        [ 0.1078,  0.0664],\n",
            "        [ 0.1892, -0.0453],\n",
            "        [ 0.1366,  0.2082],\n",
            "        [ 0.0976,  0.2762],\n",
            "        [ 0.1976, -0.1534],\n",
            "        [ 0.3292, -0.0954],\n",
            "        [ 0.2758, -0.1629],\n",
            "        [ 0.1527, -0.2591],\n",
            "        [ 0.2172, -0.1017],\n",
            "        [ 0.2556, -0.1525],\n",
            "        [ 0.5333, -0.6121],\n",
            "        [ 0.2430, -0.2276],\n",
            "        [ 0.3751,  0.2773],\n",
            "        [ 0.2239,  0.0969],\n",
            "        [ 0.3476, -0.2786],\n",
            "        [ 0.4084,  0.1707],\n",
            "        [ 0.4075,  0.2250],\n",
            "        [ 0.1413,  0.3490],\n",
            "        [-0.0307,  0.4543],\n",
            "        [ 0.3677, -0.1128],\n",
            "        [ 0.4133, -0.1028],\n",
            "        [ 0.2464, -0.2083],\n",
            "        [ 0.4219, -0.0975],\n",
            "        [ 0.3961, -0.1110],\n",
            "        [ 0.3042, -0.3435]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0054,  0.2057],\n",
            "        [-0.0252,  0.0729],\n",
            "        [ 0.3649, -0.4036],\n",
            "        [ 0.3838, -0.2837],\n",
            "        [ 0.1017,  0.0908],\n",
            "        [ 0.3047, -0.1032],\n",
            "        [ 0.2664, -0.2371],\n",
            "        [ 0.2662,  0.0256],\n",
            "        [ 0.3879, -0.3586],\n",
            "        [ 0.3736, -0.2332],\n",
            "        [ 0.0907,  0.0135],\n",
            "        [ 0.2518, -0.2539],\n",
            "        [ 0.2256,  0.1540],\n",
            "        [ 0.2310, -0.0581],\n",
            "        [ 0.3866, -0.3403],\n",
            "        [ 0.0754,  0.3204],\n",
            "        [ 0.3822, -0.2962],\n",
            "        [ 0.4592, -0.1753],\n",
            "        [ 0.3842, -0.2548],\n",
            "        [ 0.5574, -0.2765],\n",
            "        [ 0.3688, -0.1184],\n",
            "        [ 0.3481, -0.2209],\n",
            "        [ 0.5176, -0.4064],\n",
            "        [ 0.1496, -0.1680],\n",
            "        [ 0.3143, -0.2422],\n",
            "        [ 0.0579,  0.2233],\n",
            "        [ 0.5037, -0.4698],\n",
            "        [ 0.2396, -0.1851],\n",
            "        [ 0.0313,  0.3274],\n",
            "        [ 0.2688, -0.2959],\n",
            "        [ 0.3801, -0.1304],\n",
            "        [ 0.4781, -0.6227]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2489, -0.1509],\n",
            "        [ 0.4213, -0.2448],\n",
            "        [ 0.5822, -0.0037],\n",
            "        [ 0.3179, -0.1120],\n",
            "        [ 0.3806, -0.0498],\n",
            "        [ 0.0470,  0.1062],\n",
            "        [ 0.4244, -0.6087],\n",
            "        [ 0.1870,  0.0520],\n",
            "        [ 0.0135,  0.1090],\n",
            "        [ 0.2938, -0.0141],\n",
            "        [-0.0966,  0.3173],\n",
            "        [ 0.3170, -0.4030],\n",
            "        [-0.0237, -0.0816],\n",
            "        [ 0.3053, -0.0500],\n",
            "        [ 0.2857, -0.2518],\n",
            "        [ 0.1827,  0.0399],\n",
            "        [ 0.5393, -0.2877],\n",
            "        [ 0.2416, -0.1220],\n",
            "        [ 0.4223, -0.4542],\n",
            "        [ 0.0976, -0.1074],\n",
            "        [-0.0287,  0.1914],\n",
            "        [ 0.0822,  0.2650],\n",
            "        [-0.0134,  0.0872],\n",
            "        [ 0.4076, -0.1809],\n",
            "        [ 0.4425, -0.2545],\n",
            "        [ 0.0535,  0.2767],\n",
            "        [ 0.3110, -0.0911],\n",
            "        [ 0.4563, -0.6792],\n",
            "        [ 0.3602, -0.0703],\n",
            "        [ 0.4671, -0.0765],\n",
            "        [-0.0486,  0.4720],\n",
            "        [ 0.3803, -0.1027]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 4.9515e-01, -1.9884e-01],\n",
            "        [ 5.4296e-01, -8.4353e-02],\n",
            "        [ 1.2432e-01,  3.0655e-01],\n",
            "        [ 3.5946e-01, -2.3852e-01],\n",
            "        [-1.1973e-02,  2.5835e-01],\n",
            "        [ 5.3346e-01, -5.3107e-01],\n",
            "        [ 2.1523e-01,  7.2784e-03],\n",
            "        [ 6.0522e-02,  2.5247e-01],\n",
            "        [ 1.3789e-01,  1.1652e-01],\n",
            "        [ 3.6453e-01, -3.5626e-01],\n",
            "        [ 3.4759e-01, -2.1122e-01],\n",
            "        [ 1.6511e-01,  3.7913e-01],\n",
            "        [ 3.0084e-01, -3.9394e-01],\n",
            "        [ 2.7530e-01, -1.8086e-01],\n",
            "        [ 4.1320e-01, -3.7327e-01],\n",
            "        [ 3.3506e-01, -1.6817e-01],\n",
            "        [ 1.5497e-01,  2.4172e-01],\n",
            "        [ 6.6312e-02,  2.4664e-01],\n",
            "        [ 2.9262e-01, -5.4580e-02],\n",
            "        [ 3.2601e-01,  5.1120e-02],\n",
            "        [ 3.4029e-01, -1.8576e-01],\n",
            "        [ 1.8150e-01, -1.8051e-01],\n",
            "        [ 4.0202e-01, -3.3522e-01],\n",
            "        [ 4.1031e-01, -3.6015e-01],\n",
            "        [-4.5917e-02,  2.4213e-01],\n",
            "        [ 1.5883e-05,  2.1943e-01],\n",
            "        [ 4.1142e-01, -1.5617e-01],\n",
            "        [ 4.1362e-01, -9.9012e-02],\n",
            "        [ 3.7903e-01, -2.6898e-01],\n",
            "        [ 3.0449e-01,  3.5568e-02],\n",
            "        [ 1.0083e-01,  4.7421e-01],\n",
            "        [ 2.9981e-01, -4.5428e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0705, -0.0614],\n",
            "        [ 0.3115, -0.0347],\n",
            "        [ 0.2407, -0.1418],\n",
            "        [-0.1240,  0.3174],\n",
            "        [ 0.3089, -0.0314],\n",
            "        [ 0.2183, -0.5293],\n",
            "        [ 0.6559, -0.1773],\n",
            "        [ 0.2101, -0.1648],\n",
            "        [ 0.3852, -0.2308],\n",
            "        [ 0.3776, -0.0978],\n",
            "        [ 0.0803,  0.2560],\n",
            "        [ 0.2167, -0.4580],\n",
            "        [ 0.3972, -0.1323],\n",
            "        [ 0.5980, -0.2942],\n",
            "        [-0.0539,  0.4272],\n",
            "        [ 0.4012, -0.2281],\n",
            "        [ 0.0561,  0.0388],\n",
            "        [ 0.5462, -0.5840],\n",
            "        [ 0.2828,  0.1368],\n",
            "        [-0.0340,  0.1595],\n",
            "        [ 0.2453,  0.2277],\n",
            "        [ 0.6762, -0.4855],\n",
            "        [ 0.4348, -0.3184],\n",
            "        [-0.1143,  0.1297],\n",
            "        [ 0.4940, -0.3768],\n",
            "        [ 0.0278,  0.1813],\n",
            "        [ 0.4114, -0.5437],\n",
            "        [ 0.1258,  0.1634],\n",
            "        [-0.0235, -0.2139],\n",
            "        [ 0.0401,  0.3566],\n",
            "        [ 0.3343, -0.0926],\n",
            "        [ 0.3894,  0.4839]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5873e-01, -2.3673e-01],\n",
            "        [ 1.2411e-01,  8.5771e-02],\n",
            "        [ 2.1408e-01,  3.2907e-01],\n",
            "        [ 1.8981e-01, -4.8614e-02],\n",
            "        [ 3.1376e-01, -7.0524e-02],\n",
            "        [ 6.4236e-01, -5.9161e-01],\n",
            "        [ 8.3386e-02,  1.5802e-01],\n",
            "        [ 2.9655e-01,  2.5975e-02],\n",
            "        [ 1.5604e-01,  2.7314e-02],\n",
            "        [ 2.8030e-02, -4.3203e-02],\n",
            "        [-2.2445e-02,  5.0896e-02],\n",
            "        [ 1.2061e-03,  2.8164e-01],\n",
            "        [ 3.5027e-01, -7.3226e-01],\n",
            "        [-2.4838e-01,  4.5942e-01],\n",
            "        [ 3.6061e-01, -7.2268e-02],\n",
            "        [ 1.9482e-01,  1.0412e-01],\n",
            "        [ 2.1261e-01, -1.2526e-01],\n",
            "        [ 2.7119e-01, -3.5304e-01],\n",
            "        [-6.6902e-02,  2.2537e-01],\n",
            "        [ 3.0644e-02,  1.2739e-01],\n",
            "        [ 4.4465e-01, -7.2203e-01],\n",
            "        [ 4.2149e-01, -2.8609e-01],\n",
            "        [ 2.2323e-01, -4.9502e-01],\n",
            "        [ 2.8613e-01, -2.6330e-01],\n",
            "        [ 1.4658e-01, -5.0776e-02],\n",
            "        [ 4.5277e-01, -3.4294e-01],\n",
            "        [ 8.3421e-02,  1.5404e-05],\n",
            "        [ 8.7959e-02,  3.7335e-01],\n",
            "        [ 1.7751e-01, -2.5456e-01],\n",
            "        [ 7.1263e-02,  9.1000e-02],\n",
            "        [ 3.0637e-01,  1.5918e-01],\n",
            "        [ 3.9991e-01, -2.3201e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2057, -0.1023],\n",
            "        [ 0.3845, -0.3275],\n",
            "        [-0.0286,  0.3294],\n",
            "        [ 0.4112, -0.2283],\n",
            "        [ 0.0428,  0.0061],\n",
            "        [ 0.4424, -0.5300],\n",
            "        [ 0.0607, -0.0370],\n",
            "        [ 0.0556, -0.1556],\n",
            "        [-0.1517,  0.3963],\n",
            "        [ 0.4118, -0.2040],\n",
            "        [ 0.2338, -0.1239],\n",
            "        [ 0.3716, -0.1130],\n",
            "        [ 0.3399,  0.0979],\n",
            "        [-0.0606,  0.4152],\n",
            "        [ 0.4356, -0.7667],\n",
            "        [ 0.5340, -0.1627],\n",
            "        [ 0.2389, -0.1634],\n",
            "        [ 0.3940, -0.3485],\n",
            "        [ 0.2817, -0.1632],\n",
            "        [-0.1468,  0.2291],\n",
            "        [ 0.3681, -0.0616],\n",
            "        [ 0.3529, -0.2906],\n",
            "        [ 0.3928, -0.2227],\n",
            "        [ 0.3296, -0.0615],\n",
            "        [ 0.2481, -0.1140],\n",
            "        [ 0.0465, -0.0488],\n",
            "        [ 0.5459, -0.1634],\n",
            "        [ 0.0404,  0.1833],\n",
            "        [ 0.3742, -0.0874],\n",
            "        [ 0.3916, -0.0755],\n",
            "        [ 0.3153, -0.2059],\n",
            "        [-0.0643,  0.1606]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5801, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0612, -0.0916],\n",
            "        [ 0.4688, -0.1888],\n",
            "        [ 0.1704,  0.0925],\n",
            "        [ 0.2091, -0.0953],\n",
            "        [ 0.6560, -0.6648],\n",
            "        [ 0.2549, -0.0321],\n",
            "        [ 0.2685, -0.1540],\n",
            "        [ 0.5370, -0.2286],\n",
            "        [-0.0539,  0.0824],\n",
            "        [ 0.2258, -0.2086],\n",
            "        [ 0.0814,  0.5084],\n",
            "        [ 0.4919, -0.2319],\n",
            "        [ 0.3589, -0.3251],\n",
            "        [-0.0022,  0.5794],\n",
            "        [ 0.4556, -0.2609],\n",
            "        [ 0.5620, -0.6177],\n",
            "        [-0.0743,  0.3424],\n",
            "        [-0.1066,  0.3648],\n",
            "        [ 0.1511, -0.1343],\n",
            "        [ 0.3649, -0.2663],\n",
            "        [-0.1962,  0.6664],\n",
            "        [ 0.4606, -0.1304],\n",
            "        [-0.0816,  0.3059],\n",
            "        [ 0.5894, -0.1282],\n",
            "        [-0.1764,  0.4182],\n",
            "        [-0.2732,  0.4324],\n",
            "        [ 0.0689,  0.2766],\n",
            "        [ 0.3787, -0.2967],\n",
            "        [ 0.3002, -0.3063],\n",
            "        [ 0.1487, -0.2007],\n",
            "        [ 0.3045,  0.2342],\n",
            "        [-0.0861,  0.0055]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0043,  0.2991],\n",
            "        [ 0.4636, -0.8080],\n",
            "        [ 0.0430,  0.0731],\n",
            "        [ 0.3239, -0.3074],\n",
            "        [ 0.1667,  0.1903],\n",
            "        [ 0.5659, -0.4354],\n",
            "        [ 0.3085, -0.1408],\n",
            "        [ 0.4897, -0.1121],\n",
            "        [ 0.0979,  0.1052],\n",
            "        [ 0.4650, -0.2852],\n",
            "        [ 0.4697, -0.2027],\n",
            "        [ 0.5012, -0.3532],\n",
            "        [ 0.3875, -0.3225],\n",
            "        [ 0.2567, -0.5226],\n",
            "        [-0.0017,  0.3040],\n",
            "        [ 0.4854, -0.2739],\n",
            "        [ 0.3570, -0.1678],\n",
            "        [ 0.5565, -0.3505],\n",
            "        [ 0.4929, -0.4413],\n",
            "        [ 0.3450, -0.2311],\n",
            "        [ 0.4209, -0.2860],\n",
            "        [ 0.1165,  0.2069],\n",
            "        [-0.1668,  0.5767],\n",
            "        [ 0.4376, -0.7278],\n",
            "        [-0.2189,  0.4330],\n",
            "        [ 0.3835,  0.1972],\n",
            "        [ 0.4465, -0.6239],\n",
            "        [ 0.5626, -0.5826],\n",
            "        [ 0.0527,  0.1825],\n",
            "        [ 0.1538, -0.1989],\n",
            "        [ 0.3608, -0.7609],\n",
            "        [ 0.2676, -0.0827]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4413, -0.4311],\n",
            "        [ 0.1073,  0.0619],\n",
            "        [ 0.3710, -0.3914],\n",
            "        [ 0.4352, -0.2408],\n",
            "        [ 0.0220,  0.2264],\n",
            "        [ 0.4563, -0.2562],\n",
            "        [ 0.2936, -0.1630],\n",
            "        [-0.0100,  0.2468],\n",
            "        [-0.0398,  0.4036],\n",
            "        [-0.2171,  0.4441],\n",
            "        [ 0.4176, -0.4704],\n",
            "        [ 0.2432, -0.1704],\n",
            "        [ 0.4831, -0.3861],\n",
            "        [ 0.5476, -0.4251],\n",
            "        [-0.1991,  0.5360],\n",
            "        [ 0.2762, -0.0437],\n",
            "        [-0.0527,  0.4311],\n",
            "        [ 0.5215, -0.2461],\n",
            "        [ 0.2411, -0.1075],\n",
            "        [ 0.5416, -0.7742],\n",
            "        [ 0.0751,  0.0249],\n",
            "        [ 0.6091, -0.5137],\n",
            "        [ 0.4952, -0.6251],\n",
            "        [ 0.0632,  0.2025],\n",
            "        [ 0.3003, -0.1448],\n",
            "        [ 0.3312, -0.1127],\n",
            "        [-0.0414,  0.0511],\n",
            "        [ 0.3886, -0.4173],\n",
            "        [ 0.4397, -0.4452],\n",
            "        [ 0.2485, -0.2583],\n",
            "        [ 0.6388, -0.4700],\n",
            "        [ 0.2624,  0.1152]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0317,  0.4386],\n",
            "        [ 0.4645, -0.5403],\n",
            "        [-0.1665,  0.4722],\n",
            "        [-0.1547,  0.3418],\n",
            "        [ 0.3385, -0.2258],\n",
            "        [-0.0535,  0.6568],\n",
            "        [-0.1755,  0.3289],\n",
            "        [ 0.2636, -0.1843],\n",
            "        [ 0.5697, -0.8806],\n",
            "        [ 0.2756, -0.1107],\n",
            "        [-0.0830,  0.1316],\n",
            "        [ 0.4596, -0.8345],\n",
            "        [-0.1097,  0.3045],\n",
            "        [ 0.2676, -0.6339],\n",
            "        [-0.2182,  0.6929],\n",
            "        [ 0.0083, -0.1706],\n",
            "        [ 0.1328,  0.1382],\n",
            "        [ 0.3412, -0.1098],\n",
            "        [ 0.0878, -0.2542],\n",
            "        [ 0.1243, -0.0232],\n",
            "        [-0.0212,  0.1511],\n",
            "        [ 0.2384, -0.4507],\n",
            "        [ 0.1144,  0.4206],\n",
            "        [ 0.2757, -0.2333],\n",
            "        [ 0.4853, -0.3898],\n",
            "        [ 0.0556,  0.3639],\n",
            "        [ 0.4995, -0.7713],\n",
            "        [ 0.3943, -0.2355],\n",
            "        [-0.1649,  0.3148],\n",
            "        [ 0.4344, -0.2908],\n",
            "        [ 0.3355, -0.9624],\n",
            "        [ 0.2450, -0.0011]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0937,  0.2697],\n",
            "        [ 0.4439, -0.1847],\n",
            "        [ 0.4589, -0.1922],\n",
            "        [ 0.1944,  0.0638],\n",
            "        [ 0.3980, -0.4118],\n",
            "        [-0.3296,  0.9169],\n",
            "        [ 0.1648, -0.3267],\n",
            "        [ 0.2695, -0.7013],\n",
            "        [-0.1777,  0.3959],\n",
            "        [ 0.5869, -0.4856],\n",
            "        [ 0.3344, -0.0867],\n",
            "        [ 0.2256,  0.1176],\n",
            "        [ 0.5607, -0.9750],\n",
            "        [ 0.5602, -0.1618],\n",
            "        [ 0.2092, -0.0821],\n",
            "        [ 0.5424, -0.5565],\n",
            "        [ 0.2032,  0.0420],\n",
            "        [ 0.2926, -0.1918],\n",
            "        [ 0.1950, -0.6370],\n",
            "        [ 0.4406, -0.6542],\n",
            "        [ 0.3386, -0.5478],\n",
            "        [ 0.2448,  0.0187],\n",
            "        [ 0.2631, -0.5012],\n",
            "        [ 0.5769, -0.6781],\n",
            "        [ 0.1892,  0.1279],\n",
            "        [-0.3242,  0.7387],\n",
            "        [ 0.5125, -0.2287],\n",
            "        [ 0.1260, -0.2379],\n",
            "        [ 0.6876, -0.5726],\n",
            "        [ 0.7093, -0.6210],\n",
            "        [ 0.0478,  0.3281],\n",
            "        [ 0.5095, -0.5325]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2504, -0.2442],\n",
            "        [ 0.2725,  0.0893],\n",
            "        [-0.0370,  0.2920],\n",
            "        [ 0.0359, -0.3287],\n",
            "        [ 0.3075, -0.1768],\n",
            "        [-0.2310,  0.4425],\n",
            "        [ 0.0323,  0.3511],\n",
            "        [ 0.4310, -0.6788],\n",
            "        [ 0.5057, -0.7510],\n",
            "        [ 0.4939, -0.7919],\n",
            "        [-0.3822,  0.4954],\n",
            "        [-0.0419, -0.1969],\n",
            "        [ 0.0690,  0.1335],\n",
            "        [ 0.2731, -0.1926],\n",
            "        [ 0.1589, -0.0700],\n",
            "        [-0.1722,  0.1549],\n",
            "        [-0.2427,  0.6170],\n",
            "        [ 0.6131, -0.5369],\n",
            "        [-0.2349,  0.7454],\n",
            "        [ 0.2617, -0.2733],\n",
            "        [-0.1629,  0.3638],\n",
            "        [ 0.4814, -0.5472],\n",
            "        [ 0.2148, -0.7379],\n",
            "        [ 0.2928, -0.7468],\n",
            "        [ 0.5408, -0.7540],\n",
            "        [ 0.4394, -0.0401],\n",
            "        [-0.0740,  0.3710],\n",
            "        [ 0.5057, -0.7980],\n",
            "        [ 0.4514, -0.3359],\n",
            "        [ 0.4931, -0.6102],\n",
            "        [-0.2281,  0.2642],\n",
            "        [ 0.4818, -0.3981]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2533, -0.7375],\n",
            "        [ 0.2534, -0.6574],\n",
            "        [-0.0610,  0.4320],\n",
            "        [ 0.4077, -0.5397],\n",
            "        [ 0.3555, -0.5246],\n",
            "        [-0.1656,  0.4879],\n",
            "        [ 0.1626, -0.4939],\n",
            "        [ 0.5055, -0.4545],\n",
            "        [ 0.4200, -0.6054],\n",
            "        [ 0.3974, -0.2533],\n",
            "        [ 0.1536,  0.1823],\n",
            "        [ 0.4525, -0.4385],\n",
            "        [ 0.3122, -0.5118],\n",
            "        [ 0.2247, -0.7158],\n",
            "        [-0.0966,  0.3946],\n",
            "        [ 0.1721, -0.2133],\n",
            "        [ 0.6660, -0.5005],\n",
            "        [ 0.4847, -0.6776],\n",
            "        [ 0.0549, -0.0119],\n",
            "        [-0.1844,  0.7174],\n",
            "        [-0.2440,  0.5053],\n",
            "        [ 0.5854, -0.5073],\n",
            "        [ 0.2488,  0.1901],\n",
            "        [ 0.3117, -0.7328],\n",
            "        [ 0.0648, -0.7414],\n",
            "        [ 0.4461, -0.4857],\n",
            "        [ 0.5570, -0.3583],\n",
            "        [ 0.3445, -0.2600],\n",
            "        [-0.0786,  0.2379],\n",
            "        [-0.2576,  0.2845],\n",
            "        [ 0.2667,  0.1706],\n",
            "        [ 0.3810, -0.6146]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4270, -0.5435],\n",
            "        [ 0.5985, -0.5879],\n",
            "        [ 0.5558, -0.8646],\n",
            "        [ 0.4269, -0.2491],\n",
            "        [ 0.3323, -0.3517],\n",
            "        [ 0.5612, -0.8763],\n",
            "        [ 0.2402, -0.7327],\n",
            "        [ 0.2754, -0.1168],\n",
            "        [ 0.5843, -0.5744],\n",
            "        [ 0.4246, -0.9068],\n",
            "        [ 0.5597, -0.3505],\n",
            "        [ 0.1786,  0.0642],\n",
            "        [ 0.4449, -0.1706],\n",
            "        [ 0.2678, -0.2859],\n",
            "        [-0.0667,  0.6740],\n",
            "        [ 0.2524, -0.4148],\n",
            "        [ 0.3609, -0.0660],\n",
            "        [ 0.1691, -0.0691],\n",
            "        [-0.2429,  0.2081],\n",
            "        [ 0.0070,  0.3897],\n",
            "        [ 0.6202, -0.7349],\n",
            "        [ 0.3098, -0.6638],\n",
            "        [ 0.3578, -0.3424],\n",
            "        [-0.4442,  0.4443],\n",
            "        [-0.0730,  0.4668],\n",
            "        [ 0.4814, -0.7257],\n",
            "        [ 0.5901, -0.6581],\n",
            "        [-0.0896,  0.1616],\n",
            "        [ 0.5099, -0.4509],\n",
            "        [ 0.3113, -0.5072],\n",
            "        [ 0.2207, -0.4011],\n",
            "        [-0.3842,  0.6079]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0567, -0.0348],\n",
            "        [-0.5572,  0.8854],\n",
            "        [ 0.4750, -0.5761],\n",
            "        [ 0.1615, -0.1488],\n",
            "        [ 0.1336, -0.0841],\n",
            "        [ 0.5303, -0.4057],\n",
            "        [ 0.3828, -0.4796],\n",
            "        [ 0.3766, -0.6585],\n",
            "        [ 0.3551, -0.6161],\n",
            "        [ 0.2144, -0.7606],\n",
            "        [ 0.4579, -0.0317],\n",
            "        [-0.1014,  0.3909],\n",
            "        [ 0.4580, -0.5640],\n",
            "        [ 0.4021, -0.5198],\n",
            "        [ 0.2037, -0.1529],\n",
            "        [-0.0661,  0.3601],\n",
            "        [-0.3857,  0.4440],\n",
            "        [-0.1680,  0.2288],\n",
            "        [ 0.8752, -0.3756],\n",
            "        [ 0.0635, -0.1035],\n",
            "        [-0.3225,  0.4969],\n",
            "        [-0.2045,  0.5870],\n",
            "        [ 0.0914,  0.1414],\n",
            "        [ 0.1453,  0.2393],\n",
            "        [ 0.2609, -0.6041],\n",
            "        [ 0.3931, -0.4290],\n",
            "        [ 0.0188, -0.4373],\n",
            "        [ 0.1676, -0.6313],\n",
            "        [-0.4203,  0.8728],\n",
            "        [ 0.5075, -0.6207],\n",
            "        [ 0.1241, -0.7024],\n",
            "        [ 0.5996, -0.5795]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2313, -0.1640],\n",
            "        [ 0.6213, -0.1947],\n",
            "        [ 0.5846, -0.7045],\n",
            "        [-0.2084,  0.2710],\n",
            "        [ 0.3502, -0.0499],\n",
            "        [ 0.2909, -0.3494],\n",
            "        [ 0.1924, -0.5991],\n",
            "        [-0.0398,  0.1938],\n",
            "        [ 0.3096, -0.4348],\n",
            "        [-0.1079,  0.5319],\n",
            "        [ 0.4806, -0.4399],\n",
            "        [-0.0247,  0.3844],\n",
            "        [-0.1377,  0.5433],\n",
            "        [-0.1067,  0.2080],\n",
            "        [ 0.0696, -0.0040],\n",
            "        [-0.1639,  0.1290],\n",
            "        [ 0.4964, -0.3322],\n",
            "        [ 0.1308, -0.3370],\n",
            "        [ 0.5582, -0.5890],\n",
            "        [ 0.1713,  0.3405],\n",
            "        [ 0.2054, -0.7426],\n",
            "        [-0.2020,  0.2533],\n",
            "        [-0.1031,  0.5424],\n",
            "        [ 0.5083, -0.3944],\n",
            "        [ 0.5768, -0.6612],\n",
            "        [ 0.1843, -0.1458],\n",
            "        [ 0.3712, -0.7356],\n",
            "        [ 0.2852, -0.0099],\n",
            "        [-0.0796, -0.0087],\n",
            "        [ 0.1451, -0.1355],\n",
            "        [-0.3056,  0.5309],\n",
            "        [ 0.1235, -0.6480]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4881, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2580, -0.6113],\n",
            "        [ 0.3624, -0.5006],\n",
            "        [ 0.6563, -0.6712],\n",
            "        [ 0.2602, -0.3858],\n",
            "        [ 0.5977, -0.9166],\n",
            "        [ 0.1578, -0.1369],\n",
            "        [-0.1616,  0.3701],\n",
            "        [-0.4559,  0.6322],\n",
            "        [-0.3069,  0.4669],\n",
            "        [-0.1823,  0.5066],\n",
            "        [ 0.2829, -0.7803],\n",
            "        [-0.4838,  0.4927],\n",
            "        [-0.4873,  0.6274],\n",
            "        [-0.2774,  0.3344],\n",
            "        [ 0.0693, -0.0163],\n",
            "        [ 0.4566, -0.3910],\n",
            "        [-0.3028,  0.4877],\n",
            "        [ 0.2538, -0.5846],\n",
            "        [ 0.4484, -0.6635],\n",
            "        [-0.1826,  0.4041],\n",
            "        [ 0.4190, -0.0657],\n",
            "        [ 0.4873, -0.8078],\n",
            "        [ 0.4789, -0.1923],\n",
            "        [ 0.4195, -0.6626],\n",
            "        [-0.0097,  0.1159],\n",
            "        [-0.3256,  0.5933],\n",
            "        [-0.1332,  0.4352],\n",
            "        [-0.5405,  0.7045],\n",
            "        [ 0.1140,  0.3950],\n",
            "        [ 0.1836, -0.5650],\n",
            "        [-0.3068,  0.6039],\n",
            "        [ 0.5471, -0.6863]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5579, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3609,  0.5378],\n",
            "        [ 0.2646,  0.0385],\n",
            "        [ 0.4599, -0.5233],\n",
            "        [ 0.3723, -0.6918],\n",
            "        [-0.3399,  0.6373],\n",
            "        [-0.0210, -0.4426],\n",
            "        [ 0.5077, -0.6691],\n",
            "        [ 0.6161, -0.4987],\n",
            "        [ 0.6535, -0.7651],\n",
            "        [ 0.6128, -0.8631],\n",
            "        [ 0.4523, -0.6877],\n",
            "        [ 0.1214,  0.1125],\n",
            "        [ 0.3875, -0.6268],\n",
            "        [-0.3287,  0.5979],\n",
            "        [ 0.3380, -0.3732],\n",
            "        [ 0.2350, -0.1261],\n",
            "        [-0.4306,  0.6272],\n",
            "        [ 0.4875, -0.6318],\n",
            "        [-0.1119,  0.4979],\n",
            "        [ 0.0522, -0.6646],\n",
            "        [-0.2824,  0.4425],\n",
            "        [-0.0079,  0.3012],\n",
            "        [ 0.5999, -0.4447],\n",
            "        [-0.2868,  0.7131],\n",
            "        [-0.3769,  0.4132],\n",
            "        [ 0.5522, -0.7311],\n",
            "        [-0.0886,  0.2709],\n",
            "        [-0.0204, -0.2077],\n",
            "        [ 0.3862, -0.7223],\n",
            "        [-0.2580,  0.3954],\n",
            "        [ 0.2464, -0.5968],\n",
            "        [ 0.0770, -0.2635]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5111, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1631, -0.1954],\n",
            "        [ 0.5104, -0.7552],\n",
            "        [-0.0203,  0.3632],\n",
            "        [-0.0163, -0.1099],\n",
            "        [ 0.0561, -0.1756],\n",
            "        [ 0.4209, -0.6814],\n",
            "        [ 0.3581, -0.4590],\n",
            "        [-0.5148,  0.8053],\n",
            "        [ 0.4007, -0.5490],\n",
            "        [ 0.2195, -0.2568],\n",
            "        [ 0.5332, -0.5767],\n",
            "        [ 0.4370, -0.2442],\n",
            "        [ 0.5792, -0.5920],\n",
            "        [ 0.3309, -0.5424],\n",
            "        [ 0.5095, -0.0177],\n",
            "        [ 0.5126, -0.4683],\n",
            "        [ 0.5470, -0.5718],\n",
            "        [ 0.2559, -0.1057],\n",
            "        [ 0.2756, -0.0849],\n",
            "        [-0.2286,  0.4737],\n",
            "        [ 0.4699, -0.4250],\n",
            "        [ 0.4199, -0.2542],\n",
            "        [ 0.4305, -0.2216],\n",
            "        [-0.1125,  0.2565],\n",
            "        [ 0.6056, -0.6007],\n",
            "        [ 0.2780, -0.3642],\n",
            "        [ 0.6619, -0.3777],\n",
            "        [ 0.5668, -0.6068],\n",
            "        [ 0.2521, -0.4724],\n",
            "        [ 0.4034, -0.6956],\n",
            "        [ 0.5312, -0.5423],\n",
            "        [ 0.7432, -0.7624]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3244, -0.6527],\n",
            "        [-0.5227,  0.5373],\n",
            "        [ 0.4573, -0.4639],\n",
            "        [-0.4058,  0.6551],\n",
            "        [ 0.2183, -0.3049],\n",
            "        [-0.4606,  0.3960],\n",
            "        [ 0.2607, -0.2426],\n",
            "        [ 0.1856,  0.1245],\n",
            "        [ 0.2364, -0.1311],\n",
            "        [-0.2705,  0.6908],\n",
            "        [ 0.2499, -0.4255],\n",
            "        [ 0.3776, -0.6646],\n",
            "        [-0.4019,  0.5746],\n",
            "        [ 0.2075,  0.0776],\n",
            "        [ 0.4546, -0.8739],\n",
            "        [ 0.4448, -0.7129],\n",
            "        [ 0.0827,  0.2028],\n",
            "        [-0.1608,  0.2411],\n",
            "        [-0.3300,  0.5866],\n",
            "        [ 0.3841, -0.8174],\n",
            "        [ 0.4979, -0.3965],\n",
            "        [ 0.1030,  0.0452],\n",
            "        [ 0.5414, -0.4892],\n",
            "        [-0.5373,  0.7428],\n",
            "        [ 0.3700, -0.6349],\n",
            "        [ 0.1760, -0.2321],\n",
            "        [ 0.5514, -0.5697],\n",
            "        [-0.6544,  0.6880],\n",
            "        [ 0.3922, -0.1565],\n",
            "        [-0.5576,  0.6230],\n",
            "        [-0.0256,  0.6311],\n",
            "        [ 0.0894, -0.1037]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4416,  0.6588],\n",
            "        [ 0.2662, -0.6756],\n",
            "        [ 0.3772, -0.3215],\n",
            "        [ 0.2024, -0.9249],\n",
            "        [ 0.4057, -0.6198],\n",
            "        [ 0.4238, -0.7930],\n",
            "        [ 0.3954, -0.7026],\n",
            "        [ 0.4695, -0.5146],\n",
            "        [ 0.4451, -0.4762],\n",
            "        [ 0.3493, -0.6568],\n",
            "        [ 0.6798, -0.2462],\n",
            "        [ 0.0728, -0.8235],\n",
            "        [ 0.5107, -0.7040],\n",
            "        [-0.4034,  0.6472],\n",
            "        [ 0.3093, -0.0159],\n",
            "        [ 0.5362, -0.7730],\n",
            "        [ 0.6263, -0.5212],\n",
            "        [ 0.4838, -0.8568],\n",
            "        [ 0.3552, -0.1508],\n",
            "        [-0.1801,  0.4271],\n",
            "        [ 0.2949, -0.2516],\n",
            "        [-0.5201,  0.6132],\n",
            "        [-0.0729,  0.2086],\n",
            "        [-0.2622,  0.4158],\n",
            "        [-0.2469,  0.2059],\n",
            "        [ 0.1239,  0.0338],\n",
            "        [ 0.4136, -0.7945],\n",
            "        [ 0.3837, -0.4750],\n",
            "        [ 0.2690, -0.4689],\n",
            "        [ 0.4362, -0.3555],\n",
            "        [ 0.3615, -0.4723],\n",
            "        [-0.4474,  0.5709]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5357, -0.5444],\n",
            "        [ 0.4262, -0.8344],\n",
            "        [ 0.6109, -0.6368],\n",
            "        [ 0.0244, -0.6563],\n",
            "        [ 0.3858, -0.3532],\n",
            "        [ 0.0036,  0.5046],\n",
            "        [ 0.4106, -0.5870],\n",
            "        [-0.3055,  0.5516],\n",
            "        [ 0.2264, -0.0524],\n",
            "        [ 0.0713,  0.0030],\n",
            "        [ 0.1476, -0.1514],\n",
            "        [ 0.3371, -0.1213],\n",
            "        [ 0.5962, -0.8639],\n",
            "        [-0.2848,  0.6555],\n",
            "        [-0.1258,  0.3167],\n",
            "        [ 0.1597,  0.2457],\n",
            "        [ 0.1167, -0.3021],\n",
            "        [ 0.1050, -0.3723],\n",
            "        [ 0.4696, -0.3507],\n",
            "        [ 0.5448, -0.5069],\n",
            "        [ 0.1379, -0.3127],\n",
            "        [ 0.4170, -0.3870],\n",
            "        [-0.1104,  0.6137],\n",
            "        [ 0.2655, -0.4270],\n",
            "        [ 0.4022, -0.6510],\n",
            "        [-0.4601,  0.5953],\n",
            "        [ 0.4708, -0.0589],\n",
            "        [-0.3898,  0.5839],\n",
            "        [ 0.0723, -0.2170],\n",
            "        [ 0.2437,  0.0271],\n",
            "        [-0.3749,  0.5518],\n",
            "        [ 0.4523, -0.5864]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2313, -0.4672],\n",
            "        [ 0.4002, -0.4884],\n",
            "        [ 0.4057, -0.5935],\n",
            "        [ 0.3867, -0.3287],\n",
            "        [-0.1940,  0.6549],\n",
            "        [-0.5458,  0.7767],\n",
            "        [ 0.1922, -0.6324],\n",
            "        [-0.1971,  0.3896],\n",
            "        [ 0.4687, -0.4599],\n",
            "        [-0.1030, -0.1172],\n",
            "        [-0.3228,  0.3168],\n",
            "        [ 0.0532, -0.2794],\n",
            "        [ 0.0602, -0.5411],\n",
            "        [-0.1101,  0.6777],\n",
            "        [-0.2710,  0.2962],\n",
            "        [ 0.2554, -0.7566],\n",
            "        [-0.3350,  0.6975],\n",
            "        [ 0.3835, -0.3868],\n",
            "        [-0.2904,  0.2347],\n",
            "        [-0.4458,  0.5431],\n",
            "        [ 0.4574, -0.6583],\n",
            "        [ 0.5807, -0.2377],\n",
            "        [ 0.5183, -0.5898],\n",
            "        [ 0.3617, -0.6365],\n",
            "        [-0.0253, -0.1946],\n",
            "        [ 0.2801, -0.2961],\n",
            "        [-0.3966,  0.2540],\n",
            "        [ 0.7578, -0.7159],\n",
            "        [ 0.3017, -0.8687],\n",
            "        [ 0.6462, -0.2906],\n",
            "        [ 0.3295, -0.4988],\n",
            "        [ 0.3389, -0.6634]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4423, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6512, -0.1284],\n",
            "        [ 0.5723, -0.5843],\n",
            "        [-0.1006,  0.3500],\n",
            "        [ 0.1629, -0.3288],\n",
            "        [ 0.4640, -0.5237],\n",
            "        [-0.1260,  0.4145],\n",
            "        [-0.6064,  0.7198],\n",
            "        [ 0.5003, -0.7580],\n",
            "        [-0.1235,  0.5246],\n",
            "        [ 0.2845, -0.3603],\n",
            "        [ 0.5093, -0.8204],\n",
            "        [ 0.4036, -0.7012],\n",
            "        [ 0.0999,  0.0626],\n",
            "        [-0.4464,  0.8490],\n",
            "        [ 0.4572, -0.5339],\n",
            "        [ 0.2373, -0.1363],\n",
            "        [-0.3682,  0.5502],\n",
            "        [ 0.2413, -0.3780],\n",
            "        [ 0.3935, -0.5310],\n",
            "        [-0.2715,  0.4168],\n",
            "        [ 0.4677, -1.0804],\n",
            "        [ 0.4790, -0.6772],\n",
            "        [ 0.1073, -0.3344],\n",
            "        [-0.1445, -0.2284],\n",
            "        [ 0.4184, -0.3902],\n",
            "        [-0.1337,  0.4781],\n",
            "        [ 0.3703, -0.4768],\n",
            "        [ 0.4475, -0.6681],\n",
            "        [-0.4524,  0.8488],\n",
            "        [ 0.5415, -0.6433],\n",
            "        [-0.3057,  0.7525],\n",
            "        [ 0.0432,  0.0585]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1202,  0.1176],\n",
            "        [-0.1102,  0.0387],\n",
            "        [ 0.3310, -0.3348],\n",
            "        [-0.0240, -0.2279],\n",
            "        [ 0.2993, -0.7030],\n",
            "        [ 0.7039, -0.8282],\n",
            "        [ 0.3792, -0.5980],\n",
            "        [ 0.3560, -0.7900],\n",
            "        [-0.3782,  0.5633],\n",
            "        [ 0.1215, -0.5966],\n",
            "        [ 0.1075,  0.2088],\n",
            "        [ 0.6792, -0.9703],\n",
            "        [-0.2783,  0.7924],\n",
            "        [ 0.3228, -0.4775],\n",
            "        [ 0.3394, -0.6854],\n",
            "        [-0.0341,  0.4151],\n",
            "        [ 0.4127, -0.1449],\n",
            "        [-0.6070,  0.7861],\n",
            "        [ 0.4430, -0.5094],\n",
            "        [ 0.3010, -0.3828],\n",
            "        [-0.0910, -0.1456],\n",
            "        [ 0.5166, -0.6770],\n",
            "        [-0.0086, -0.0830],\n",
            "        [ 0.3496, -0.4200],\n",
            "        [ 0.1007, -0.1096],\n",
            "        [ 0.2327, -0.6634],\n",
            "        [-0.4254,  0.7527],\n",
            "        [ 0.0888, -0.0242],\n",
            "        [ 0.5352, -0.4451],\n",
            "        [ 0.3547, -0.5286],\n",
            "        [-0.3813,  0.7215],\n",
            "        [ 0.1685, -0.2317]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4874, -0.7951],\n",
            "        [ 0.2665, -0.4931],\n",
            "        [-0.4750,  0.6174],\n",
            "        [ 0.4113, -0.3658],\n",
            "        [ 0.4849, -0.4162],\n",
            "        [ 0.5112, -0.6062],\n",
            "        [-0.4455,  0.6858],\n",
            "        [ 0.5115, -0.3581],\n",
            "        [-0.5465,  0.6715],\n",
            "        [ 0.1109,  0.0467],\n",
            "        [ 0.0818, -0.1875],\n",
            "        [-0.6088,  0.9213],\n",
            "        [ 0.4469, -0.6734],\n",
            "        [ 0.1618, -0.5542],\n",
            "        [-0.2603,  0.2576],\n",
            "        [ 0.4987, -0.4794],\n",
            "        [ 0.3542, -0.6539],\n",
            "        [ 0.1425,  0.1950],\n",
            "        [ 0.4229, -0.6218],\n",
            "        [ 0.1088, -0.6265],\n",
            "        [ 0.6580, -0.7747],\n",
            "        [ 0.1124,  0.3432],\n",
            "        [-0.1852,  0.3615],\n",
            "        [-0.2383,  0.7306],\n",
            "        [ 0.3247, -0.1252],\n",
            "        [-0.3335,  0.3312],\n",
            "        [ 0.5967, -0.3517],\n",
            "        [-0.2916,  0.4253],\n",
            "        [-0.6460,  1.0357],\n",
            "        [ 0.4496, -0.6749],\n",
            "        [ 0.0140,  0.0723],\n",
            "        [ 0.6854, -0.6705]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5093,  0.9486],\n",
            "        [ 0.5963, -0.6335],\n",
            "        [ 0.2427, -0.4637],\n",
            "        [ 0.4113, -0.7996],\n",
            "        [ 0.2684, -0.3428],\n",
            "        [-0.4468,  0.7951],\n",
            "        [ 0.1995, -0.2969],\n",
            "        [-0.4033,  0.6268],\n",
            "        [-0.1332,  0.0177],\n",
            "        [-0.3855,  0.0355],\n",
            "        [ 0.7084, -0.8753],\n",
            "        [-0.3007,  0.8628],\n",
            "        [ 0.3139, -0.0701],\n",
            "        [ 0.3347, -0.2968],\n",
            "        [ 0.4399, -0.5971],\n",
            "        [ 0.1899, -0.7534],\n",
            "        [-0.3820,  0.4184],\n",
            "        [ 0.3208, -0.6701],\n",
            "        [ 0.4914, -0.6136],\n",
            "        [ 0.0258, -0.2805],\n",
            "        [ 0.3182, -0.4221],\n",
            "        [-0.4794,  0.7430],\n",
            "        [-0.2493,  0.5185],\n",
            "        [ 0.3448, -0.5168],\n",
            "        [ 0.4070, -0.7249],\n",
            "        [-0.7009,  0.7826],\n",
            "        [ 0.3256, -0.5200],\n",
            "        [ 0.2671, -0.1956],\n",
            "        [ 0.2720, -0.7921],\n",
            "        [ 0.1405, -0.5566],\n",
            "        [-0.2552,  0.5715],\n",
            "        [-0.3458,  0.4049]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5756, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6147,  0.6856],\n",
            "        [ 0.6458, -0.7785],\n",
            "        [ 0.2443, -0.3186],\n",
            "        [ 0.2027, -0.0714],\n",
            "        [ 0.2410, -0.5119],\n",
            "        [ 0.5483, -0.4911],\n",
            "        [-0.0813, -0.2190],\n",
            "        [ 0.2638, -0.3506],\n",
            "        [ 0.4176, -0.0111],\n",
            "        [ 0.2234, -0.2997],\n",
            "        [-0.1810,  0.3743],\n",
            "        [ 0.4449, -0.7877],\n",
            "        [ 0.1323,  0.2819],\n",
            "        [ 0.3407, -0.1616],\n",
            "        [ 0.4266, -0.5345],\n",
            "        [ 0.2887, -0.4568],\n",
            "        [-0.3390,  0.3469],\n",
            "        [-0.2534,  0.2911],\n",
            "        [ 0.4465, -0.6608],\n",
            "        [-0.6068,  0.3964],\n",
            "        [ 0.5466, -0.7777],\n",
            "        [ 0.5296, -0.6419],\n",
            "        [-0.7443,  0.7278],\n",
            "        [-0.0771,  0.0178],\n",
            "        [ 0.2652, -0.4917],\n",
            "        [ 0.6209, -0.6570],\n",
            "        [ 0.2350, -0.6497],\n",
            "        [-0.2580,  0.5519],\n",
            "        [ 0.0488,  0.3892],\n",
            "        [ 0.2846, -0.4108],\n",
            "        [ 0.1296, -0.1531],\n",
            "        [-0.5855,  0.4855]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6252, -0.6571],\n",
            "        [ 0.5029, -0.1632],\n",
            "        [ 0.6975, -0.7578],\n",
            "        [ 0.3788, -0.6094],\n",
            "        [ 0.2345, -0.3717],\n",
            "        [-0.2215,  0.3462],\n",
            "        [ 0.1542,  0.0596],\n",
            "        [ 0.4135, -0.5180],\n",
            "        [ 0.3523, -0.9083],\n",
            "        [-0.4763,  0.5315],\n",
            "        [ 0.5471, -0.9032],\n",
            "        [ 0.2706, -0.8582],\n",
            "        [ 0.2840, -0.6437],\n",
            "        [ 0.5915, -0.7364],\n",
            "        [-0.3653,  0.8250],\n",
            "        [-0.5988,  0.4302],\n",
            "        [-0.5344,  0.6459],\n",
            "        [-0.6630,  0.5654],\n",
            "        [ 0.5178, -0.4914],\n",
            "        [-0.5025,  0.8741],\n",
            "        [ 0.1418, -0.5269],\n",
            "        [-0.1795,  0.1899],\n",
            "        [ 0.1153, -0.4020],\n",
            "        [ 0.3192,  0.1985],\n",
            "        [ 0.1019, -0.4407],\n",
            "        [-0.3462,  0.6884],\n",
            "        [ 0.6068, -0.5140],\n",
            "        [ 0.5002, -0.7639],\n",
            "        [-0.5407,  0.9429],\n",
            "        [ 0.2748, -0.3786],\n",
            "        [ 0.0441, -0.3392],\n",
            "        [-0.4761,  0.8985]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4692, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4690,  0.8588],\n",
            "        [ 0.4939, -0.4881],\n",
            "        [ 0.4953, -0.4024],\n",
            "        [ 0.2460, -0.6936],\n",
            "        [ 0.4172, -0.7193],\n",
            "        [-0.5325,  0.5512],\n",
            "        [ 0.6315, -0.7454],\n",
            "        [ 0.3619, -0.3615],\n",
            "        [-0.3945,  0.4594],\n",
            "        [ 0.3113, -0.5156],\n",
            "        [ 0.2795, -0.2922],\n",
            "        [ 0.3771, -0.4718],\n",
            "        [-0.5308,  0.9793],\n",
            "        [-0.5453,  0.4527],\n",
            "        [ 0.5577, -0.1888],\n",
            "        [-0.8877,  1.0213],\n",
            "        [ 0.2634, -0.1614],\n",
            "        [-0.4217,  0.6656],\n",
            "        [-0.1321,  0.0298],\n",
            "        [ 0.5768, -0.5919],\n",
            "        [ 0.4196, -0.2495],\n",
            "        [ 0.4066, -0.4781],\n",
            "        [ 0.2126,  0.0463],\n",
            "        [-0.5835,  0.8840],\n",
            "        [-0.2212,  0.2022],\n",
            "        [-0.5318,  0.6367],\n",
            "        [ 0.1935, -0.3621],\n",
            "        [ 0.5906, -0.7045],\n",
            "        [-0.2588,  0.7456],\n",
            "        [ 0.2375, -0.7702],\n",
            "        [ 0.3986, -0.5204],\n",
            "        [ 0.3884, -0.7695]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1069, -0.0733],\n",
            "        [ 0.5913, -0.8445],\n",
            "        [ 0.0672, -0.2952],\n",
            "        [ 0.1942, -0.2746],\n",
            "        [ 0.4605, -0.1126],\n",
            "        [ 0.3160, -0.3564],\n",
            "        [ 0.1037, -0.0996],\n",
            "        [ 0.3477, -0.5452],\n",
            "        [ 0.1382, -0.1561],\n",
            "        [ 0.1825, -0.3771],\n",
            "        [-0.0720,  0.2142],\n",
            "        [ 0.3475, -0.6501],\n",
            "        [ 0.0502, -0.2153],\n",
            "        [ 0.6920, -0.5870],\n",
            "        [ 0.5207, -0.8689],\n",
            "        [ 0.3232, -0.4392],\n",
            "        [-0.6138,  1.0275],\n",
            "        [ 0.4456, -0.4696],\n",
            "        [ 0.2725, -0.3465],\n",
            "        [ 0.3539, -0.2728],\n",
            "        [ 0.2836, -0.5294],\n",
            "        [-0.0434,  0.3903],\n",
            "        [ 0.2025,  0.0473],\n",
            "        [-0.7243,  0.6539],\n",
            "        [ 0.0178,  0.2768],\n",
            "        [ 0.6152, -0.5900],\n",
            "        [-0.9015,  0.6257],\n",
            "        [ 0.6089, -0.4428],\n",
            "        [-0.2500,  0.1014],\n",
            "        [-0.1093,  0.2069],\n",
            "        [ 0.5208, -0.4367],\n",
            "        [-0.6139,  0.5654]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2717, -0.6400],\n",
            "        [ 0.0108, -0.2413],\n",
            "        [-0.1716,  0.2290],\n",
            "        [ 0.4852, -0.7106],\n",
            "        [-0.1360,  0.5802],\n",
            "        [-0.6102,  0.7876],\n",
            "        [ 0.0353,  0.3141],\n",
            "        [ 0.1600, -0.5149],\n",
            "        [ 0.1761,  0.0135],\n",
            "        [-0.0419, -0.6843],\n",
            "        [ 0.1909, -0.1448],\n",
            "        [ 0.4555, -0.4966],\n",
            "        [-0.4411,  0.8792],\n",
            "        [-0.1127,  0.5899],\n",
            "        [-0.0287, -0.0543],\n",
            "        [-0.0396, -0.0340],\n",
            "        [-0.3889,  0.5556],\n",
            "        [ 0.3665, -0.5322],\n",
            "        [ 0.0083, -0.6080],\n",
            "        [ 0.5270, -0.5529],\n",
            "        [-0.4771,  0.9517],\n",
            "        [-0.5732,  0.8494],\n",
            "        [ 0.4456, -0.8557],\n",
            "        [-0.0377,  0.3809],\n",
            "        [-0.2630,  0.5222],\n",
            "        [-0.3442,  0.6553],\n",
            "        [-0.5362,  0.9469],\n",
            "        [-0.2955,  0.5690],\n",
            "        [-0.0851,  0.2120],\n",
            "        [-0.2799,  0.2037],\n",
            "        [-0.5666,  0.7853],\n",
            "        [-0.0534,  0.2417]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0936,  0.1816],\n",
            "        [ 0.0030,  0.1410],\n",
            "        [ 0.0693, -0.1653],\n",
            "        [ 0.1478, -0.0826],\n",
            "        [-0.3548, -0.0258],\n",
            "        [ 0.3447, -0.6599],\n",
            "        [-0.5122,  0.1559],\n",
            "        [-0.4957,  0.3171],\n",
            "        [ 0.1272, -0.4789],\n",
            "        [ 0.6168, -0.6204],\n",
            "        [-0.2865,  0.3055],\n",
            "        [ 0.0599,  0.0075],\n",
            "        [ 0.6588, -0.3845],\n",
            "        [ 0.6145, -0.5970],\n",
            "        [ 0.3062, -0.3468],\n",
            "        [ 0.3071, -0.5075],\n",
            "        [-0.3543,  0.8797],\n",
            "        [ 0.1306,  0.1251],\n",
            "        [-0.0412,  0.0387],\n",
            "        [ 0.0256, -0.8060],\n",
            "        [ 0.0377, -0.0370],\n",
            "        [ 0.2100, -0.5471],\n",
            "        [-0.8917,  1.0829],\n",
            "        [ 0.2407, -0.0464],\n",
            "        [ 0.0121,  0.1650],\n",
            "        [-0.3720,  0.8053],\n",
            "        [ 0.5080, -0.5079],\n",
            "        [ 0.1452, -0.1899],\n",
            "        [ 0.2624, -0.5030],\n",
            "        [ 0.4864, -0.7424],\n",
            "        [ 0.3208, -0.7862],\n",
            "        [-0.6609,  0.8401]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch    50  of    191.    Elapsed: 0:01:14.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2165, -0.3751],\n",
            "        [ 0.0113, -0.2214],\n",
            "        [ 0.3154, -0.2793],\n",
            "        [ 0.1299, -0.4329],\n",
            "        [-0.5698,  0.6654],\n",
            "        [ 0.0906, -0.2192],\n",
            "        [-0.5031,  0.7461],\n",
            "        [-0.2155,  0.6395],\n",
            "        [-0.3629,  0.7465],\n",
            "        [ 0.0256, -0.0296],\n",
            "        [-0.1656,  0.6197],\n",
            "        [-0.3987,  0.8828],\n",
            "        [-0.6529,  0.5787],\n",
            "        [-0.2764,  0.0461],\n",
            "        [-0.5780,  0.7850],\n",
            "        [-0.5531,  0.9379],\n",
            "        [-0.6399,  0.9044],\n",
            "        [-0.2866,  0.3904],\n",
            "        [ 0.0823, -0.2725],\n",
            "        [-0.3220,  0.6584],\n",
            "        [-0.4578,  0.8778],\n",
            "        [-0.3801,  0.7412],\n",
            "        [ 0.5051, -0.5398],\n",
            "        [ 0.4048, -0.4279],\n",
            "        [ 0.1106,  0.3240],\n",
            "        [ 0.5302, -0.7105],\n",
            "        [-0.6775,  0.7369],\n",
            "        [ 0.0964,  0.0502],\n",
            "        [ 0.2445, -0.2163],\n",
            "        [-0.4952,  0.2357],\n",
            "        [ 0.6000, -0.4684],\n",
            "        [-0.6611,  0.7420]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3669,  0.9541],\n",
            "        [ 0.2945, -0.5459],\n",
            "        [-0.3041, -0.1005],\n",
            "        [ 0.4735, -0.7185],\n",
            "        [ 0.3634, -0.6261],\n",
            "        [ 0.0841, -0.2128],\n",
            "        [-0.7507,  0.7886],\n",
            "        [ 0.1856, -0.1226],\n",
            "        [ 0.3678, -0.5288],\n",
            "        [ 0.3153, -0.6160],\n",
            "        [-0.4488,  0.7640],\n",
            "        [-0.3727,  0.5247],\n",
            "        [-0.2144,  0.4027],\n",
            "        [-0.1063,  0.1626],\n",
            "        [-0.4801,  0.5207],\n",
            "        [-0.4578,  0.3996],\n",
            "        [ 0.1847, -0.5986],\n",
            "        [ 0.3311, -0.2983],\n",
            "        [ 0.0497, -0.1033],\n",
            "        [ 0.3878, -0.2720],\n",
            "        [-0.0749,  0.4529],\n",
            "        [ 0.1515, -0.1602],\n",
            "        [ 0.1432, -0.0911],\n",
            "        [ 0.4509, -0.2790],\n",
            "        [ 0.1568, -0.2840],\n",
            "        [ 0.5469, -0.4142],\n",
            "        [ 0.4914,  0.1517],\n",
            "        [ 0.3632, -0.3535],\n",
            "        [-0.1582,  0.1915],\n",
            "        [ 0.3805, -0.7169],\n",
            "        [ 0.6847, -0.6078],\n",
            "        [-0.7621,  0.9823]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4279, -0.9719],\n",
            "        [ 0.3330, -0.2888],\n",
            "        [-0.5556,  0.6464],\n",
            "        [ 0.5600, -0.2189],\n",
            "        [ 0.3490, -0.2796],\n",
            "        [-0.3589,  0.4246],\n",
            "        [-0.5857,  0.6955],\n",
            "        [-0.6370,  0.5871],\n",
            "        [-0.2605,  0.8358],\n",
            "        [ 0.5461, -0.6434],\n",
            "        [-0.1029,  0.4504],\n",
            "        [-0.6673,  0.9069],\n",
            "        [-0.3337,  0.3901],\n",
            "        [ 0.1956, -0.3641],\n",
            "        [-0.6964,  0.9879],\n",
            "        [-0.5836,  0.4338],\n",
            "        [ 0.4671, -0.8195],\n",
            "        [ 0.1201,  0.1615],\n",
            "        [ 0.2198, -0.6069],\n",
            "        [ 0.4498, -0.4229],\n",
            "        [-0.5519,  0.7307],\n",
            "        [ 0.4471, -0.6253],\n",
            "        [ 0.3683, -0.7713],\n",
            "        [ 0.0430, -0.6421],\n",
            "        [-0.1404,  0.3624],\n",
            "        [ 0.0120, -0.4434],\n",
            "        [ 0.4665, -0.5447],\n",
            "        [-0.4323,  0.6273],\n",
            "        [ 0.1132, -0.2162],\n",
            "        [-0.4486,  0.6638],\n",
            "        [ 0.4588, -0.6344],\n",
            "        [ 0.3994, -0.0688]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3804, -0.3055],\n",
            "        [ 0.3717, -0.5814],\n",
            "        [ 0.4599, -0.8559],\n",
            "        [ 0.5752, -0.5276],\n",
            "        [-0.4389,  0.6010],\n",
            "        [-0.5567,  0.8429],\n",
            "        [ 0.4824, -0.6345],\n",
            "        [-0.8161,  0.9458],\n",
            "        [ 0.2308, -0.3478],\n",
            "        [ 0.1420, -0.5045],\n",
            "        [ 0.1587, -0.2059],\n",
            "        [-0.1269,  0.1550],\n",
            "        [ 0.5362, -0.2396],\n",
            "        [ 0.2786, -0.2470],\n",
            "        [-0.0021, -0.1287],\n",
            "        [ 0.2301, -0.3682],\n",
            "        [ 0.5235, -0.6824],\n",
            "        [-0.4110,  0.7159],\n",
            "        [ 0.2490, -0.2352],\n",
            "        [ 0.2502, -0.2417],\n",
            "        [ 0.6001, -0.7983],\n",
            "        [ 0.3358, -0.2532],\n",
            "        [-0.5660,  0.9750],\n",
            "        [ 0.2870, -0.4629],\n",
            "        [ 0.1469, -0.7863],\n",
            "        [-0.8924,  0.8267],\n",
            "        [ 0.4916, -0.5118],\n",
            "        [ 0.1818, -0.3637],\n",
            "        [ 0.2879, -0.3463],\n",
            "        [ 0.5951, -0.6932],\n",
            "        [ 0.2210, -0.4093],\n",
            "        [ 0.0332, -0.1717]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2736,  0.8370],\n",
            "        [-0.1439, -0.0814],\n",
            "        [-0.3314,  0.7643],\n",
            "        [-0.2953,  0.0835],\n",
            "        [ 0.3279, -0.5931],\n",
            "        [-0.1263,  0.0401],\n",
            "        [-0.3247,  0.4370],\n",
            "        [-0.4811,  0.7895],\n",
            "        [ 0.6032, -0.9002],\n",
            "        [-0.5702,  0.8840],\n",
            "        [ 0.6850, -0.6841],\n",
            "        [ 0.0712,  0.0716],\n",
            "        [-0.6209,  0.9619],\n",
            "        [-0.4379,  1.1986],\n",
            "        [ 0.5021, -0.7419],\n",
            "        [-0.4822,  0.8081],\n",
            "        [ 0.6532, -0.7734],\n",
            "        [-0.7376,  1.0421],\n",
            "        [ 0.2430, -0.7156],\n",
            "        [-0.9613,  0.7709],\n",
            "        [ 0.4531, -0.5738],\n",
            "        [ 0.2898, -0.5932],\n",
            "        [-0.0408,  0.5204],\n",
            "        [ 0.4511, -0.7928],\n",
            "        [-0.2035,  0.3102],\n",
            "        [ 0.6716, -0.7508],\n",
            "        [ 0.3643, -0.6881],\n",
            "        [ 0.0259,  0.3401],\n",
            "        [-0.4445,  0.3379],\n",
            "        [ 0.5786, -0.8052],\n",
            "        [-0.6635,  0.8593],\n",
            "        [-0.6733,  0.7607]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4811, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1035, -0.0341],\n",
            "        [ 0.2937, -0.3600],\n",
            "        [ 0.0969, -0.5596],\n",
            "        [ 0.0842, -0.1615],\n",
            "        [-0.7371,  1.1010],\n",
            "        [ 0.3125, -0.4887],\n",
            "        [ 0.1943, -0.5585],\n",
            "        [-0.8338,  0.8504],\n",
            "        [ 0.3029, -0.6124],\n",
            "        [ 0.5918, -0.4184],\n",
            "        [-0.2355,  0.5652],\n",
            "        [-0.1614,  0.0525],\n",
            "        [ 0.4138, -0.7330],\n",
            "        [ 0.2941, -0.7477],\n",
            "        [ 0.1209, -0.6511],\n",
            "        [ 0.2091, -0.2010],\n",
            "        [-0.0761, -0.2052],\n",
            "        [ 0.5518, -0.6774],\n",
            "        [-0.6772,  0.7697],\n",
            "        [-0.3990,  0.5386],\n",
            "        [ 0.3056, -0.5741],\n",
            "        [ 0.0742, -0.4730],\n",
            "        [ 0.4728, -0.3159],\n",
            "        [-0.3886,  0.5425],\n",
            "        [ 0.2819, -0.3701],\n",
            "        [ 0.7188, -0.6805],\n",
            "        [ 0.5688, -0.8039],\n",
            "        [ 0.5279, -0.7705],\n",
            "        [-0.6884,  0.7489],\n",
            "        [-0.6950,  0.9171],\n",
            "        [-0.1041,  0.2865],\n",
            "        [ 0.4750, -0.7091]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4181,  0.8015],\n",
            "        [ 0.1882, -0.3330],\n",
            "        [-0.9867,  0.8955],\n",
            "        [-0.0794,  0.0079],\n",
            "        [ 0.0283, -0.0563],\n",
            "        [ 0.0242,  0.0591],\n",
            "        [ 0.3111, -0.5232],\n",
            "        [-0.7230,  0.9077],\n",
            "        [ 0.2033, -0.4105],\n",
            "        [-0.8870,  0.6557],\n",
            "        [ 0.5733, -0.7621],\n",
            "        [ 0.5690, -0.8944],\n",
            "        [ 0.4946, -0.4524],\n",
            "        [-0.6099,  0.9078],\n",
            "        [ 0.3602, -0.4867],\n",
            "        [ 0.0966,  0.2417],\n",
            "        [ 0.5262, -0.3841],\n",
            "        [ 0.3212, -0.5551],\n",
            "        [ 0.1643, -0.1827],\n",
            "        [ 0.0660, -0.3088],\n",
            "        [ 0.1257,  0.2486],\n",
            "        [ 0.5138, -0.3526],\n",
            "        [ 0.0190,  0.3790],\n",
            "        [ 0.3612, -0.4052],\n",
            "        [-0.9609,  1.1150],\n",
            "        [ 0.1653, -0.6928],\n",
            "        [ 0.4749, -0.8578],\n",
            "        [-0.8638,  1.0127],\n",
            "        [ 0.4625, -0.4990],\n",
            "        [-0.5623,  0.6306],\n",
            "        [ 0.4433, -0.3691],\n",
            "        [-0.7744,  0.9028]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.7828, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3148, -0.5621],\n",
            "        [-0.6988,  0.8679],\n",
            "        [ 0.8556, -0.8041],\n",
            "        [-0.5049,  0.7259],\n",
            "        [ 0.4671, -0.5051],\n",
            "        [-0.5659,  0.5906],\n",
            "        [-0.5259,  0.8697],\n",
            "        [ 0.4782, -0.3826],\n",
            "        [ 0.4221, -0.3284],\n",
            "        [ 0.1732, -0.5249],\n",
            "        [-0.0843, -0.1057],\n",
            "        [ 0.1515,  0.2833],\n",
            "        [ 0.0721, -0.2302],\n",
            "        [ 0.0681, -0.4970],\n",
            "        [-0.7228,  0.5837],\n",
            "        [ 0.2098, -0.4684],\n",
            "        [ 0.0309, -0.1486],\n",
            "        [ 0.0941,  0.3219],\n",
            "        [ 0.2247, -0.8842],\n",
            "        [-0.2740,  0.1296],\n",
            "        [ 0.5078, -0.6866],\n",
            "        [-0.0337, -0.0683],\n",
            "        [-0.3902,  0.6968],\n",
            "        [ 0.0525,  0.1309],\n",
            "        [-0.5426,  0.7324],\n",
            "        [ 0.3732, -0.8186],\n",
            "        [ 0.4801, -0.8349],\n",
            "        [ 0.3816, -0.2048],\n",
            "        [ 0.1500, -0.1447],\n",
            "        [ 0.4216, -0.8065],\n",
            "        [ 0.6036, -0.3402],\n",
            "        [-0.6862,  0.7237]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2846, -0.4021],\n",
            "        [-0.5017,  0.3691],\n",
            "        [ 0.2342, -0.6325],\n",
            "        [ 0.5429, -0.4711],\n",
            "        [-0.8551,  1.1117],\n",
            "        [ 0.4684, -0.8377],\n",
            "        [ 0.4847, -0.3920],\n",
            "        [-0.4298,  0.5535],\n",
            "        [ 0.3187, -0.6652],\n",
            "        [-0.6093,  0.7121],\n",
            "        [-0.3453,  0.9680],\n",
            "        [ 0.4472, -0.1355],\n",
            "        [ 0.6552, -0.5884],\n",
            "        [ 0.1245, -0.3667],\n",
            "        [ 0.3025, -0.5859],\n",
            "        [ 0.2777, -0.1836],\n",
            "        [ 0.7101, -0.5087],\n",
            "        [ 0.4542, -0.4268],\n",
            "        [ 0.2357, -0.4291],\n",
            "        [ 0.2345, -0.5133],\n",
            "        [ 0.5485, -0.9004],\n",
            "        [-0.4905,  0.6364],\n",
            "        [-0.0553, -0.1302],\n",
            "        [ 0.4402, -0.6735],\n",
            "        [-0.4679,  0.2238],\n",
            "        [-0.5051,  0.5718],\n",
            "        [ 0.4525, -0.4235],\n",
            "        [ 0.6388, -0.4389],\n",
            "        [-0.6332,  1.0349],\n",
            "        [-0.2681, -0.1312],\n",
            "        [ 0.2663, -0.3609],\n",
            "        [ 0.3956, -0.1799]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4707, -0.7709],\n",
            "        [ 0.1055, -0.2607],\n",
            "        [-0.6241,  0.9404],\n",
            "        [-0.7770,  0.8562],\n",
            "        [-0.4451,  0.2833],\n",
            "        [ 0.4290, -0.5987],\n",
            "        [ 0.3285, -0.5671],\n",
            "        [ 0.1586, -0.6864],\n",
            "        [-0.6257,  1.0072],\n",
            "        [-0.2661,  0.3701],\n",
            "        [ 0.0965,  0.1943],\n",
            "        [ 0.5226, -0.7692],\n",
            "        [ 0.5003, -0.8669],\n",
            "        [-0.3801,  0.3335],\n",
            "        [ 0.2570, -0.6328],\n",
            "        [ 0.4840, -0.7191],\n",
            "        [ 0.6098, -0.6096],\n",
            "        [-0.1048,  0.4523],\n",
            "        [ 0.1074, -0.1685],\n",
            "        [ 0.6711, -0.7101],\n",
            "        [ 0.5182, -0.0959],\n",
            "        [ 0.1180,  0.1749],\n",
            "        [ 0.1174, -0.5410],\n",
            "        [-0.3305,  0.5303],\n",
            "        [-0.6069,  0.8363],\n",
            "        [ 0.3607, -0.2685],\n",
            "        [-0.3835,  0.7504],\n",
            "        [-0.1723,  0.1211],\n",
            "        [ 0.3370, -0.5914],\n",
            "        [ 0.3217, -0.7187],\n",
            "        [ 0.0797,  0.0823],\n",
            "        [-0.7775,  0.9177]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5693, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3307,  0.5286],\n",
            "        [-0.6713,  0.7177],\n",
            "        [ 0.3520, -0.6614],\n",
            "        [ 0.1984, -0.5590],\n",
            "        [-0.2187,  0.1765],\n",
            "        [ 0.5800, -0.3869],\n",
            "        [-0.5407,  0.9090],\n",
            "        [ 0.5451, -0.5731],\n",
            "        [ 0.2976,  0.4774],\n",
            "        [ 0.4942, -0.5685],\n",
            "        [ 0.5155, -0.6176],\n",
            "        [-0.6267,  0.7533],\n",
            "        [-0.3846,  0.3732],\n",
            "        [-0.3813,  0.3296],\n",
            "        [ 0.2696, -0.6695],\n",
            "        [ 0.4974, -0.3069],\n",
            "        [ 0.1019, -0.2207],\n",
            "        [ 0.3321, -0.6271],\n",
            "        [ 0.4830, -0.9260],\n",
            "        [ 0.1342, -0.4885],\n",
            "        [ 0.0566, -0.3104],\n",
            "        [ 0.3714, -0.7962],\n",
            "        [ 0.3631, -0.5103],\n",
            "        [-0.4897,  0.2133],\n",
            "        [-0.3382,  0.7284],\n",
            "        [ 0.5491, -0.5805],\n",
            "        [ 0.4029, -0.3394],\n",
            "        [ 0.5170, -0.8038],\n",
            "        [ 0.1782, -0.5267],\n",
            "        [ 0.1907, -0.3279],\n",
            "        [ 0.3241, -0.5729],\n",
            "        [-0.0825,  0.0119]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0927,  0.0538],\n",
            "        [-0.5486,  0.8837],\n",
            "        [-0.5110,  0.9979],\n",
            "        [ 0.1843, -0.1929],\n",
            "        [ 0.7889, -0.6618],\n",
            "        [-0.4426,  0.4414],\n",
            "        [ 0.4172, -0.2349],\n",
            "        [ 0.1952, -0.4196],\n",
            "        [ 0.6638, -0.5295],\n",
            "        [ 0.5212, -0.5238],\n",
            "        [ 0.1901, -0.2434],\n",
            "        [ 0.6142, -0.6507],\n",
            "        [ 0.0714, -0.6540],\n",
            "        [ 0.1774, -0.1750],\n",
            "        [-0.3133,  0.0290],\n",
            "        [ 0.3002, -0.0671],\n",
            "        [ 0.3098, -0.5241],\n",
            "        [ 0.1960, -0.5335],\n",
            "        [ 0.4929, -0.8130],\n",
            "        [-0.6001,  0.7998],\n",
            "        [ 0.1685, -0.4425],\n",
            "        [ 0.3852, -0.5369],\n",
            "        [ 0.5953, -0.6519],\n",
            "        [ 0.3205, -0.7307],\n",
            "        [ 0.0391, -0.4403],\n",
            "        [ 0.6004, -0.6126],\n",
            "        [ 0.3527, -0.6079],\n",
            "        [ 0.5148, -0.8010],\n",
            "        [-0.6494,  0.7095],\n",
            "        [ 0.2494, -0.2767],\n",
            "        [-0.5080,  0.5466],\n",
            "        [-0.6963,  1.0186]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4859, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5893,  0.5620],\n",
            "        [ 0.0243, -0.0896],\n",
            "        [-0.1209, -0.0444],\n",
            "        [-0.2923,  0.7732],\n",
            "        [ 0.3292, -0.2951],\n",
            "        [ 0.4334, -0.7640],\n",
            "        [ 0.3124, -1.0179],\n",
            "        [-0.7047,  0.8173],\n",
            "        [ 0.3296, -1.0024],\n",
            "        [ 0.7059, -0.8694],\n",
            "        [ 0.1494,  0.0835],\n",
            "        [-0.4620,  0.7249],\n",
            "        [ 0.4475, -0.8019],\n",
            "        [ 0.5368, -0.7819],\n",
            "        [-0.7377,  0.8032],\n",
            "        [ 0.6142, -0.7721],\n",
            "        [ 0.3244, -0.6183],\n",
            "        [ 0.4970, -0.5211],\n",
            "        [ 0.4024, -0.3114],\n",
            "        [ 0.6373, -0.8231],\n",
            "        [ 0.4483, -0.8665],\n",
            "        [ 0.6282, -0.7979],\n",
            "        [ 0.0524, -0.6380],\n",
            "        [ 0.2860, -0.3828],\n",
            "        [ 0.3124, -0.5323],\n",
            "        [ 0.4675, -0.6347],\n",
            "        [ 0.6164, -0.8594],\n",
            "        [ 0.3589, -0.4057],\n",
            "        [ 0.3031, -0.6847],\n",
            "        [-0.8296,  0.9861],\n",
            "        [-0.0500,  0.2386],\n",
            "        [ 0.6465, -0.6472]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5151, -0.6671],\n",
            "        [ 0.4237, -0.5701],\n",
            "        [ 0.2636, -0.5981],\n",
            "        [ 0.0583, -0.0331],\n",
            "        [ 0.3759, -0.4654],\n",
            "        [-0.0498,  0.0345],\n",
            "        [ 0.4980, -0.6407],\n",
            "        [-0.1595,  0.1885],\n",
            "        [-0.6197,  0.5787],\n",
            "        [ 0.3287, -0.2743],\n",
            "        [ 0.2775, -0.7617],\n",
            "        [ 0.4438, -0.8497],\n",
            "        [-0.4340,  0.6126],\n",
            "        [ 0.3349, -0.2489],\n",
            "        [ 0.0504, -0.2366],\n",
            "        [ 0.3374, -0.4232],\n",
            "        [ 0.4137, -0.6530],\n",
            "        [ 0.5605, -0.7620],\n",
            "        [ 0.4634, -0.5740],\n",
            "        [ 0.4808, -1.0214],\n",
            "        [ 0.5508, -0.6481],\n",
            "        [ 0.1028,  0.3463],\n",
            "        [ 0.5162, -0.6831],\n",
            "        [ 0.1935, -0.8492],\n",
            "        [ 0.4493, -0.1161],\n",
            "        [ 0.3465, -0.9146],\n",
            "        [ 0.4709, -0.5015],\n",
            "        [ 0.4353, -0.7930],\n",
            "        [ 0.3735, -0.4695],\n",
            "        [ 0.1784, -0.4197],\n",
            "        [-0.2082, -0.2391],\n",
            "        [-0.3572,  0.1469]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7077e-01, -5.5862e-01],\n",
            "        [ 3.4172e-01, -3.8132e-01],\n",
            "        [-2.1839e-01,  4.9397e-01],\n",
            "        [ 4.1590e-01, -7.6018e-01],\n",
            "        [ 3.6110e-01, -7.0390e-01],\n",
            "        [ 4.8292e-01, -8.7952e-01],\n",
            "        [-4.8132e-01,  4.4333e-01],\n",
            "        [-5.8104e-02, -3.0729e-01],\n",
            "        [ 2.3093e-01, -7.4157e-01],\n",
            "        [-3.6686e-01,  7.9129e-01],\n",
            "        [ 4.4692e-01, -6.3703e-01],\n",
            "        [ 5.1515e-01, -6.0256e-01],\n",
            "        [ 5.9874e-01, -7.3668e-01],\n",
            "        [ 7.0154e-03, -1.7856e-01],\n",
            "        [ 4.8691e-01, -6.3255e-01],\n",
            "        [-5.3444e-01,  9.3020e-01],\n",
            "        [ 4.0115e-01, -6.4275e-01],\n",
            "        [ 2.6935e-01, -4.4642e-01],\n",
            "        [ 4.3219e-01, -5.9378e-01],\n",
            "        [-8.1716e-02, -1.9188e-01],\n",
            "        [-4.7253e-01,  8.7393e-01],\n",
            "        [ 2.5931e-01, -5.0420e-01],\n",
            "        [ 7.2616e-01, -8.2260e-01],\n",
            "        [-8.2056e-01,  8.9585e-01],\n",
            "        [ 4.5421e-01, -5.3755e-01],\n",
            "        [-5.5866e-02,  3.4742e-01],\n",
            "        [-2.8735e-01, -7.4387e-04],\n",
            "        [ 1.0923e-01, -6.3817e-01],\n",
            "        [ 1.5239e-01, -2.4706e-02],\n",
            "        [ 1.2680e-01, -1.7665e-01],\n",
            "        [ 3.7779e-01, -7.2934e-01],\n",
            "        [ 4.5656e-01, -4.6077e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8249,  0.7483],\n",
            "        [-0.3361,  0.6730],\n",
            "        [ 0.2627, -0.5101],\n",
            "        [ 0.4732, -0.6213],\n",
            "        [-0.4321,  1.0662],\n",
            "        [-0.0738, -0.0420],\n",
            "        [-0.6016,  0.7848],\n",
            "        [ 0.0843, -0.0483],\n",
            "        [ 0.3814, -0.7116],\n",
            "        [-0.7774,  0.7480],\n",
            "        [-0.8647,  0.7533],\n",
            "        [ 0.3876, -0.0958],\n",
            "        [ 0.2507, -0.0776],\n",
            "        [ 0.4473, -0.4875],\n",
            "        [-0.4667,  0.4649],\n",
            "        [ 0.0931, -0.4456],\n",
            "        [ 0.1683, -0.6909],\n",
            "        [ 0.6265, -0.7463],\n",
            "        [ 0.3175, -0.0531],\n",
            "        [-0.3230,  0.2354],\n",
            "        [ 0.4168, -0.5210],\n",
            "        [ 0.7809, -0.7241],\n",
            "        [-0.8216,  0.9583],\n",
            "        [ 0.2802, -0.6152],\n",
            "        [-0.0669,  0.2647],\n",
            "        [ 0.1190, -0.1623],\n",
            "        [ 0.2412, -0.6178],\n",
            "        [ 0.4347, -0.5210],\n",
            "        [ 0.8005, -0.5886],\n",
            "        [ 0.6062, -0.7000],\n",
            "        [ 0.3181, -0.6824],\n",
            "        [-0.1368, -0.0365]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4021, -0.4746],\n",
            "        [-0.4049,  0.7414],\n",
            "        [ 0.5306, -0.7314],\n",
            "        [-0.3173,  0.1364],\n",
            "        [-0.6001,  0.7406],\n",
            "        [ 0.4805, -0.3289],\n",
            "        [-0.7056,  0.8913],\n",
            "        [-0.0202, -0.1806],\n",
            "        [ 0.2905, -0.5406],\n",
            "        [ 0.3953, -0.6546],\n",
            "        [ 0.4068, -0.5228],\n",
            "        [-0.6498,  0.7535],\n",
            "        [ 0.4588, -0.5765],\n",
            "        [-0.4346,  0.6976],\n",
            "        [-0.5328,  0.9056],\n",
            "        [ 0.3353, -0.5477],\n",
            "        [ 0.1896,  0.0153],\n",
            "        [-0.8708,  0.9593],\n",
            "        [-0.8095,  0.7197],\n",
            "        [ 0.5482, -0.7732],\n",
            "        [ 0.7573, -0.5729],\n",
            "        [ 0.1496, -0.4939],\n",
            "        [ 0.3391, -0.3760],\n",
            "        [-0.0319, -0.6615],\n",
            "        [-0.1985, -0.1915],\n",
            "        [-0.8398,  1.0586],\n",
            "        [ 0.2554, -0.5259],\n",
            "        [ 0.2784, -0.4955],\n",
            "        [ 0.3152, -0.3549],\n",
            "        [-0.4939,  0.9234],\n",
            "        [-0.6952,  0.6487],\n",
            "        [ 0.2050, -0.7829]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6692, -0.8133],\n",
            "        [ 0.5811, -0.4970],\n",
            "        [ 0.3839, -0.6502],\n",
            "        [ 0.3892, -0.5041],\n",
            "        [-0.8153,  0.8384],\n",
            "        [-0.4480,  0.5276],\n",
            "        [-0.2979,  0.3094],\n",
            "        [-0.0154, -0.3843],\n",
            "        [ 0.3704, -0.6183],\n",
            "        [ 0.1564, -0.4337],\n",
            "        [ 0.4536, -1.0849],\n",
            "        [ 0.2995, -0.2858],\n",
            "        [-0.5594,  0.5486],\n",
            "        [-0.8945,  0.8485],\n",
            "        [-0.1214,  0.2014],\n",
            "        [ 0.2991, -0.5963],\n",
            "        [ 0.2570, -0.9042],\n",
            "        [ 0.1187, -0.2673],\n",
            "        [ 0.3615, -0.6944],\n",
            "        [ 0.5674, -0.6984],\n",
            "        [-0.3724,  0.3019],\n",
            "        [ 0.3059, -0.1390],\n",
            "        [-0.5790,  1.0182],\n",
            "        [ 0.3917, -0.9632],\n",
            "        [ 0.5326, -0.5545],\n",
            "        [ 0.0695, -0.3143],\n",
            "        [-0.7380,  0.8972],\n",
            "        [ 0.4128, -0.6615],\n",
            "        [ 0.4678, -0.7323],\n",
            "        [ 0.3498, -0.3718],\n",
            "        [-0.4309,  0.6900],\n",
            "        [ 0.3429, -0.7169]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4825, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8020e-01, -4.7845e-01],\n",
            "        [ 3.0325e-01, -6.2345e-01],\n",
            "        [-3.8669e-01,  8.2112e-01],\n",
            "        [-9.4919e-01,  9.3767e-01],\n",
            "        [ 1.7330e-01, -5.3346e-01],\n",
            "        [-5.4464e-01,  5.5697e-01],\n",
            "        [ 1.4588e-01, -4.5357e-01],\n",
            "        [ 3.3561e-01, -8.3020e-01],\n",
            "        [-8.3479e-02,  3.4340e-01],\n",
            "        [ 4.7392e-01, -6.7177e-01],\n",
            "        [ 3.1043e-01, -7.0631e-01],\n",
            "        [ 5.0330e-02, -1.0058e-04],\n",
            "        [-4.4176e-01,  9.3762e-01],\n",
            "        [ 2.0178e-01, -1.8079e-01],\n",
            "        [ 3.1404e-01, -4.8863e-01],\n",
            "        [-7.0154e-01,  9.4111e-01],\n",
            "        [ 2.6347e-01, -3.0732e-01],\n",
            "        [-7.8691e-01,  8.8565e-01],\n",
            "        [ 4.5982e-01, -1.8957e-01],\n",
            "        [ 8.5534e-02, -5.2215e-01],\n",
            "        [ 4.3642e-01, -7.7101e-01],\n",
            "        [-2.3033e-01,  6.8085e-01],\n",
            "        [-7.7615e-01,  8.2328e-01],\n",
            "        [ 7.7073e-01, -5.6917e-01],\n",
            "        [ 6.2432e-02, -5.0418e-01],\n",
            "        [ 5.2854e-01, -7.4520e-01],\n",
            "        [ 6.7980e-02, -1.6520e-01],\n",
            "        [-4.8584e-01,  7.3801e-01],\n",
            "        [ 4.4689e-01, -5.8483e-01],\n",
            "        [-3.3361e-01,  7.7286e-01],\n",
            "        [-8.4698e-01,  9.3356e-01],\n",
            "        [-2.0437e-01, -1.2517e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5285,  0.8248],\n",
            "        [-0.8732,  1.1792],\n",
            "        [ 0.5221, -0.6407],\n",
            "        [-0.3403,  0.3295],\n",
            "        [ 0.4605, -0.4631],\n",
            "        [ 0.3684, -0.5259],\n",
            "        [ 0.6841, -0.4398],\n",
            "        [ 0.1402, -0.5475],\n",
            "        [-0.9350,  0.9303],\n",
            "        [-0.7180,  1.0230],\n",
            "        [ 0.3565, -0.3779],\n",
            "        [ 0.2864, -0.8308],\n",
            "        [-0.5773,  0.3282],\n",
            "        [-0.4839,  0.6796],\n",
            "        [ 0.4542, -0.8130],\n",
            "        [-0.6472,  0.8378],\n",
            "        [ 0.0226,  0.1397],\n",
            "        [-0.8126,  0.8509],\n",
            "        [-0.6723,  0.8672],\n",
            "        [-0.5586,  0.6779],\n",
            "        [ 0.1902,  0.0181],\n",
            "        [ 0.0585, -0.0035],\n",
            "        [ 0.4327, -0.7328],\n",
            "        [ 0.5023, -0.3987],\n",
            "        [ 0.3026, -0.4567],\n",
            "        [-0.0144,  0.0122],\n",
            "        [-0.4089,  0.1948],\n",
            "        [-0.3505,  0.5209],\n",
            "        [-0.0020,  0.3861],\n",
            "        [ 0.5073, -0.8951],\n",
            "        [ 0.3386, -0.8893],\n",
            "        [-0.3317,  0.3499]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7392, -0.6378],\n",
            "        [ 0.1737, -0.5952],\n",
            "        [ 0.4862, -0.4014],\n",
            "        [ 0.1404, -0.3150],\n",
            "        [-0.6647,  1.1388],\n",
            "        [-0.5288,  0.7683],\n",
            "        [ 0.0849, -0.5432],\n",
            "        [ 0.4363, -0.5996],\n",
            "        [ 0.2695, -0.5667],\n",
            "        [ 0.0629, -0.0072],\n",
            "        [ 0.3602, -0.5052],\n",
            "        [ 0.5549, -0.8301],\n",
            "        [ 0.3753, -0.5717],\n",
            "        [ 0.4269, -0.6693],\n",
            "        [ 0.1791, -0.5049],\n",
            "        [ 0.6605, -0.8257],\n",
            "        [ 0.5668, -0.6334],\n",
            "        [ 0.5610, -0.4251],\n",
            "        [-0.6391,  1.1829],\n",
            "        [-0.7989,  0.8219],\n",
            "        [ 0.5854, -0.7217],\n",
            "        [ 0.4362, -0.7945],\n",
            "        [-1.1236,  1.1206],\n",
            "        [-0.1740, -0.2666],\n",
            "        [-0.1586,  0.3799],\n",
            "        [ 0.4352, -0.3579],\n",
            "        [ 0.3291, -0.2255],\n",
            "        [ 0.5165, -0.9949],\n",
            "        [ 0.4740, -0.8413],\n",
            "        [-0.6145,  0.9298],\n",
            "        [-0.8289,  0.9630],\n",
            "        [ 0.2331, -0.5595]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4050, -0.2594],\n",
            "        [ 0.0764, -0.4262],\n",
            "        [ 0.2157, -0.6057],\n",
            "        [-0.3753, -0.1990],\n",
            "        [ 0.4287, -0.5948],\n",
            "        [-0.0805,  0.2482],\n",
            "        [ 0.3768, -0.7612],\n",
            "        [ 0.2676, -0.4952],\n",
            "        [ 0.1859, -0.3344],\n",
            "        [-0.8064,  1.1086],\n",
            "        [-0.6450,  0.9439],\n",
            "        [-0.4144,  0.2251],\n",
            "        [ 0.5055, -0.5514],\n",
            "        [ 0.4708, -0.8475],\n",
            "        [ 0.4790, -0.4346],\n",
            "        [ 0.3473, -0.7338],\n",
            "        [ 0.5272, -0.3372],\n",
            "        [ 0.4034, -0.3512],\n",
            "        [-0.1196,  0.3520],\n",
            "        [ 0.4414, -0.4818],\n",
            "        [-0.8753,  0.7292],\n",
            "        [-0.6615,  0.9757],\n",
            "        [ 0.2154, -0.8734],\n",
            "        [ 0.2440, -0.7484],\n",
            "        [ 0.5397, -0.5593],\n",
            "        [ 0.2034, -0.4847],\n",
            "        [ 0.4322, -0.3483],\n",
            "        [-0.3611,  0.3244],\n",
            "        [ 0.2987, -0.4432],\n",
            "        [-0.0338, -0.4493],\n",
            "        [ 0.2175, -0.5886],\n",
            "        [-0.3532,  0.5421]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5801,  1.0076],\n",
            "        [-0.9082,  0.9512],\n",
            "        [ 0.3780, -0.2644],\n",
            "        [ 0.2131, -0.2772],\n",
            "        [-0.3398,  0.1854],\n",
            "        [ 0.3686, -0.5313],\n",
            "        [ 0.2753, -0.7057],\n",
            "        [ 0.4113, -0.5267],\n",
            "        [ 0.3598, -0.7831],\n",
            "        [ 0.5071, -0.5969],\n",
            "        [ 0.5320, -0.4090],\n",
            "        [ 0.3007, -0.7727],\n",
            "        [ 0.4260, -0.8353],\n",
            "        [ 0.2352, -0.1273],\n",
            "        [ 0.0341, -0.0569],\n",
            "        [ 0.5004, -0.6715],\n",
            "        [-0.8818,  0.9932],\n",
            "        [-0.8828,  0.8826],\n",
            "        [ 0.2932, -0.6485],\n",
            "        [ 0.1466, -0.0105],\n",
            "        [-0.6626,  0.3698],\n",
            "        [ 0.5706, -0.8750],\n",
            "        [-0.3795,  0.5237],\n",
            "        [ 0.3287, -0.8046],\n",
            "        [ 0.2457, -0.3620],\n",
            "        [-0.7547,  0.9791],\n",
            "        [ 0.3339, -0.5682],\n",
            "        [ 0.2330, -0.1301],\n",
            "        [ 0.6260, -0.9402],\n",
            "        [-0.9328,  0.7750],\n",
            "        [ 0.1283, -0.3301],\n",
            "        [ 0.2632, -0.6150]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9127,  0.9439],\n",
            "        [-0.5203,  0.5624],\n",
            "        [ 0.4156, -0.1035],\n",
            "        [ 0.6456, -0.5203],\n",
            "        [ 0.1128, -0.1891],\n",
            "        [-0.5536,  0.6839],\n",
            "        [-0.4934,  0.8178],\n",
            "        [-0.0991, -0.0042],\n",
            "        [-0.0982, -0.1926],\n",
            "        [ 0.3270, -0.4736],\n",
            "        [-0.2183, -0.3864],\n",
            "        [ 0.5771, -0.7084],\n",
            "        [ 0.5847, -0.6539],\n",
            "        [ 0.4796, -0.5386],\n",
            "        [ 0.2110, -0.2743],\n",
            "        [-0.5046,  0.8133],\n",
            "        [ 0.3638, -0.8272],\n",
            "        [-0.8571,  0.9592],\n",
            "        [-0.0399, -0.1728],\n",
            "        [-0.6099,  0.9662],\n",
            "        [-0.8468,  0.7260],\n",
            "        [ 0.4234, -0.6408],\n",
            "        [ 0.2803, -0.4022],\n",
            "        [-0.7300,  0.6072],\n",
            "        [-0.0786, -0.2271],\n",
            "        [ 0.3373, -0.5284],\n",
            "        [-0.8938,  0.9669],\n",
            "        [-0.6197,  0.0775],\n",
            "        [-0.7064,  0.8505],\n",
            "        [ 0.0743, -0.5924],\n",
            "        [ 0.3203, -0.9102],\n",
            "        [ 0.4691, -0.8949]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3701, -0.5161],\n",
            "        [ 0.5532, -0.7881],\n",
            "        [-0.5884,  0.3851],\n",
            "        [ 0.2107, -0.4084],\n",
            "        [ 0.3918, -0.7899],\n",
            "        [ 0.0836, -0.2843],\n",
            "        [-0.6609,  0.7416],\n",
            "        [ 0.4792, -0.5378],\n",
            "        [ 0.3299, -0.5230],\n",
            "        [-0.5710,  0.8126],\n",
            "        [-0.1991,  0.0911],\n",
            "        [ 0.0475, -0.6499],\n",
            "        [-0.9222,  0.9404],\n",
            "        [ 0.4092, -0.4065],\n",
            "        [ 0.3581, -0.6859],\n",
            "        [ 0.5720, -0.9005],\n",
            "        [-0.6003,  1.0590],\n",
            "        [ 0.2406, -0.7495],\n",
            "        [-0.6937,  0.9883],\n",
            "        [ 0.2703, -0.5657],\n",
            "        [ 0.4399, -0.7250],\n",
            "        [-0.5011,  0.4901],\n",
            "        [-0.9195,  0.9180],\n",
            "        [-0.1232,  0.4410],\n",
            "        [ 0.4667, -0.7111],\n",
            "        [ 0.4890, -0.4782],\n",
            "        [-0.4946,  0.8881],\n",
            "        [ 0.7274, -0.6787],\n",
            "        [ 0.3657, -0.5336],\n",
            "        [ 0.1263, -0.4250],\n",
            "        [-0.2878,  0.7604],\n",
            "        [-0.8401,  0.9393]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3234, -0.7332],\n",
            "        [ 0.4939, -0.8799],\n",
            "        [ 0.3047, -0.5578],\n",
            "        [ 0.1316, -0.0157],\n",
            "        [ 0.2338, -0.5865],\n",
            "        [ 0.3479, -0.1338],\n",
            "        [ 0.2871, -0.5580],\n",
            "        [-0.3578,  0.5198],\n",
            "        [-0.4644,  0.2521],\n",
            "        [-0.7133,  0.9175],\n",
            "        [-0.4558,  0.7947],\n",
            "        [-0.8919,  1.0052],\n",
            "        [-0.2683,  0.0720],\n",
            "        [ 0.5522, -0.5474],\n",
            "        [-1.0043,  0.9236],\n",
            "        [-0.0162,  0.3193],\n",
            "        [ 0.4730, -0.3653],\n",
            "        [ 0.4320, -0.7721],\n",
            "        [-0.4188,  0.3263],\n",
            "        [ 0.5148, -0.8281],\n",
            "        [ 0.2010, -0.2532],\n",
            "        [-0.1398,  0.2181],\n",
            "        [-0.3626,  0.6685],\n",
            "        [ 0.3380, -0.6134],\n",
            "        [-0.9287,  0.8366],\n",
            "        [ 0.5058, -0.8815],\n",
            "        [ 0.4138, -0.3262],\n",
            "        [-0.4664,  0.3811],\n",
            "        [-0.5559,  0.9777],\n",
            "        [-0.0186, -0.2558],\n",
            "        [ 0.6897, -0.5196],\n",
            "        [ 0.3794, -0.8113]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4032, -0.6926],\n",
            "        [-0.7078,  0.7903],\n",
            "        [ 0.4408, -0.7367],\n",
            "        [ 0.6949, -0.5802],\n",
            "        [ 0.0060,  0.5912],\n",
            "        [ 0.0059,  0.1068],\n",
            "        [-0.8280,  0.8464],\n",
            "        [ 0.3850, -0.4193],\n",
            "        [ 0.2430, -0.4390],\n",
            "        [-0.5029,  0.9088],\n",
            "        [ 0.2283, -0.1288],\n",
            "        [-0.6439,  1.0769],\n",
            "        [-0.2635, -0.2459],\n",
            "        [-0.4460,  0.4242],\n",
            "        [ 0.5714, -0.3930],\n",
            "        [ 0.5319, -0.9416],\n",
            "        [-0.4707,  0.5139],\n",
            "        [-0.8090,  1.0048],\n",
            "        [ 0.5680, -0.7273],\n",
            "        [-0.0452, -0.3688],\n",
            "        [ 0.2644, -0.4224],\n",
            "        [-0.1080, -0.0422],\n",
            "        [ 0.1547, -0.4572],\n",
            "        [-0.0228, -0.3391],\n",
            "        [ 0.3847, -0.6034],\n",
            "        [ 0.3518, -0.5500],\n",
            "        [ 0.5600, -0.5767],\n",
            "        [ 0.5561, -0.8683],\n",
            "        [ 0.4076, -0.6143],\n",
            "        [ 0.3733, -0.6530],\n",
            "        [-0.4284,  0.2164],\n",
            "        [-0.6119,  0.7386]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5651,  0.6504],\n",
            "        [ 0.5142, -0.2362],\n",
            "        [ 0.4702, -0.5199],\n",
            "        [-0.2713,  0.2868],\n",
            "        [-0.8734,  0.9702],\n",
            "        [-0.9003,  1.0278],\n",
            "        [ 0.1511, -0.4566],\n",
            "        [ 0.5960, -0.9016],\n",
            "        [ 0.5082, -0.4426],\n",
            "        [-0.5333,  0.3336],\n",
            "        [-0.8767,  0.9594],\n",
            "        [ 0.4292, -0.9218],\n",
            "        [-0.1308,  0.3297],\n",
            "        [ 0.0138, -0.2516],\n",
            "        [ 0.3313, -0.7186],\n",
            "        [ 0.3088, -0.6834],\n",
            "        [-0.5193,  0.9018],\n",
            "        [-0.7339,  0.8704],\n",
            "        [ 0.6498, -0.8517],\n",
            "        [-0.2055,  0.1024],\n",
            "        [-0.6219,  0.9229],\n",
            "        [-0.7269,  0.9229],\n",
            "        [ 0.2171, -0.7008],\n",
            "        [ 0.3354, -0.8688],\n",
            "        [-0.3453,  0.6567],\n",
            "        [ 0.4241, -0.2794],\n",
            "        [ 0.2234,  0.1259],\n",
            "        [ 0.0445, -0.5054],\n",
            "        [ 0.0818, -0.4066],\n",
            "        [-0.8004,  0.8689],\n",
            "        [-0.8270,  1.1198],\n",
            "        [ 0.0722, -0.1054]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4988, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3466, -0.8137],\n",
            "        [ 0.1257, -0.5381],\n",
            "        [ 0.1792, -0.4784],\n",
            "        [ 0.4999, -0.6493],\n",
            "        [ 0.4652, -0.8771],\n",
            "        [-0.2291,  0.3802],\n",
            "        [ 0.2435, -0.5844],\n",
            "        [-0.6358,  1.0314],\n",
            "        [-0.0837,  0.2042],\n",
            "        [ 0.0976, -0.5697],\n",
            "        [ 0.0807, -0.2839],\n",
            "        [ 0.4655, -0.4794],\n",
            "        [ 0.2701, -0.5331],\n",
            "        [ 0.5250, -0.9481],\n",
            "        [-0.0271, -0.0424],\n",
            "        [-0.7938,  0.9567],\n",
            "        [-0.8555,  0.9205],\n",
            "        [ 0.2656, -0.1638],\n",
            "        [ 0.6711, -0.6203],\n",
            "        [ 0.4210, -0.3850],\n",
            "        [ 0.3131, -0.3897],\n",
            "        [-0.6725,  1.0400],\n",
            "        [ 0.0853, -0.0532],\n",
            "        [ 0.1925, -0.5607],\n",
            "        [ 0.2377, -0.6584],\n",
            "        [ 0.2217, -0.5886],\n",
            "        [-0.3167,  0.7914],\n",
            "        [-0.3448,  0.2059],\n",
            "        [ 0.1366, -0.7596],\n",
            "        [-0.9160,  0.9404],\n",
            "        [-0.8464,  1.0532],\n",
            "        [ 0.5295, -1.0395]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9339,  1.0267],\n",
            "        [ 0.4450, -0.6440],\n",
            "        [ 0.7294, -0.6516],\n",
            "        [ 0.4898, -0.9676],\n",
            "        [ 0.1705, -0.2480],\n",
            "        [ 0.2526, -0.6767],\n",
            "        [-0.7059,  0.8619],\n",
            "        [ 0.0121,  0.0108],\n",
            "        [ 0.3974, -0.5722],\n",
            "        [ 0.2574, -0.4701],\n",
            "        [ 0.7384, -0.9743],\n",
            "        [-0.7205,  1.2200],\n",
            "        [ 0.0693,  0.2896],\n",
            "        [ 0.3027, -0.5023],\n",
            "        [-0.0217, -0.4594],\n",
            "        [ 0.3854, -0.5277],\n",
            "        [-0.7586,  0.9809],\n",
            "        [ 0.3039, -0.6961],\n",
            "        [ 0.4850, -0.8018],\n",
            "        [-0.6810,  0.7138],\n",
            "        [-1.0747,  1.0051],\n",
            "        [ 0.4666, -0.7199],\n",
            "        [ 0.1362, -0.3875],\n",
            "        [ 0.5248, -0.6701],\n",
            "        [-0.8135,  0.8242],\n",
            "        [ 0.1538, -0.5795],\n",
            "        [ 0.3443, -0.7764],\n",
            "        [ 0.2353, -0.3966],\n",
            "        [ 0.6633, -0.5471],\n",
            "        [ 0.2381, -0.4926],\n",
            "        [ 0.5150, -0.7055],\n",
            "        [ 0.4979, -0.3689]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5909, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6738,  0.8932],\n",
            "        [-0.6259,  0.9639],\n",
            "        [ 0.2417, -0.7448],\n",
            "        [-0.9455,  0.9270],\n",
            "        [ 0.2475, -0.9056],\n",
            "        [ 0.6287, -0.6387],\n",
            "        [ 0.4887, -0.5556],\n",
            "        [ 0.1839, -0.9346],\n",
            "        [-0.2704,  0.3325],\n",
            "        [ 0.3550, -0.6249],\n",
            "        [ 0.3521, -0.5448],\n",
            "        [-0.6725,  1.1420],\n",
            "        [-0.1368,  0.0740],\n",
            "        [ 0.0720, -0.4971],\n",
            "        [-1.0198,  1.0024],\n",
            "        [ 0.4225, -0.7118],\n",
            "        [-0.3788,  0.5021],\n",
            "        [-0.6937,  0.7119],\n",
            "        [ 0.1049, -0.2661],\n",
            "        [ 0.0786, -0.7736],\n",
            "        [-0.9229,  0.6791],\n",
            "        [ 0.6578, -0.9415],\n",
            "        [ 0.4166, -0.7529],\n",
            "        [-0.6580,  0.7798],\n",
            "        [ 0.5265, -0.9518],\n",
            "        [ 0.3665, -0.6437],\n",
            "        [ 0.8556, -0.6401],\n",
            "        [ 0.3676, -0.4177],\n",
            "        [-0.2861,  0.2835],\n",
            "        [ 0.1949, -0.5432],\n",
            "        [ 0.1013, -0.4200],\n",
            "        [-0.4788,  0.8613]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4664, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6046,  0.7912],\n",
            "        [-0.9843,  1.1746],\n",
            "        [ 0.3727,  0.0856],\n",
            "        [ 0.4615, -0.6153],\n",
            "        [-0.6091,  0.6188],\n",
            "        [-0.7026,  1.1001],\n",
            "        [ 0.2567, -0.4372],\n",
            "        [-0.4499,  0.5678],\n",
            "        [ 0.1710, -0.6561],\n",
            "        [-0.8303,  1.0429],\n",
            "        [-0.3855,  0.5830],\n",
            "        [ 0.4851, -0.7984],\n",
            "        [-0.6923,  1.0995],\n",
            "        [ 0.2455, -0.4561],\n",
            "        [ 0.4158, -0.6881],\n",
            "        [ 0.0595, -0.7684],\n",
            "        [-0.9403,  1.1375],\n",
            "        [ 0.4200, -0.8948],\n",
            "        [ 0.3350, -0.6319],\n",
            "        [ 0.1060,  0.0538],\n",
            "        [-1.0576,  1.1530],\n",
            "        [ 0.4124, -0.7084],\n",
            "        [-0.7738,  0.9164],\n",
            "        [ 0.4755, -0.9046],\n",
            "        [ 0.2465, -0.3761],\n",
            "        [ 0.1250, -0.0738],\n",
            "        [ 0.0118, -0.3064],\n",
            "        [ 0.3641, -0.5220],\n",
            "        [-0.6685,  0.9953],\n",
            "        [-0.1090, -0.1282],\n",
            "        [ 0.7304, -0.6393],\n",
            "        [ 0.0929,  0.1194]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2544, -0.3045],\n",
            "        [-0.0238, -0.6619],\n",
            "        [ 0.2324, -0.7837],\n",
            "        [-0.6296,  0.7409],\n",
            "        [-0.0970,  0.2821],\n",
            "        [ 0.2961, -0.6072],\n",
            "        [ 0.7289, -0.6952],\n",
            "        [ 0.2344, -0.4692],\n",
            "        [ 0.3660, -0.5485],\n",
            "        [-0.1428,  0.2592],\n",
            "        [ 0.2761, -0.8654],\n",
            "        [-0.7542,  1.0170],\n",
            "        [ 0.4083, -0.4874],\n",
            "        [-0.6872,  0.6529],\n",
            "        [-0.7862,  0.9959],\n",
            "        [ 0.3144, -0.8692],\n",
            "        [ 0.5080, -0.8197],\n",
            "        [ 0.2967, -0.8223],\n",
            "        [-0.8469,  0.9623],\n",
            "        [ 0.2829, -0.5971],\n",
            "        [ 0.5262, -0.7135],\n",
            "        [ 0.3378, -0.8890],\n",
            "        [ 0.3154, -0.2074],\n",
            "        [-0.0635, -0.5273],\n",
            "        [ 0.2747, -0.4272],\n",
            "        [-0.4185,  0.2365],\n",
            "        [ 0.6391, -0.5537],\n",
            "        [ 0.5104, -0.6380],\n",
            "        [ 0.1671, -0.4081],\n",
            "        [-0.5513,  0.4102],\n",
            "        [ 0.3822, -0.7749],\n",
            "        [ 0.1332, -0.3581]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2465,  0.4250],\n",
            "        [-0.7891,  0.5481],\n",
            "        [ 0.2558, -0.6380],\n",
            "        [ 0.6958, -0.8381],\n",
            "        [ 0.0716, -0.3463],\n",
            "        [-0.2723,  0.2568],\n",
            "        [ 0.2635, -0.5817],\n",
            "        [ 0.1744, -0.6753],\n",
            "        [ 0.3131, -0.5866],\n",
            "        [ 0.5286, -0.6033],\n",
            "        [ 0.3512, -0.2869],\n",
            "        [ 0.1471, -0.1722],\n",
            "        [ 0.0095, -0.4300],\n",
            "        [ 0.1400, -0.3639],\n",
            "        [ 0.2687, -0.4848],\n",
            "        [ 0.3335, -0.6640],\n",
            "        [ 0.1479, -0.5417],\n",
            "        [ 0.6402, -0.9776],\n",
            "        [-0.9164,  0.6314],\n",
            "        [-1.0812,  1.0104],\n",
            "        [ 0.0727, -0.6764],\n",
            "        [ 0.6896, -0.7897],\n",
            "        [-0.2941,  0.3380],\n",
            "        [ 0.4048, -0.7851],\n",
            "        [-0.7977,  1.1094],\n",
            "        [ 0.5178, -0.2654],\n",
            "        [ 0.5223, -0.6167],\n",
            "        [ 0.7522, -1.0043],\n",
            "        [ 0.5129, -0.9049],\n",
            "        [ 0.5741, -0.7092],\n",
            "        [-0.0018, -0.0801],\n",
            "        [ 0.2980, -0.6653]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4479,  0.4845],\n",
            "        [ 0.6309, -0.5247],\n",
            "        [ 0.2924, -0.6928],\n",
            "        [ 0.6374, -0.8524],\n",
            "        [ 0.1644, -0.4605],\n",
            "        [ 0.7480, -0.6471],\n",
            "        [ 0.6408, -0.8062],\n",
            "        [ 0.3275, -0.2528],\n",
            "        [ 0.0369, -0.0432],\n",
            "        [-0.0973,  0.4060],\n",
            "        [-0.3896,  0.5996],\n",
            "        [-0.9667,  1.0222],\n",
            "        [ 0.0150, -0.0105],\n",
            "        [ 0.2037, -0.7777],\n",
            "        [-0.1736, -0.1556],\n",
            "        [ 0.6199, -0.9032],\n",
            "        [ 0.1952, -0.4204],\n",
            "        [ 0.4284, -0.6188],\n",
            "        [ 0.6579, -0.9269],\n",
            "        [-1.0804,  0.9019],\n",
            "        [ 0.3365, -0.6905],\n",
            "        [-0.8493,  1.0791],\n",
            "        [ 0.3551, -0.4061],\n",
            "        [ 0.2925, -0.5880],\n",
            "        [-0.5723,  0.7540],\n",
            "        [ 0.5257, -0.9575],\n",
            "        [ 0.1764, -0.3992],\n",
            "        [-0.1575, -0.2286],\n",
            "        [-0.7037,  1.1510],\n",
            "        [ 0.3860, -0.6417],\n",
            "        [ 0.2974, -0.8826],\n",
            "        [ 0.5014, -0.6660]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1347, -0.5576],\n",
            "        [-0.3707,  0.5967],\n",
            "        [-0.7591,  0.8734],\n",
            "        [-0.7641,  0.8432],\n",
            "        [-0.2753,  0.5657],\n",
            "        [-0.0535, -0.1336],\n",
            "        [ 0.5046, -0.4711],\n",
            "        [-0.9413,  1.1988],\n",
            "        [-0.4846,  0.7126],\n",
            "        [-0.9213,  1.1594],\n",
            "        [-0.3685,  0.4809],\n",
            "        [ 0.0717, -0.1788],\n",
            "        [ 0.5086, -0.8043],\n",
            "        [ 0.3637, -0.7263],\n",
            "        [ 0.0407, -0.1980],\n",
            "        [ 0.6742, -0.8142],\n",
            "        [-0.1097, -0.5238],\n",
            "        [ 0.0664, -0.2721],\n",
            "        [ 0.6342, -0.5969],\n",
            "        [ 0.1841, -0.5571],\n",
            "        [ 0.3227, -0.6985],\n",
            "        [-0.5046,  0.9583],\n",
            "        [ 0.5738, -0.5747],\n",
            "        [ 0.2036, -0.5856],\n",
            "        [ 0.6629, -1.0184],\n",
            "        [-0.5844,  0.3032],\n",
            "        [ 0.6436, -0.7470],\n",
            "        [ 0.5491, -0.6270],\n",
            "        [ 0.2951, -0.4199],\n",
            "        [-0.9063,  0.9835],\n",
            "        [ 0.6701, -0.5582],\n",
            "        [ 0.0363,  0.0452]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3377,  0.6182],\n",
            "        [ 0.6591, -0.6864],\n",
            "        [ 0.5469, -0.8428],\n",
            "        [ 0.2276, -0.1568],\n",
            "        [-0.6248,  0.5218],\n",
            "        [ 0.5728, -0.5622],\n",
            "        [ 0.2691, -0.8316],\n",
            "        [-0.6191,  0.9414],\n",
            "        [ 0.7247, -0.7339],\n",
            "        [ 0.1459, -0.4723],\n",
            "        [-0.2583,  0.5059],\n",
            "        [ 0.3492, -0.2590],\n",
            "        [ 0.2550, -0.6065],\n",
            "        [ 0.4536, -0.7511],\n",
            "        [ 0.4251, -0.8233],\n",
            "        [ 0.5983, -0.6073],\n",
            "        [ 0.0184, -0.2134],\n",
            "        [-0.0448,  0.5229],\n",
            "        [-0.2622, -0.0328],\n",
            "        [ 0.0801, -0.0612],\n",
            "        [ 0.8332, -0.8789],\n",
            "        [ 0.2019, -0.0477],\n",
            "        [-0.6589,  1.0875],\n",
            "        [ 0.0150, -0.5545],\n",
            "        [-0.6125,  0.6460],\n",
            "        [ 0.1109,  0.0232],\n",
            "        [ 0.3758, -0.5626],\n",
            "        [ 0.2506, -0.6227],\n",
            "        [ 0.6179, -1.1946],\n",
            "        [ 0.3640, -0.8947],\n",
            "        [ 0.5249, -0.2401],\n",
            "        [ 0.2405, -0.3147]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8789,  0.9338],\n",
            "        [-0.6612,  0.9841],\n",
            "        [ 0.1366, -0.6923],\n",
            "        [ 0.1609, -0.5608],\n",
            "        [-0.6087,  1.1610],\n",
            "        [ 0.2386, -0.3419],\n",
            "        [-0.9247,  1.0668],\n",
            "        [ 0.4171, -0.6785],\n",
            "        [ 0.6640, -0.7961],\n",
            "        [ 0.0684, -0.3996],\n",
            "        [ 0.4202, -0.7812],\n",
            "        [ 0.3873, -0.5552],\n",
            "        [ 0.5110, -0.5704],\n",
            "        [ 0.2845, -0.6869],\n",
            "        [ 0.5319, -0.6921],\n",
            "        [-0.9409,  1.3098],\n",
            "        [ 0.5615, -0.7955],\n",
            "        [-1.0131,  0.8526],\n",
            "        [ 0.2833, -0.3982],\n",
            "        [-0.2243,  0.7317],\n",
            "        [ 0.2597, -0.4042],\n",
            "        [ 0.3860, -0.7233],\n",
            "        [ 0.1754, -0.3638],\n",
            "        [ 0.0725,  0.1822],\n",
            "        [ 0.2243, -0.6421],\n",
            "        [ 0.1758, -0.5064],\n",
            "        [ 0.6491, -0.6558],\n",
            "        [-0.5211,  0.8465],\n",
            "        [-0.2199,  0.2540],\n",
            "        [ 0.3717, -0.6234],\n",
            "        [ 0.5123, -0.7329],\n",
            "        [ 0.5192, -0.5355]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4471, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5482, -0.4534],\n",
            "        [ 0.2954, -0.8653],\n",
            "        [ 0.4413, -0.6592],\n",
            "        [ 0.6042, -0.7752],\n",
            "        [ 0.3393, -0.7302],\n",
            "        [ 0.2043, -0.1981],\n",
            "        [ 0.4857, -0.4192],\n",
            "        [ 0.0241, -0.0203],\n",
            "        [-0.7825,  0.7797],\n",
            "        [ 0.5574, -0.6422],\n",
            "        [ 0.4064, -0.6454],\n",
            "        [-0.6026,  0.4869],\n",
            "        [-0.1083, -0.0975],\n",
            "        [ 0.3664, -0.6959],\n",
            "        [-0.7323,  0.9702],\n",
            "        [ 0.5841, -0.7114],\n",
            "        [ 0.3580, -0.4975],\n",
            "        [ 0.4045, -0.5145],\n",
            "        [ 0.4959, -0.7098],\n",
            "        [ 0.0335, -0.2285],\n",
            "        [ 0.2245, -0.8566],\n",
            "        [ 0.3560, -0.3786],\n",
            "        [-0.8779,  1.0866],\n",
            "        [-0.1874,  0.4941],\n",
            "        [ 0.6452, -0.7670],\n",
            "        [ 0.5257, -0.5968],\n",
            "        [-0.4203,  0.5363],\n",
            "        [ 0.5858, -0.8636],\n",
            "        [ 0.1755, -0.4679],\n",
            "        [ 0.5730, -1.0500],\n",
            "        [-0.7695,  0.9259],\n",
            "        [-0.2942,  0.3672]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8372,  0.7786],\n",
            "        [-0.1949,  0.3045],\n",
            "        [-0.2992,  0.3357],\n",
            "        [ 0.6521, -0.7630],\n",
            "        [-0.6334,  1.0731],\n",
            "        [ 0.4982, -0.6271],\n",
            "        [ 0.1816, -0.4623],\n",
            "        [-0.3671,  0.5670],\n",
            "        [ 0.4354, -0.5297],\n",
            "        [-0.1045,  0.1508],\n",
            "        [ 0.4311, -0.5255],\n",
            "        [-0.9125,  0.8839],\n",
            "        [ 0.5903, -0.6602],\n",
            "        [ 0.2747, -0.7348],\n",
            "        [ 0.3073, -0.5856],\n",
            "        [ 0.6206, -0.9697],\n",
            "        [ 0.4808, -0.5685],\n",
            "        [ 0.4485, -0.6098],\n",
            "        [ 0.4960, -0.7129],\n",
            "        [ 0.5905, -0.9325],\n",
            "        [ 0.7432, -0.7374],\n",
            "        [ 0.5058, -0.5446],\n",
            "        [-0.7029,  1.0403],\n",
            "        [-0.1363,  0.1878],\n",
            "        [-0.3367,  0.3574],\n",
            "        [ 0.6066, -0.5392],\n",
            "        [-0.8121,  1.0354],\n",
            "        [-0.7985,  1.2067],\n",
            "        [ 0.0565, -0.2936],\n",
            "        [ 0.5549, -0.6084],\n",
            "        [-0.6181,  0.8728],\n",
            "        [-0.8311,  1.1324]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3681, -0.8926],\n",
            "        [ 0.1371, -0.6510],\n",
            "        [ 0.3656, -0.4839],\n",
            "        [ 0.4404, -0.6451],\n",
            "        [ 0.6320, -0.7158],\n",
            "        [ 0.5443, -0.9618],\n",
            "        [ 0.5179, -0.6482],\n",
            "        [-0.6685,  1.0209],\n",
            "        [ 0.4149, -0.5333],\n",
            "        [ 0.5165, -0.6712],\n",
            "        [-0.2029,  0.0402],\n",
            "        [ 0.6222, -0.6069],\n",
            "        [ 0.3371, -0.5691],\n",
            "        [ 0.7622, -0.7860],\n",
            "        [-0.7186,  0.7176],\n",
            "        [ 0.5741, -0.6171],\n",
            "        [ 0.5065, -0.5205],\n",
            "        [ 0.3588, -0.2942],\n",
            "        [ 0.3016, -0.5541],\n",
            "        [-0.5312,  0.9777],\n",
            "        [ 0.5452, -0.6366],\n",
            "        [-0.5666,  0.8367],\n",
            "        [-0.6836,  1.1558],\n",
            "        [-0.2231, -0.0790],\n",
            "        [ 0.1772, -0.6057],\n",
            "        [-1.2326,  1.2632],\n",
            "        [ 0.4985, -0.7764],\n",
            "        [ 0.4849, -0.6061],\n",
            "        [-0.1667,  0.5315],\n",
            "        [ 0.4426, -0.5468],\n",
            "        [ 0.5054, -0.8431],\n",
            "        [ 0.6840, -0.6682]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1397,  1.2136],\n",
            "        [-0.4084,  0.6181],\n",
            "        [ 0.2030,  0.2034],\n",
            "        [ 0.5912, -0.8727],\n",
            "        [-0.8025,  0.8696],\n",
            "        [-0.0463,  0.1437],\n",
            "        [-0.3341,  0.7887],\n",
            "        [ 0.2363, -0.5559],\n",
            "        [-0.3397,  0.3362],\n",
            "        [ 0.3790, -0.6317],\n",
            "        [ 0.7569, -0.5988],\n",
            "        [ 0.6608, -0.9262],\n",
            "        [ 0.8130, -0.7435],\n",
            "        [ 0.1840, -0.2110],\n",
            "        [ 0.4485, -0.5300],\n",
            "        [-0.9037,  1.1099],\n",
            "        [ 0.5486, -0.8432],\n",
            "        [ 0.3350, -0.5947],\n",
            "        [ 0.7618, -0.8352],\n",
            "        [ 0.5716, -0.8755],\n",
            "        [ 0.5454, -0.8412],\n",
            "        [-0.7634,  0.9314],\n",
            "        [ 0.6464, -0.8448],\n",
            "        [ 0.7467, -0.8766],\n",
            "        [-0.7166,  0.3377],\n",
            "        [-0.5042,  0.7144],\n",
            "        [ 0.3487, -0.1697],\n",
            "        [-0.1977,  0.2311],\n",
            "        [ 0.1906, -0.1369],\n",
            "        [ 0.5824, -0.9149],\n",
            "        [ 0.6829, -0.7628],\n",
            "        [-0.6127,  0.9105]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0227e-01, -2.5373e-01],\n",
            "        [ 1.5136e-01, -6.3022e-01],\n",
            "        [-9.0271e-01,  8.9390e-01],\n",
            "        [-8.4718e-01,  1.0674e+00],\n",
            "        [ 5.4877e-01, -3.1878e-01],\n",
            "        [ 4.7047e-01, -3.9357e-01],\n",
            "        [ 5.2623e-01, -7.9019e-01],\n",
            "        [ 6.6079e-01, -8.1983e-01],\n",
            "        [ 4.0658e-01, -6.6719e-01],\n",
            "        [ 5.4678e-02, -4.2421e-01],\n",
            "        [-7.0356e-01,  8.8720e-01],\n",
            "        [-7.6341e-01,  1.0479e+00],\n",
            "        [-1.6731e-01,  6.4709e-01],\n",
            "        [-7.6870e-02,  2.0481e-01],\n",
            "        [-3.9426e-01,  1.0278e+00],\n",
            "        [-2.1007e-01,  4.5969e-01],\n",
            "        [-1.0352e-01,  2.1064e-01],\n",
            "        [ 5.2697e-01, -4.2173e-01],\n",
            "        [-7.0671e-01,  1.1936e+00],\n",
            "        [-7.3952e-01,  7.9502e-01],\n",
            "        [ 1.1891e-03, -4.5910e-01],\n",
            "        [ 4.2444e-01, -1.0422e+00],\n",
            "        [-3.7531e-01,  6.6348e-01],\n",
            "        [-2.3093e-01,  1.2301e-01],\n",
            "        [-9.0193e-01,  1.0833e+00],\n",
            "        [-9.3313e-01,  9.0208e-01],\n",
            "        [ 6.0337e-01, -9.1994e-01],\n",
            "        [ 2.2389e-01, -5.6611e-01],\n",
            "        [ 4.7065e-01, -6.9239e-01],\n",
            "        [ 3.5592e-01,  2.8323e-02],\n",
            "        [-5.4263e-01,  5.2000e-01],\n",
            "        [ 5.6529e-01, -9.7785e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0373,  0.4692],\n",
            "        [ 0.6247, -0.7942],\n",
            "        [ 0.5063, -0.9760],\n",
            "        [ 0.1737, -0.6940],\n",
            "        [ 0.6923, -0.9303],\n",
            "        [ 0.6938, -0.5189],\n",
            "        [ 0.3557, -0.5767],\n",
            "        [ 0.5095, -0.8241],\n",
            "        [ 0.7018, -0.6035],\n",
            "        [ 0.5076, -0.3676],\n",
            "        [ 0.5912, -0.7300],\n",
            "        [-0.0963,  0.4939],\n",
            "        [ 0.3653, -0.7190],\n",
            "        [-0.3912,  0.2125],\n",
            "        [ 0.0439,  0.2254],\n",
            "        [ 0.3003, -0.6157],\n",
            "        [ 0.5834, -0.7170],\n",
            "        [ 0.7663, -0.9394],\n",
            "        [ 0.6859, -0.7477],\n",
            "        [ 0.6785, -0.8328],\n",
            "        [-0.5663,  0.6002],\n",
            "        [-0.8981,  1.0934],\n",
            "        [ 0.6065, -0.8924],\n",
            "        [ 0.0660, -0.3398],\n",
            "        [ 0.7577, -0.9159],\n",
            "        [ 0.2427, -0.4821],\n",
            "        [ 0.6917, -0.7988],\n",
            "        [ 0.5740, -0.6875],\n",
            "        [-0.7538,  1.0234],\n",
            "        [ 0.4647, -0.8072],\n",
            "        [ 0.5003, -0.6093],\n",
            "        [-0.5613,  0.8067]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5679, -0.8460],\n",
            "        [-0.6935,  1.1174],\n",
            "        [-0.4947,  0.7315],\n",
            "        [-0.3282,  0.5705],\n",
            "        [ 0.5444, -0.6226],\n",
            "        [-0.1277, -0.2070],\n",
            "        [ 0.5586, -0.6241],\n",
            "        [-0.2277,  0.3842],\n",
            "        [ 0.0429, -0.0847],\n",
            "        [-0.5812,  0.9422],\n",
            "        [ 0.4301, -0.4456],\n",
            "        [ 0.4096, -0.8150],\n",
            "        [ 0.6414, -0.6409],\n",
            "        [ 0.2833, -1.0118],\n",
            "        [ 0.3604, -0.7607],\n",
            "        [ 0.5546, -0.8861],\n",
            "        [ 0.3526, -0.9126],\n",
            "        [ 0.3957, -0.8470],\n",
            "        [ 0.1044, -0.4026],\n",
            "        [ 0.4562, -0.8619],\n",
            "        [-0.6372,  0.6716],\n",
            "        [ 0.7477, -0.7469],\n",
            "        [ 0.1411, -0.4515],\n",
            "        [ 0.4382, -0.6007],\n",
            "        [-0.1350,  0.2549],\n",
            "        [-0.6328,  0.8532],\n",
            "        [ 0.3924, -0.5040],\n",
            "        [ 0.5508, -0.9951],\n",
            "        [ 0.3950, -0.5001],\n",
            "        [-0.5728,  0.8918],\n",
            "        [-0.9632,  0.9198],\n",
            "        [ 0.0644, -0.1400]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5316, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4464, -1.0614],\n",
            "        [ 0.4114, -0.2982],\n",
            "        [ 0.5204, -0.8055],\n",
            "        [ 0.5612, -0.4413],\n",
            "        [ 0.3874, -0.5825],\n",
            "        [ 0.3091,  0.0909],\n",
            "        [-0.2187,  0.0728],\n",
            "        [ 0.6424, -0.5549],\n",
            "        [ 0.7078, -0.7511],\n",
            "        [ 0.6433, -0.4641],\n",
            "        [ 0.0127, -0.1520],\n",
            "        [ 0.1433, -0.6600],\n",
            "        [ 0.8765, -0.8951],\n",
            "        [-0.7358,  1.0690],\n",
            "        [ 0.4143, -0.7575],\n",
            "        [-0.1514,  0.4337],\n",
            "        [ 0.1177, -0.4954],\n",
            "        [ 0.3183, -0.6522],\n",
            "        [-0.4017,  0.3966],\n",
            "        [ 0.2130, -0.5070],\n",
            "        [ 0.5494, -0.2787],\n",
            "        [-0.6214,  0.9261],\n",
            "        [ 0.3364, -0.6994],\n",
            "        [ 0.2436, -0.1353],\n",
            "        [-0.5391,  1.1378],\n",
            "        [ 0.1805, -0.1837],\n",
            "        [-0.0165,  0.2969],\n",
            "        [ 0.5860, -0.8111],\n",
            "        [-0.8516,  0.9741],\n",
            "        [ 0.5407, -0.6887],\n",
            "        [-0.4075,  0.6722],\n",
            "        [ 0.4585, -0.4830]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5726, -0.8421],\n",
            "        [-0.1123,  0.0700],\n",
            "        [-0.9750,  1.2239],\n",
            "        [ 0.1325, -0.8827],\n",
            "        [ 0.2505, -0.1266],\n",
            "        [-0.4990,  0.8771],\n",
            "        [ 0.4609, -0.2158],\n",
            "        [-0.6420,  1.1392],\n",
            "        [-0.8112,  0.6562],\n",
            "        [-0.2842,  0.4898],\n",
            "        [ 0.4280, -0.3040],\n",
            "        [-0.2868,  0.6377],\n",
            "        [ 0.0534,  0.2868],\n",
            "        [ 0.2928, -0.6299],\n",
            "        [ 0.4239, -0.4511],\n",
            "        [ 0.5469, -0.6690],\n",
            "        [ 0.2202, -0.8269],\n",
            "        [-0.0954,  0.1550],\n",
            "        [ 0.0274,  0.4088],\n",
            "        [-0.6768,  0.7835],\n",
            "        [-0.7381,  0.9610],\n",
            "        [ 0.7669, -0.6554],\n",
            "        [ 0.2131, -0.1318],\n",
            "        [-0.0543, -0.3266],\n",
            "        [-0.8665,  0.9305],\n",
            "        [ 0.4834, -0.5138],\n",
            "        [ 0.4631, -0.8180],\n",
            "        [-0.8746,  1.1549],\n",
            "        [ 0.4017, -0.7323],\n",
            "        [-0.5196,  0.6337],\n",
            "        [ 0.5612, -0.6218],\n",
            "        [ 0.6744, -0.7473]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9178,  0.8946],\n",
            "        [-0.5735,  0.6562],\n",
            "        [ 0.6111, -0.7234],\n",
            "        [ 0.5455, -0.5446],\n",
            "        [ 0.1866,  0.2921],\n",
            "        [-0.6373,  0.4874],\n",
            "        [ 0.6092, -0.3734],\n",
            "        [ 0.4786, -0.7165],\n",
            "        [ 0.3462, -0.4345],\n",
            "        [-0.8131,  0.9338],\n",
            "        [-0.0853,  0.2572],\n",
            "        [-0.2930,  0.0853],\n",
            "        [ 0.6016, -0.8061],\n",
            "        [ 0.3973, -0.6220],\n",
            "        [-1.1411,  1.1410],\n",
            "        [-0.8762,  1.0651],\n",
            "        [ 0.5713, -0.8577],\n",
            "        [-0.9631,  1.0973],\n",
            "        [ 0.3038, -0.5786],\n",
            "        [-0.8704,  1.1299],\n",
            "        [ 0.4077, -0.6231],\n",
            "        [ 0.4532, -0.7899],\n",
            "        [ 0.4301, -0.8509],\n",
            "        [ 0.5153, -0.6320],\n",
            "        [ 0.7467, -0.6626],\n",
            "        [ 0.6467, -0.9066],\n",
            "        [-0.6165,  0.8121],\n",
            "        [-0.5436,  0.6848],\n",
            "        [ 0.6655, -0.7187],\n",
            "        [ 0.5147, -0.9292],\n",
            "        [ 0.4205, -0.4775],\n",
            "        [ 0.2934, -0.3612]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2389, -0.3668],\n",
            "        [ 0.4975, -0.7267],\n",
            "        [ 0.3811, -0.6203],\n",
            "        [ 0.6071, -0.6858],\n",
            "        [ 0.3989, -0.5913],\n",
            "        [ 0.6034, -0.7090],\n",
            "        [-1.1032,  1.1550],\n",
            "        [-0.3411,  0.4393],\n",
            "        [-0.4118,  0.6054],\n",
            "        [ 0.4117, -0.4205],\n",
            "        [-0.9370,  0.7769],\n",
            "        [ 0.4676, -0.7899],\n",
            "        [-0.8401,  0.9971],\n",
            "        [-0.5269,  0.9521],\n",
            "        [ 0.6989, -0.7764],\n",
            "        [-0.8542,  1.0332],\n",
            "        [ 0.2669, -0.8745],\n",
            "        [-0.5334,  0.5988],\n",
            "        [ 0.5868, -0.6440],\n",
            "        [ 0.3698, -0.1834],\n",
            "        [-0.5912,  0.6464],\n",
            "        [ 0.6103, -0.7864],\n",
            "        [ 0.3951, -1.0171],\n",
            "        [ 0.4732, -1.1935],\n",
            "        [ 0.5082, -0.5731],\n",
            "        [ 0.4513, -0.8460],\n",
            "        [ 0.1000, -0.1644],\n",
            "        [ 0.2612, -0.5021],\n",
            "        [ 0.4804, -0.8565],\n",
            "        [ 0.4495, -0.7988],\n",
            "        [ 0.7615, -1.0467],\n",
            "        [ 0.5360, -0.5085]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8283,  1.2771],\n",
            "        [-0.4338,  0.7055],\n",
            "        [-0.9645,  0.9498],\n",
            "        [-0.5853,  0.6985],\n",
            "        [-0.9379,  1.2135],\n",
            "        [ 0.4486, -0.8003],\n",
            "        [-0.6099,  0.5786],\n",
            "        [ 0.2936, -0.4437],\n",
            "        [-1.2162,  1.0706],\n",
            "        [-1.0995,  1.1954],\n",
            "        [-1.1360,  1.0418],\n",
            "        [-0.8735,  1.0293],\n",
            "        [ 0.5247, -0.5653],\n",
            "        [-0.9696,  1.0738],\n",
            "        [-0.0066, -0.2026],\n",
            "        [-0.3150,  0.1155],\n",
            "        [ 0.3625, -0.1228],\n",
            "        [ 0.7329, -0.8904],\n",
            "        [-0.8020,  0.9827],\n",
            "        [-0.2036,  0.2273],\n",
            "        [ 0.5743, -0.6506],\n",
            "        [ 0.1947, -0.4940],\n",
            "        [-0.9332,  0.7789],\n",
            "        [-0.3813,  0.7520],\n",
            "        [ 0.0020,  0.1719],\n",
            "        [ 0.0254,  0.2397],\n",
            "        [ 0.4720, -0.6571],\n",
            "        [-0.7920,  1.1128],\n",
            "        [ 0.7121, -1.0466],\n",
            "        [ 0.6470, -0.8405],\n",
            "        [ 0.4913, -0.7125],\n",
            "        [-0.1704,  0.1424]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch   100  of    191.    Elapsed: 0:02:23.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4336, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9510,  0.7980],\n",
            "        [-1.0148,  1.1886],\n",
            "        [ 0.6835, -0.9178],\n",
            "        [ 0.4345, -0.4601],\n",
            "        [-0.0272,  0.0204],\n",
            "        [ 0.2965, -0.4074],\n",
            "        [ 0.4063, -0.6095],\n",
            "        [ 0.6391, -0.9417],\n",
            "        [-0.4447,  0.5856],\n",
            "        [ 0.5302, -0.8745],\n",
            "        [ 0.4512, -0.6390],\n",
            "        [ 0.5932, -0.8885],\n",
            "        [ 0.3416, -0.4911],\n",
            "        [-0.6539,  0.9807],\n",
            "        [ 0.3326, -0.8468],\n",
            "        [-0.1578, -0.0036],\n",
            "        [ 0.0692,  0.3273],\n",
            "        [-0.8688,  1.0605],\n",
            "        [ 0.6637, -0.7141],\n",
            "        [ 0.2985, -0.3279],\n",
            "        [-0.9303,  0.9693],\n",
            "        [ 0.4737, -0.6956],\n",
            "        [ 0.4799, -0.6849],\n",
            "        [-0.9371,  1.1899],\n",
            "        [ 0.4511, -0.3754],\n",
            "        [-0.7668,  1.2069],\n",
            "        [ 0.4576, -0.5340],\n",
            "        [ 0.7141, -0.8072],\n",
            "        [ 0.1496, -0.6057],\n",
            "        [ 0.4558, -0.9244],\n",
            "        [ 0.5961, -0.6517],\n",
            "        [ 0.4539, -0.6333]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1879,  0.0621],\n",
            "        [ 0.7769, -0.7612],\n",
            "        [-0.0650,  0.1880],\n",
            "        [-0.7921,  0.7386],\n",
            "        [-0.2571,  0.3607],\n",
            "        [ 0.6326, -0.6837],\n",
            "        [ 0.5248, -0.7270],\n",
            "        [ 0.4344, -0.4272],\n",
            "        [-0.7749,  0.9749],\n",
            "        [ 0.5668, -0.5473],\n",
            "        [-0.2400,  0.2276],\n",
            "        [-0.9034,  0.9364],\n",
            "        [ 0.3399, -0.6024],\n",
            "        [-0.8768,  1.0014],\n",
            "        [ 0.2779, -0.5972],\n",
            "        [-0.1623, -0.0894],\n",
            "        [-0.7258,  1.0202],\n",
            "        [ 0.0034,  0.3647],\n",
            "        [-0.7636,  1.1264],\n",
            "        [ 0.4140, -0.8365],\n",
            "        [ 0.5196, -0.8806],\n",
            "        [ 0.2173, -0.4084],\n",
            "        [ 0.4363, -0.8040],\n",
            "        [ 0.5882, -0.5625],\n",
            "        [-0.0224, -0.2996],\n",
            "        [-0.1497, -0.1055],\n",
            "        [ 0.1083, -0.1545],\n",
            "        [ 0.3340, -0.6625],\n",
            "        [-0.1886,  0.1679],\n",
            "        [ 0.5749, -0.6398],\n",
            "        [ 0.3298, -0.6162],\n",
            "        [ 0.5554, -0.7733]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5601,  0.7703],\n",
            "        [ 0.4662, -0.7236],\n",
            "        [ 0.3152, -0.4869],\n",
            "        [ 0.4979, -0.7140],\n",
            "        [-0.0995, -0.2034],\n",
            "        [ 0.2734, -0.7368],\n",
            "        [ 0.7066, -1.0001],\n",
            "        [-0.7732,  1.0015],\n",
            "        [-0.5829,  0.2438],\n",
            "        [-0.2086,  0.1146],\n",
            "        [ 0.2613, -0.6060],\n",
            "        [-0.6114,  0.8867],\n",
            "        [-0.2643,  0.4737],\n",
            "        [ 0.1672, -0.3697],\n",
            "        [ 0.9303, -0.7054],\n",
            "        [ 0.6889, -0.7799],\n",
            "        [-0.9169,  1.0092],\n",
            "        [ 0.3333, -0.1410],\n",
            "        [ 0.4552, -0.6799],\n",
            "        [ 0.5725, -0.5129],\n",
            "        [ 0.0998, -0.1163],\n",
            "        [ 0.5470, -0.5754],\n",
            "        [-0.3398,  0.5075],\n",
            "        [-0.5385,  0.9224],\n",
            "        [-0.8051,  1.1343],\n",
            "        [ 0.1353, -0.5732],\n",
            "        [ 0.5832, -1.2304],\n",
            "        [ 0.7500, -1.0495],\n",
            "        [-0.5488,  0.7157],\n",
            "        [-0.8653,  0.8744],\n",
            "        [ 0.5301, -0.6038],\n",
            "        [ 0.5935, -0.4345]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5140, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6317, -0.7139],\n",
            "        [ 0.8779, -0.9952],\n",
            "        [ 0.7303, -0.8961],\n",
            "        [-0.7878,  0.8888],\n",
            "        [ 0.0403, -0.2182],\n",
            "        [ 0.2735, -0.4515],\n",
            "        [ 0.5764, -0.7028],\n",
            "        [-0.7515,  0.7874],\n",
            "        [-0.3689,  0.5844],\n",
            "        [ 0.6104, -0.7083],\n",
            "        [-0.1640,  0.0015],\n",
            "        [ 0.6262, -0.6975],\n",
            "        [ 0.4856, -0.7487],\n",
            "        [ 0.3740, -0.0879],\n",
            "        [ 0.3233, -0.6275],\n",
            "        [ 0.4830, -0.4471],\n",
            "        [ 0.4823, -0.4279],\n",
            "        [ 0.8980, -1.0294],\n",
            "        [ 0.6169, -0.1764],\n",
            "        [-0.6026,  0.8652],\n",
            "        [ 0.5661, -0.5149],\n",
            "        [-0.3875,  0.5740],\n",
            "        [ 0.5712, -0.7199],\n",
            "        [ 0.0712, -0.3675],\n",
            "        [ 0.1925, -0.5657],\n",
            "        [ 0.5780, -0.6364],\n",
            "        [ 0.0860, -0.1369],\n",
            "        [ 0.5215, -0.6783],\n",
            "        [ 0.3975, -0.1996],\n",
            "        [ 0.5597, -0.7199],\n",
            "        [ 0.4444, -0.8294],\n",
            "        [ 0.3906, -0.8153]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7928,  1.0058],\n",
            "        [-0.9706,  1.1851],\n",
            "        [ 0.5925, -0.6042],\n",
            "        [ 0.6827, -0.6035],\n",
            "        [-0.9706,  1.0780],\n",
            "        [ 0.1325, -0.7585],\n",
            "        [ 0.5834, -0.7035],\n",
            "        [ 0.6958, -0.7400],\n",
            "        [ 0.7199, -0.8838],\n",
            "        [ 0.6231, -0.7436],\n",
            "        [ 0.7219, -0.5801],\n",
            "        [ 0.7592, -0.8679],\n",
            "        [ 0.4211, -0.8360],\n",
            "        [-0.0447, -0.1594],\n",
            "        [-0.6396,  0.9868],\n",
            "        [ 0.5436, -0.8374],\n",
            "        [ 0.8094, -0.7026],\n",
            "        [ 0.2102, -0.4845],\n",
            "        [ 0.2386, -0.7449],\n",
            "        [-0.8765,  1.1694],\n",
            "        [ 0.5843, -0.7661],\n",
            "        [-0.8604,  1.1675],\n",
            "        [ 0.4486, -0.9050],\n",
            "        [ 0.2861, -0.3168],\n",
            "        [-0.7276,  0.7311],\n",
            "        [ 0.1187, -0.5563],\n",
            "        [-0.7552,  1.1516],\n",
            "        [-0.1190,  0.3454],\n",
            "        [ 0.6498, -0.7928],\n",
            "        [ 0.3926, -0.1153],\n",
            "        [ 0.9456, -0.8361],\n",
            "        [ 0.1254, -0.4331]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7655,  0.6892],\n",
            "        [-0.8287,  1.0120],\n",
            "        [ 0.0841,  0.1504],\n",
            "        [ 0.5001, -0.6800],\n",
            "        [-0.5686,  0.4467],\n",
            "        [ 0.3533, -0.7691],\n",
            "        [ 0.1303, -0.4181],\n",
            "        [ 0.1799, -0.4779],\n",
            "        [-0.2190,  0.0326],\n",
            "        [-0.8688,  1.1574],\n",
            "        [ 0.3570, -0.6458],\n",
            "        [ 0.6258, -0.7464],\n",
            "        [ 0.2554, -0.7509],\n",
            "        [ 0.5281, -0.7542],\n",
            "        [ 0.4399, -0.8084],\n",
            "        [ 0.8199, -0.7618],\n",
            "        [ 0.5425, -0.8094],\n",
            "        [-0.3445,  0.1133],\n",
            "        [ 0.5815, -0.8920],\n",
            "        [ 0.3882, -0.8656],\n",
            "        [-0.2851,  0.0793],\n",
            "        [-0.4789,  0.5796],\n",
            "        [-0.4805,  0.6141],\n",
            "        [-0.7622,  1.0348],\n",
            "        [-0.7768,  0.8517],\n",
            "        [-0.6504,  0.9746],\n",
            "        [ 0.7021, -0.8937],\n",
            "        [ 0.7141, -0.8960],\n",
            "        [ 0.7055, -0.8353],\n",
            "        [-0.2328,  0.4714],\n",
            "        [-0.9178,  0.7206],\n",
            "        [-0.6931,  0.5953]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4499, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1253, -0.6000],\n",
            "        [-0.1488,  0.1025],\n",
            "        [-0.1314,  0.3269],\n",
            "        [ 0.4115, -0.9411],\n",
            "        [-0.9295,  1.1786],\n",
            "        [ 0.1908, -0.6481],\n",
            "        [ 0.5238, -0.9609],\n",
            "        [ 0.7963, -1.0349],\n",
            "        [-0.8819,  1.1142],\n",
            "        [ 0.5158, -0.8147],\n",
            "        [-0.5942,  0.8019],\n",
            "        [ 0.6866, -0.7580],\n",
            "        [ 0.2817, -0.3328],\n",
            "        [-0.8687,  1.2085],\n",
            "        [ 0.5968, -0.4952],\n",
            "        [ 0.3677, -0.5854],\n",
            "        [-0.1265,  0.0773],\n",
            "        [ 0.5598, -0.7706],\n",
            "        [-0.8339,  0.8062],\n",
            "        [ 0.4817, -0.8431],\n",
            "        [ 0.2334, -0.4623],\n",
            "        [-0.6088,  0.9926],\n",
            "        [-0.7710,  1.0827],\n",
            "        [ 0.0325,  0.0378],\n",
            "        [ 0.3249, -0.1275],\n",
            "        [ 0.1642, -0.3077],\n",
            "        [-0.9017,  0.9375],\n",
            "        [-0.9004,  1.0435],\n",
            "        [-0.8349,  1.1160],\n",
            "        [ 0.5775, -0.5637],\n",
            "        [ 0.6639, -1.0727],\n",
            "        [-0.7298,  0.8233]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8460,  0.9357],\n",
            "        [ 0.6310, -0.8055],\n",
            "        [ 0.4723, -0.8006],\n",
            "        [-0.9475,  1.0386],\n",
            "        [ 0.7897, -0.7937],\n",
            "        [-0.1507,  0.0949],\n",
            "        [ 0.7981, -0.6852],\n",
            "        [ 0.5365, -0.9830],\n",
            "        [ 0.5611, -0.9132],\n",
            "        [-0.8347,  0.9210],\n",
            "        [ 0.6569, -0.7202],\n",
            "        [-0.8137,  0.9382],\n",
            "        [ 0.0761, -0.1961],\n",
            "        [-0.3737,  0.3970],\n",
            "        [ 0.6286, -0.9443],\n",
            "        [ 0.4571, -0.8829],\n",
            "        [ 0.5038, -1.1384],\n",
            "        [-0.6109,  0.5362],\n",
            "        [-0.9357,  0.9532],\n",
            "        [ 0.5136, -0.5910],\n",
            "        [ 0.5006, -0.3871],\n",
            "        [ 0.2988, -0.0861],\n",
            "        [ 0.6907, -1.1121],\n",
            "        [-1.0955,  1.1720],\n",
            "        [ 0.4520, -0.5455],\n",
            "        [ 0.4901, -0.4619],\n",
            "        [ 0.5918, -0.7921],\n",
            "        [-0.3094,  0.5188],\n",
            "        [ 0.4734, -0.2933],\n",
            "        [-0.3886,  0.4431],\n",
            "        [ 0.5022, -0.4599],\n",
            "        [-0.5070,  0.6212]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1244,  0.9304],\n",
            "        [ 0.6493, -0.8614],\n",
            "        [ 0.5934, -0.7265],\n",
            "        [ 0.7243, -0.4742],\n",
            "        [ 0.7021, -0.6062],\n",
            "        [ 0.7998, -0.7716],\n",
            "        [ 0.4233, -0.6853],\n",
            "        [-0.7191,  0.5838],\n",
            "        [ 0.3582, -0.7036],\n",
            "        [ 0.5124, -0.7099],\n",
            "        [ 0.2419, -0.5217],\n",
            "        [-0.9557,  1.0613],\n",
            "        [ 0.8730, -0.9527],\n",
            "        [-0.8225,  1.0535],\n",
            "        [ 0.4546, -0.3077],\n",
            "        [ 0.4581, -0.9342],\n",
            "        [ 0.0450, -0.1992],\n",
            "        [ 0.5599, -0.7535],\n",
            "        [ 0.4107, -0.8354],\n",
            "        [ 0.4518, -0.7713],\n",
            "        [ 0.3302, -0.4759],\n",
            "        [ 0.4579, -0.6509],\n",
            "        [-0.8575,  0.9331],\n",
            "        [-0.9156,  0.9992],\n",
            "        [-0.5928,  0.7190],\n",
            "        [ 0.5787, -0.6526],\n",
            "        [ 0.4995, -0.7055],\n",
            "        [ 0.1167, -0.2786],\n",
            "        [ 0.3687, -0.6269],\n",
            "        [ 0.5213, -0.1938],\n",
            "        [-0.9822,  0.8322],\n",
            "        [ 0.4143, -0.8014]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0405, -0.6325],\n",
            "        [-0.1226,  0.3023],\n",
            "        [ 0.2422, -0.4267],\n",
            "        [ 0.2131, -0.3852],\n",
            "        [ 0.4762, -1.2315],\n",
            "        [ 0.3855, -0.7735],\n",
            "        [-0.8719,  1.0945],\n",
            "        [ 0.5820, -0.8902],\n",
            "        [-1.1152,  1.1053],\n",
            "        [-0.0413,  0.1249],\n",
            "        [-0.9123,  1.0906],\n",
            "        [ 0.4192, -0.2022],\n",
            "        [ 0.5158, -0.7703],\n",
            "        [ 0.5168, -0.5708],\n",
            "        [-1.0315,  1.0240],\n",
            "        [ 0.4226, -0.6996],\n",
            "        [ 0.2145, -0.2786],\n",
            "        [ 0.2448, -0.1826],\n",
            "        [-0.4233,  0.6276],\n",
            "        [ 0.6458, -0.7126],\n",
            "        [ 0.5182, -0.9535],\n",
            "        [ 0.2948, -0.3101],\n",
            "        [ 0.5092, -0.7760],\n",
            "        [ 0.3684, -0.6040],\n",
            "        [ 0.6306, -0.7576],\n",
            "        [ 0.4656, -0.8546],\n",
            "        [ 0.1951, -0.4084],\n",
            "        [-0.2265,  0.6829],\n",
            "        [ 0.5590, -0.5858],\n",
            "        [ 0.5395, -0.8490],\n",
            "        [-1.1101,  1.0502],\n",
            "        [ 0.2332, -0.4515]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3165, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6232, -0.9429],\n",
            "        [-1.1539,  1.1138],\n",
            "        [ 0.3135,  0.0713],\n",
            "        [ 0.5365, -0.9693],\n",
            "        [ 0.3595, -0.5978],\n",
            "        [-0.8709,  1.0505],\n",
            "        [ 0.6854, -0.9111],\n",
            "        [ 0.4976, -0.9521],\n",
            "        [ 0.1554, -0.4850],\n",
            "        [-0.9509,  1.2320],\n",
            "        [-1.0208,  1.2853],\n",
            "        [ 0.3413, -0.5953],\n",
            "        [ 0.4087, -0.9785],\n",
            "        [-0.2777,  0.2092],\n",
            "        [ 0.5866, -0.7630],\n",
            "        [ 0.3199, -0.5958],\n",
            "        [-0.8564,  0.9481],\n",
            "        [ 0.2465, -0.6520],\n",
            "        [ 0.3104, -0.2493],\n",
            "        [ 0.7898, -0.7476],\n",
            "        [ 0.3697, -0.2377],\n",
            "        [-0.6690,  0.6630],\n",
            "        [ 0.4535, -0.6745],\n",
            "        [ 0.5673, -0.5708],\n",
            "        [-0.7728,  0.9950],\n",
            "        [ 0.4649, -0.8852],\n",
            "        [ 0.6195, -0.9084],\n",
            "        [-1.0640,  1.4499],\n",
            "        [ 0.3660, -0.3199],\n",
            "        [-0.8667,  0.5699],\n",
            "        [ 0.6951, -0.6978],\n",
            "        [ 0.0658, -0.1702]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1199, -0.4459],\n",
            "        [-0.2134, -0.0318],\n",
            "        [-0.1012,  0.1228],\n",
            "        [-0.7612,  1.0865],\n",
            "        [-0.5470,  0.5168],\n",
            "        [ 0.8264, -0.9765],\n",
            "        [ 0.0828, -0.0974],\n",
            "        [ 0.6628, -0.7989],\n",
            "        [ 0.4939, -0.9015],\n",
            "        [-1.0804,  0.9325],\n",
            "        [-0.0381, -0.1533],\n",
            "        [ 0.5942, -0.5138],\n",
            "        [ 0.0767, -0.0661],\n",
            "        [-0.2560, -0.1039],\n",
            "        [ 0.1549, -0.8386],\n",
            "        [ 0.5106, -0.5137],\n",
            "        [ 0.5336, -0.9314],\n",
            "        [ 0.6027, -0.6694],\n",
            "        [ 0.5064, -0.6691],\n",
            "        [ 0.4586, -0.6162],\n",
            "        [-0.7623,  1.2635],\n",
            "        [ 0.3911, -0.8639],\n",
            "        [ 0.1924, -0.4918],\n",
            "        [-0.4067,  0.3311],\n",
            "        [ 0.2735, -0.2207],\n",
            "        [ 0.3301, -0.5384],\n",
            "        [-0.1783, -0.1754],\n",
            "        [ 0.4455, -0.9067],\n",
            "        [ 0.2532, -0.5302],\n",
            "        [-0.0617, -0.0702],\n",
            "        [ 0.4686, -0.9275],\n",
            "        [-0.6692,  0.8571]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4975, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5326, -0.9044],\n",
            "        [ 0.0922, -0.2344],\n",
            "        [ 0.9170, -0.9609],\n",
            "        [-0.5708,  0.3463],\n",
            "        [ 0.8967, -0.8749],\n",
            "        [ 0.6922, -0.9346],\n",
            "        [ 0.6854, -0.8038],\n",
            "        [ 0.4263, -0.7736],\n",
            "        [-0.9248,  1.2162],\n",
            "        [ 0.7249, -0.8089],\n",
            "        [ 0.7848, -0.8150],\n",
            "        [-0.4458,  0.6534],\n",
            "        [ 0.7201, -0.9436],\n",
            "        [ 0.4218, -0.8454],\n",
            "        [-0.6279,  0.9069],\n",
            "        [ 0.1122, -0.2045],\n",
            "        [ 0.5919, -0.9971],\n",
            "        [-0.1997,  0.5389],\n",
            "        [ 0.6305, -0.7149],\n",
            "        [ 0.2343, -0.3127],\n",
            "        [ 0.5268, -0.7552],\n",
            "        [ 0.5436, -0.5427],\n",
            "        [ 0.1962, -0.7101],\n",
            "        [ 0.4508, -0.7352],\n",
            "        [ 0.3504, -0.6301],\n",
            "        [ 0.4758, -0.2349],\n",
            "        [ 0.4796, -0.9285],\n",
            "        [-0.9408,  0.9463],\n",
            "        [ 0.6386, -0.8453],\n",
            "        [ 0.5907, -1.0247],\n",
            "        [-0.8460,  1.0894],\n",
            "        [ 0.0230, -0.3510]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4338, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5307, -0.5224],\n",
            "        [ 0.5431, -0.8733],\n",
            "        [ 0.8633, -0.8911],\n",
            "        [-0.8364,  0.6704],\n",
            "        [ 0.5215, -0.6595],\n",
            "        [ 0.3085, -0.8812],\n",
            "        [-0.1876,  0.5392],\n",
            "        [-0.5565,  0.6712],\n",
            "        [-0.1095,  0.2870],\n",
            "        [ 0.8124, -0.7356],\n",
            "        [-0.7162,  1.0659],\n",
            "        [ 0.7734, -0.6449],\n",
            "        [ 0.5670, -0.9675],\n",
            "        [ 0.7705, -0.6735],\n",
            "        [-0.7152,  1.2033],\n",
            "        [-0.7804,  1.0753],\n",
            "        [ 0.8270, -0.8355],\n",
            "        [ 0.1742, -0.5735],\n",
            "        [ 0.7000, -0.7672],\n",
            "        [-0.0781,  0.2126],\n",
            "        [ 0.3468, -0.4891],\n",
            "        [ 0.7877, -0.7744],\n",
            "        [-0.0917,  0.1304],\n",
            "        [ 0.6553, -0.6241],\n",
            "        [ 0.7429, -0.7028],\n",
            "        [ 0.6745, -0.5783],\n",
            "        [ 0.8419, -0.7153],\n",
            "        [-0.8600,  1.3466],\n",
            "        [ 0.0881, -0.7990],\n",
            "        [-0.9510,  0.9320],\n",
            "        [-0.5703,  1.2930],\n",
            "        [ 0.1569, -0.3912]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3653, -0.4517],\n",
            "        [-1.1838,  1.2607],\n",
            "        [-0.2129, -0.0714],\n",
            "        [ 0.5314, -0.6424],\n",
            "        [ 0.6329, -0.7871],\n",
            "        [-0.2380,  0.3072],\n",
            "        [-0.3123, -0.0568],\n",
            "        [-0.5471,  0.6531],\n",
            "        [ 0.7132, -1.0949],\n",
            "        [ 0.6595, -1.1950],\n",
            "        [ 0.4627, -0.7657],\n",
            "        [-0.7030,  0.7516],\n",
            "        [-0.8184,  1.1935],\n",
            "        [-0.0612,  0.0623],\n",
            "        [ 0.4186, -0.7990],\n",
            "        [ 0.6000, -0.6681],\n",
            "        [-0.9479,  1.2926],\n",
            "        [-0.9205,  1.1334],\n",
            "        [ 0.4800, -0.6840],\n",
            "        [ 0.7858, -0.9285],\n",
            "        [ 0.5475, -0.6976],\n",
            "        [-0.6930,  0.4659],\n",
            "        [ 0.5102, -0.6377],\n",
            "        [-1.1239,  1.1398],\n",
            "        [-0.8340,  1.2368],\n",
            "        [ 0.9838, -1.1164],\n",
            "        [ 0.4333, -0.5008],\n",
            "        [ 0.3314, -0.7429],\n",
            "        [ 0.3754, -0.9841],\n",
            "        [ 0.5743, -0.9354],\n",
            "        [ 0.3922, -0.9350],\n",
            "        [ 0.4790, -0.9568]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5060, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6213, -0.8114],\n",
            "        [ 0.7159, -0.9240],\n",
            "        [ 0.8598, -0.8164],\n",
            "        [ 0.3625, -0.9404],\n",
            "        [ 0.6371, -0.8140],\n",
            "        [ 0.4420, -0.4440],\n",
            "        [ 0.3925, -0.8456],\n",
            "        [-0.6895,  0.2838],\n",
            "        [-0.1399, -0.2686],\n",
            "        [ 0.5324, -1.0302],\n",
            "        [ 0.1832, -0.0992],\n",
            "        [-0.2541,  0.4408],\n",
            "        [ 0.4321, -0.9569],\n",
            "        [ 0.7742, -0.7647],\n",
            "        [ 0.3521, -0.1620],\n",
            "        [-0.4923,  0.5462],\n",
            "        [ 0.5330, -0.5335],\n",
            "        [-0.6343,  0.7457],\n",
            "        [ 0.7234, -0.8415],\n",
            "        [-0.6943,  0.5721],\n",
            "        [-0.7467,  1.1059],\n",
            "        [-1.0467,  0.6614],\n",
            "        [-0.7007,  1.2875],\n",
            "        [-1.0677,  0.9456],\n",
            "        [ 0.5899, -0.5375],\n",
            "        [-0.6165,  0.8542],\n",
            "        [ 0.3525, -0.2866],\n",
            "        [ 0.6494, -0.7706],\n",
            "        [ 0.2150, -0.1787],\n",
            "        [ 0.5797, -0.9250],\n",
            "        [ 0.5693, -0.7015],\n",
            "        [ 0.3582, -0.8174]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2304,  0.3515],\n",
            "        [ 0.4141, -0.4041],\n",
            "        [ 0.6201, -0.4894],\n",
            "        [ 0.5264, -0.5953],\n",
            "        [-0.8159,  0.9847],\n",
            "        [ 0.3773, -0.3644],\n",
            "        [ 0.5783, -0.7212],\n",
            "        [ 0.4699, -0.8795],\n",
            "        [-0.0656, -0.0364],\n",
            "        [ 0.6589, -0.9542],\n",
            "        [-0.9933,  1.1655],\n",
            "        [-0.1924,  0.2557],\n",
            "        [-0.5435,  0.3794],\n",
            "        [ 0.6914, -0.9313],\n",
            "        [-0.7939,  1.3670],\n",
            "        [ 0.4558, -0.8251],\n",
            "        [-0.4849,  0.5217],\n",
            "        [-1.0299,  1.2544],\n",
            "        [-0.6429,  0.5363],\n",
            "        [-0.9510,  1.1819],\n",
            "        [ 0.5672, -1.0885],\n",
            "        [-1.1968,  1.1650],\n",
            "        [ 0.7817, -0.8985],\n",
            "        [ 0.3246, -0.4278],\n",
            "        [-0.9503,  1.1119],\n",
            "        [-0.6756,  0.5011],\n",
            "        [-0.7870,  0.7356],\n",
            "        [ 0.6902, -0.9370],\n",
            "        [ 0.6772, -0.9939],\n",
            "        [ 0.6431, -0.7773],\n",
            "        [-0.4537,  0.4151],\n",
            "        [ 0.5226, -0.6610]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4380, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7291, -0.9362],\n",
            "        [ 0.5896, -0.7032],\n",
            "        [-0.1439, -0.2612],\n",
            "        [-1.0406,  0.9436],\n",
            "        [-0.8992,  1.1912],\n",
            "        [ 0.1752, -0.2087],\n",
            "        [-0.1508,  0.0487],\n",
            "        [ 0.6707, -0.9524],\n",
            "        [ 0.5884, -0.5011],\n",
            "        [ 0.7937, -0.6980],\n",
            "        [ 0.5185, -0.7198],\n",
            "        [-0.3719,  0.4143],\n",
            "        [ 0.0669,  0.4079],\n",
            "        [-1.0305,  1.1013],\n",
            "        [ 0.6035, -1.0278],\n",
            "        [-0.5893,  0.5364],\n",
            "        [ 0.4168, -0.9673],\n",
            "        [ 0.7071, -1.1827],\n",
            "        [ 0.8086, -0.7588],\n",
            "        [ 0.6789, -0.9862],\n",
            "        [-0.9487,  0.7793],\n",
            "        [-1.0287,  1.2006],\n",
            "        [-0.2049,  0.2453],\n",
            "        [ 0.2388, -0.7967],\n",
            "        [-0.8167,  0.8691],\n",
            "        [-0.9044,  1.0329],\n",
            "        [-0.3997,  0.2002],\n",
            "        [-0.1018, -0.4494],\n",
            "        [-0.8588,  1.1790],\n",
            "        [ 0.4379, -0.3764],\n",
            "        [ 0.5005, -0.8658],\n",
            "        [ 0.2023, -0.3786]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4615, -0.7036],\n",
            "        [ 0.6419, -0.8823],\n",
            "        [ 0.5870, -0.8173],\n",
            "        [-0.4621,  0.3483],\n",
            "        [ 0.5280, -0.7125],\n",
            "        [ 0.3411, -0.4750],\n",
            "        [ 0.3448, -0.4489],\n",
            "        [ 0.5591, -1.1910],\n",
            "        [-0.0320,  0.0792],\n",
            "        [ 0.5772, -0.9382],\n",
            "        [ 0.6492, -0.5481],\n",
            "        [-0.9669,  0.8998],\n",
            "        [ 0.2869, -0.5357],\n",
            "        [-0.8862,  1.2013],\n",
            "        [ 0.5776, -0.6906],\n",
            "        [ 0.5634, -0.6572],\n",
            "        [ 0.6983, -0.5347],\n",
            "        [-0.8881,  1.2283],\n",
            "        [ 0.6386, -0.8857],\n",
            "        [ 0.4654, -0.7374],\n",
            "        [-0.6375,  1.0953],\n",
            "        [-0.6674,  0.7832],\n",
            "        [ 0.7496, -0.7003],\n",
            "        [ 0.5036, -0.8563],\n",
            "        [ 0.8388, -0.6717],\n",
            "        [ 0.3081, -0.7553],\n",
            "        [ 0.5423, -1.1681],\n",
            "        [ 0.4963, -0.7624],\n",
            "        [ 0.7596, -0.7049],\n",
            "        [ 0.8037, -0.9309],\n",
            "        [ 0.3620, -0.2696],\n",
            "        [ 0.7093, -0.7322]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3366, -0.5013],\n",
            "        [ 0.6592, -1.0566],\n",
            "        [ 0.4404, -0.5260],\n",
            "        [ 0.4857, -0.8536],\n",
            "        [ 0.5139, -0.6464],\n",
            "        [-1.1019,  1.0142],\n",
            "        [ 0.4547, -0.7665],\n",
            "        [ 0.7515, -0.8002],\n",
            "        [ 0.5894, -0.7675],\n",
            "        [ 0.6668, -0.9990],\n",
            "        [ 0.0101, -0.3345],\n",
            "        [ 0.5849, -0.7063],\n",
            "        [-1.0419,  1.0232],\n",
            "        [-0.8157,  0.9221],\n",
            "        [ 0.4852, -0.4378],\n",
            "        [-0.9302,  1.1981],\n",
            "        [-0.0732, -0.0059],\n",
            "        [ 0.7145, -0.9364],\n",
            "        [ 0.5012, -0.4860],\n",
            "        [-0.9222,  1.0663],\n",
            "        [ 0.1109, -0.0638],\n",
            "        [ 0.1628,  0.2579],\n",
            "        [-0.3619,  0.0361],\n",
            "        [-0.2726,  0.5221],\n",
            "        [ 0.2809, -0.6725],\n",
            "        [ 0.3137, -0.6995],\n",
            "        [ 0.4530, -0.8064],\n",
            "        [ 0.0147, -0.4405],\n",
            "        [ 0.6571, -0.7327],\n",
            "        [ 0.3862, -0.8698],\n",
            "        [ 0.3484, -0.6992],\n",
            "        [-0.9884,  1.2457]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3592, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1460,  1.1580],\n",
            "        [ 0.1686, -0.2235],\n",
            "        [ 0.0406, -0.3087],\n",
            "        [ 0.7462, -0.7446],\n",
            "        [-1.0184,  1.1705],\n",
            "        [ 0.8427, -0.8557],\n",
            "        [ 0.7971, -0.8666],\n",
            "        [-0.7258,  0.8972],\n",
            "        [ 0.3070, -0.5522],\n",
            "        [-0.0265, -0.3302],\n",
            "        [ 0.3251, -0.6469],\n",
            "        [ 0.8439, -0.7017],\n",
            "        [ 0.0700, -0.2789],\n",
            "        [ 0.0352, -0.6307],\n",
            "        [ 0.5313, -0.7085],\n",
            "        [ 0.5221, -0.8997],\n",
            "        [-0.9559,  1.1122],\n",
            "        [-0.9281,  1.1743],\n",
            "        [ 0.5237, -0.7057],\n",
            "        [-0.2795,  0.0449],\n",
            "        [ 0.6825, -0.9491],\n",
            "        [ 0.3948, -0.2848],\n",
            "        [ 0.5470, -0.4711],\n",
            "        [ 0.6609, -0.4829],\n",
            "        [ 0.3067,  0.1407],\n",
            "        [-0.3781,  0.2924],\n",
            "        [ 0.3080, -0.5671],\n",
            "        [-0.2635, -0.0458],\n",
            "        [ 0.4459, -1.0032],\n",
            "        [ 0.5368, -0.8429],\n",
            "        [-1.1073,  0.9077],\n",
            "        [-1.0337,  1.2736]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6803, -1.1165],\n",
            "        [-0.5706,  1.0073],\n",
            "        [ 0.5563, -0.4829],\n",
            "        [ 0.6765, -0.8701],\n",
            "        [-1.0401,  1.3254],\n",
            "        [ 0.0596, -0.1847],\n",
            "        [ 0.4515, -0.5402],\n",
            "        [ 0.5025, -0.7857],\n",
            "        [ 0.1732, -0.2090],\n",
            "        [-0.7675,  0.8855],\n",
            "        [-0.7221,  1.1926],\n",
            "        [-1.0949,  1.3528],\n",
            "        [-0.7935,  0.5034],\n",
            "        [ 0.8001, -0.8218],\n",
            "        [-1.1976,  1.1050],\n",
            "        [ 0.3443, -0.5776],\n",
            "        [ 0.5083, -0.8222],\n",
            "        [ 0.4752, -0.7119],\n",
            "        [-1.0961,  1.0825],\n",
            "        [-0.5689,  1.0343],\n",
            "        [ 0.5841, -0.7937],\n",
            "        [ 0.7917, -0.7748],\n",
            "        [ 0.5216, -0.8653],\n",
            "        [-0.7640,  0.8744],\n",
            "        [-0.9683,  1.1028],\n",
            "        [-0.9566,  1.1401],\n",
            "        [ 0.4252, -0.8539],\n",
            "        [ 0.6723, -0.8941],\n",
            "        [ 0.4108, -0.8744],\n",
            "        [ 0.5363, -0.4717],\n",
            "        [ 0.6012, -0.9354],\n",
            "        [-0.9583,  0.9537]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4493, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5876,  0.9453],\n",
            "        [-0.6450,  0.5272],\n",
            "        [ 0.4469, -0.7190],\n",
            "        [ 0.3605, -0.7014],\n",
            "        [ 0.2203, -0.7171],\n",
            "        [-0.9740,  1.0812],\n",
            "        [ 0.1026, -0.6860],\n",
            "        [ 0.6507, -0.6587],\n",
            "        [-0.1997,  0.1401],\n",
            "        [ 0.9693, -0.7533],\n",
            "        [-1.0742,  1.1096],\n",
            "        [-0.0265, -0.1119],\n",
            "        [-0.2255,  0.0983],\n",
            "        [-0.7810,  1.1280],\n",
            "        [ 0.3489, -0.5045],\n",
            "        [-0.1055,  0.1739],\n",
            "        [-0.7625,  1.1584],\n",
            "        [-1.0220,  1.2835],\n",
            "        [ 0.7983, -0.9426],\n",
            "        [ 0.4908, -0.8104],\n",
            "        [ 0.6180, -0.8494],\n",
            "        [ 0.1049, -0.3462],\n",
            "        [-0.9584,  1.0956],\n",
            "        [-0.3932,  0.4460],\n",
            "        [ 0.4184, -0.6035],\n",
            "        [-0.5969,  1.0279],\n",
            "        [-0.8201,  1.2497],\n",
            "        [ 0.5779, -0.6726],\n",
            "        [-0.6949,  0.6481],\n",
            "        [-0.8192,  0.9155],\n",
            "        [ 0.5261, -0.7560],\n",
            "        [ 0.6029, -0.9837]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1491, -0.5714],\n",
            "        [-0.8568,  0.9370],\n",
            "        [-0.2795, -0.0429],\n",
            "        [ 0.7262, -0.6479],\n",
            "        [ 0.5416, -0.7017],\n",
            "        [ 0.2139, -0.1636],\n",
            "        [ 0.5422, -0.7383],\n",
            "        [ 0.8098, -0.7050],\n",
            "        [ 0.6156, -1.0256],\n",
            "        [ 0.0379, -0.1319],\n",
            "        [ 0.0724, -0.5693],\n",
            "        [-0.9821,  1.1618],\n",
            "        [ 0.0145, -0.3967],\n",
            "        [-0.6484,  0.8559],\n",
            "        [ 0.1588, -0.3551],\n",
            "        [ 0.2895, -0.5667],\n",
            "        [ 0.5930, -1.1668],\n",
            "        [ 0.0761, -0.3378],\n",
            "        [ 0.6713, -0.9205],\n",
            "        [-0.7866,  0.9936],\n",
            "        [ 0.0175, -0.0380],\n",
            "        [ 0.5449, -0.7858],\n",
            "        [-0.6655,  0.6844],\n",
            "        [ 0.3375, -0.4882],\n",
            "        [ 0.3119, -0.1769],\n",
            "        [ 0.3654, -0.7476],\n",
            "        [ 0.5783, -0.6604],\n",
            "        [ 0.8302, -0.6802],\n",
            "        [-0.3001,  0.3316],\n",
            "        [ 0.9289, -0.7646],\n",
            "        [-0.3854,  0.4031],\n",
            "        [ 0.7631, -0.6954]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5140, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2762, -0.0851],\n",
            "        [-0.8515,  1.0619],\n",
            "        [ 0.4864, -0.7946],\n",
            "        [ 0.6418, -1.0884],\n",
            "        [ 0.3463, -0.8333],\n",
            "        [ 0.6653, -0.7114],\n",
            "        [ 0.6624, -0.8776],\n",
            "        [-0.7961,  0.9270],\n",
            "        [-0.8262,  1.2560],\n",
            "        [ 0.6448, -0.4436],\n",
            "        [ 0.5555, -0.6626],\n",
            "        [-0.1632,  0.3909],\n",
            "        [ 0.0034,  0.2494],\n",
            "        [ 0.4305, -0.5256],\n",
            "        [ 0.7599, -0.8281],\n",
            "        [ 0.2665, -0.8441],\n",
            "        [-0.2119,  0.2146],\n",
            "        [-0.9349,  1.1280],\n",
            "        [-0.2432,  0.4765],\n",
            "        [-1.1253,  1.2639],\n",
            "        [-0.9098,  1.4600],\n",
            "        [-1.1505,  1.1730],\n",
            "        [ 0.8802, -0.8813],\n",
            "        [-0.9112,  0.9178],\n",
            "        [ 0.3270, -0.5900],\n",
            "        [ 0.5734, -0.9698],\n",
            "        [ 0.1105, -0.4756],\n",
            "        [ 0.6908, -0.7467],\n",
            "        [ 0.5144, -0.6387],\n",
            "        [-1.1473,  1.4003],\n",
            "        [ 0.7570, -0.6583],\n",
            "        [ 0.6131, -0.8267]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4508, -0.7002],\n",
            "        [-0.8761,  0.8439],\n",
            "        [-0.8647,  1.2760],\n",
            "        [-0.9957,  1.1450],\n",
            "        [-0.2689,  0.0168],\n",
            "        [-1.1575,  1.0595],\n",
            "        [-0.6402,  1.0418],\n",
            "        [-0.8993,  1.2319],\n",
            "        [ 0.7044, -0.9152],\n",
            "        [ 0.3660, -0.8995],\n",
            "        [ 0.0569, -0.0139],\n",
            "        [ 0.3597, -0.4913],\n",
            "        [ 0.4578, -0.6926],\n",
            "        [-0.9709,  1.2134],\n",
            "        [ 0.1139, -0.6421],\n",
            "        [-1.0259,  1.1245],\n",
            "        [ 0.3894, -0.5817],\n",
            "        [ 0.3676, -0.7469],\n",
            "        [-0.4729,  0.5177],\n",
            "        [ 0.3173, -0.7698],\n",
            "        [ 0.1344, -0.9136],\n",
            "        [-0.4744,  0.6241],\n",
            "        [ 0.5466, -0.1676],\n",
            "        [ 0.4042, -0.5604],\n",
            "        [ 0.1136, -0.2091],\n",
            "        [ 0.5332, -0.7688],\n",
            "        [ 0.2114, -0.4377],\n",
            "        [ 0.6746, -0.7386],\n",
            "        [ 0.6889, -1.0573],\n",
            "        [-0.5575,  0.7548],\n",
            "        [-0.7850,  0.9132],\n",
            "        [ 0.7050, -0.9326]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2523,  1.3771],\n",
            "        [-1.0639,  1.1492],\n",
            "        [-0.8635,  1.0448],\n",
            "        [ 0.3954, -0.5711],\n",
            "        [-1.0131,  1.1590],\n",
            "        [ 0.5409, -0.1794],\n",
            "        [ 0.3071, -0.4032],\n",
            "        [-0.7495,  0.9599],\n",
            "        [ 0.4561, -0.7743],\n",
            "        [ 0.3808, -0.7497],\n",
            "        [ 0.4382, -0.5731],\n",
            "        [ 0.5826, -0.7861],\n",
            "        [-1.1883,  1.0079],\n",
            "        [ 0.8100, -1.1291],\n",
            "        [ 0.4574, -0.7858],\n",
            "        [-0.8170,  1.0781],\n",
            "        [ 0.6125, -0.9344],\n",
            "        [-0.8791,  1.1524],\n",
            "        [ 0.1771, -0.0601],\n",
            "        [-1.1055,  1.3610],\n",
            "        [-0.7989,  1.3126],\n",
            "        [ 0.3391, -0.2589],\n",
            "        [ 0.7647, -0.7863],\n",
            "        [-0.9006,  1.0343],\n",
            "        [ 0.7491, -0.9162],\n",
            "        [ 0.5592, -0.9376],\n",
            "        [-0.3156,  0.7509],\n",
            "        [-0.7968,  0.9285],\n",
            "        [ 0.2389, -0.6947],\n",
            "        [ 0.1212, -0.2235],\n",
            "        [ 0.5606, -0.8255],\n",
            "        [-0.5582,  0.4405]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7647, -0.6226],\n",
            "        [ 0.5112, -0.4464],\n",
            "        [ 0.3944, -0.6683],\n",
            "        [-0.6193,  0.6014],\n",
            "        [-0.0680,  0.3006],\n",
            "        [ 0.0825, -0.0889],\n",
            "        [ 0.5688, -0.6907],\n",
            "        [ 0.5970, -0.6873],\n",
            "        [ 0.4849, -0.8311],\n",
            "        [-0.2666, -0.1155],\n",
            "        [-0.3641,  0.6483],\n",
            "        [ 0.6133, -0.8838],\n",
            "        [-0.2714,  0.1657],\n",
            "        [ 0.3631, -0.9612],\n",
            "        [-0.7314,  0.9659],\n",
            "        [ 0.3528, -0.7766],\n",
            "        [ 0.6266, -0.7674],\n",
            "        [-0.8190,  1.1396],\n",
            "        [ 0.6429, -0.8038],\n",
            "        [ 0.1814, -0.3209],\n",
            "        [ 0.6546, -0.5937],\n",
            "        [ 0.2628, -0.5425],\n",
            "        [ 0.7068, -0.6049],\n",
            "        [-1.0581,  0.9637],\n",
            "        [ 0.7158, -0.8515],\n",
            "        [ 0.8457, -0.7744],\n",
            "        [ 0.4973, -0.8726],\n",
            "        [-1.1089,  0.9838],\n",
            "        [ 0.4323, -0.6981],\n",
            "        [ 0.4966, -0.8071],\n",
            "        [ 0.4196, -0.6493],\n",
            "        [ 0.6779, -0.4966]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1911, -0.6775],\n",
            "        [ 0.4752, -0.8107],\n",
            "        [-1.0682,  1.0566],\n",
            "        [ 0.5117, -1.0110],\n",
            "        [-1.1125,  1.2281],\n",
            "        [ 0.5841, -0.7035],\n",
            "        [-1.0034,  1.2816],\n",
            "        [-0.7866,  0.9462],\n",
            "        [ 0.4559, -0.9462],\n",
            "        [-0.9502,  1.0557],\n",
            "        [ 0.2205, -0.6223],\n",
            "        [-0.8662,  1.0646],\n",
            "        [-0.2420,  0.3180],\n",
            "        [ 0.3517, -0.5830],\n",
            "        [-0.7389,  1.1582],\n",
            "        [ 0.6676, -0.9624],\n",
            "        [ 0.8760, -1.1862],\n",
            "        [-1.2806,  0.9648],\n",
            "        [-0.8580,  0.6143],\n",
            "        [ 0.6035, -0.7601],\n",
            "        [ 0.4950, -0.8960],\n",
            "        [ 0.3826, -0.4049],\n",
            "        [ 0.6257, -0.8216],\n",
            "        [ 0.6176, -0.8557],\n",
            "        [ 0.6898, -0.5850],\n",
            "        [-0.1207, -0.3816],\n",
            "        [-0.8567,  0.7995],\n",
            "        [ 0.4705, -0.3146],\n",
            "        [-0.1024,  0.1349],\n",
            "        [-1.2135,  0.9955],\n",
            "        [-0.7372,  0.8424],\n",
            "        [ 0.2855, -0.7503]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7260, -1.1327],\n",
            "        [ 0.1718, -0.0537],\n",
            "        [-0.3677,  0.7717],\n",
            "        [ 0.5136, -0.9508],\n",
            "        [-1.0146,  1.0727],\n",
            "        [ 0.7041, -0.6761],\n",
            "        [ 0.6406, -0.7543],\n",
            "        [ 0.9533, -1.0879],\n",
            "        [-1.1099,  1.4898],\n",
            "        [-0.6946,  0.8990],\n",
            "        [ 0.4611, -0.6742],\n",
            "        [-0.9409,  1.2793],\n",
            "        [ 0.6191, -0.9119],\n",
            "        [ 0.6735, -0.6572],\n",
            "        [ 0.5173,  0.0802],\n",
            "        [-0.4219,  0.5539],\n",
            "        [-0.3372,  0.7609],\n",
            "        [ 0.6461, -0.9499],\n",
            "        [-1.0128,  1.0479],\n",
            "        [ 0.4235, -0.5271],\n",
            "        [ 0.6028, -0.9948],\n",
            "        [-0.9022,  0.9904],\n",
            "        [ 0.6683, -0.8660],\n",
            "        [-1.0568,  1.1756],\n",
            "        [-0.0453, -0.7145],\n",
            "        [-1.1716,  1.3415],\n",
            "        [ 0.6485, -0.5926],\n",
            "        [ 0.8743, -1.1092],\n",
            "        [ 0.4427, -0.6369],\n",
            "        [ 0.0129, -0.3074],\n",
            "        [ 0.8030, -0.8086],\n",
            "        [ 0.8423, -1.0048]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4199, -0.8704],\n",
            "        [-0.4263,  0.8431],\n",
            "        [ 0.6506, -0.9930],\n",
            "        [ 0.6275, -0.8439],\n",
            "        [ 0.3228, -0.6105],\n",
            "        [-0.6397,  0.5609],\n",
            "        [ 0.5999, -0.7668],\n",
            "        [-1.0700,  1.2581],\n",
            "        [ 0.3263, -0.9700],\n",
            "        [ 0.5501, -0.5925],\n",
            "        [-0.9014,  0.9967],\n",
            "        [ 0.3218, -0.4552],\n",
            "        [-0.9067,  1.2171],\n",
            "        [ 0.7199, -0.7326],\n",
            "        [ 0.4851, -0.6183],\n",
            "        [ 0.8246, -0.8300],\n",
            "        [ 0.6164, -0.8160],\n",
            "        [-0.9234,  1.1149],\n",
            "        [ 0.4963, -0.4076],\n",
            "        [-1.0658,  1.4132],\n",
            "        [-1.0451,  1.1293],\n",
            "        [ 0.5528, -0.8616],\n",
            "        [ 0.5254, -0.7564],\n",
            "        [-0.6182,  0.7622],\n",
            "        [ 0.5790, -0.9899],\n",
            "        [ 0.5325, -1.1694],\n",
            "        [ 0.2124, -0.5759],\n",
            "        [ 0.6416, -0.8852],\n",
            "        [-0.9696,  1.0733],\n",
            "        [ 0.7278, -0.8178],\n",
            "        [ 0.4934, -0.8348],\n",
            "        [ 0.5725, -0.8358]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7916, -0.9288],\n",
            "        [ 0.6212, -0.7520],\n",
            "        [-0.7310,  1.0386],\n",
            "        [-0.8689,  1.2884],\n",
            "        [-0.7675,  1.0317],\n",
            "        [ 0.7280, -1.0060],\n",
            "        [-0.9785,  0.9947],\n",
            "        [ 0.4682, -0.3624],\n",
            "        [ 0.3680, -0.4926],\n",
            "        [-0.4493,  0.4776],\n",
            "        [-1.1685,  1.4595],\n",
            "        [ 0.5510, -0.6638],\n",
            "        [ 0.7931, -0.6461],\n",
            "        [-0.6777,  0.7420],\n",
            "        [ 0.1449, -0.2231],\n",
            "        [ 0.6846, -0.7779],\n",
            "        [ 0.5413, -0.7012],\n",
            "        [-0.5901,  0.5450],\n",
            "        [-1.1560,  1.0792],\n",
            "        [ 0.8553, -0.6949],\n",
            "        [-0.6548,  0.9777],\n",
            "        [-1.0382,  1.1744],\n",
            "        [-0.9800,  1.2825],\n",
            "        [ 0.5765, -0.7290],\n",
            "        [ 0.3740, -0.5600],\n",
            "        [ 0.5036, -0.4185],\n",
            "        [ 0.4867, -0.6449],\n",
            "        [-1.1077,  1.3415],\n",
            "        [-0.8099,  0.7083],\n",
            "        [-0.8619,  1.2311],\n",
            "        [-1.1368,  1.2475],\n",
            "        [-1.1251,  1.2297]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7315, -0.7942],\n",
            "        [ 0.4318, -0.7058],\n",
            "        [ 0.6051, -0.8578],\n",
            "        [ 0.0292, -0.4010],\n",
            "        [ 0.0750, -0.2104],\n",
            "        [ 0.6069, -0.8154],\n",
            "        [ 0.4768, -0.7309],\n",
            "        [ 0.2040, -0.2456],\n",
            "        [-0.7979,  1.0369],\n",
            "        [ 0.3278, -0.5458],\n",
            "        [ 1.0393, -1.0052],\n",
            "        [ 0.3580, -0.8868],\n",
            "        [ 0.3680, -0.2596],\n",
            "        [ 0.1906, -0.2630],\n",
            "        [-0.9940,  1.0064],\n",
            "        [ 0.7809, -0.9024],\n",
            "        [ 0.7404, -0.7855],\n",
            "        [ 0.7439, -0.8010],\n",
            "        [ 0.7695, -0.7509],\n",
            "        [-0.9370,  1.0881],\n",
            "        [ 0.7278, -1.0374],\n",
            "        [ 0.2886, -0.8216],\n",
            "        [-0.6838,  0.6281],\n",
            "        [ 0.7676, -0.8579],\n",
            "        [ 0.2227, -0.0955],\n",
            "        [ 0.5103, -1.2001],\n",
            "        [-0.9160,  0.9149],\n",
            "        [ 0.5781, -1.1285],\n",
            "        [-0.5035,  0.5365],\n",
            "        [-1.1320,  1.1757],\n",
            "        [-1.0512,  1.4279],\n",
            "        [-0.7636,  0.8908]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0537,  0.8822],\n",
            "        [ 0.5604, -0.8155],\n",
            "        [ 0.5437, -0.4991],\n",
            "        [-0.6944,  0.7667],\n",
            "        [ 0.8439, -0.9042],\n",
            "        [ 0.1224, -0.5868],\n",
            "        [ 0.4736, -0.5660],\n",
            "        [ 0.4947, -1.0999],\n",
            "        [ 0.4302, -0.6455],\n",
            "        [ 0.4258, -0.7869],\n",
            "        [-0.0072, -0.1023],\n",
            "        [ 0.6544, -0.7992],\n",
            "        [-0.9354,  1.1061],\n",
            "        [-1.0395,  1.2583],\n",
            "        [ 0.6214, -0.9473],\n",
            "        [-0.0825,  0.2968],\n",
            "        [ 0.7553, -0.9007],\n",
            "        [-0.2197,  0.3187],\n",
            "        [ 0.9307, -0.9698],\n",
            "        [ 0.7006, -0.7467],\n",
            "        [ 0.6917, -0.9342],\n",
            "        [ 0.3750, -0.5909],\n",
            "        [ 0.1998, -0.0206],\n",
            "        [-0.7057,  0.9770],\n",
            "        [ 0.7872, -0.8243],\n",
            "        [ 0.7994, -1.0424],\n",
            "        [ 0.4344, -0.6293],\n",
            "        [ 0.3737, -0.5521],\n",
            "        [-0.7915,  1.1125],\n",
            "        [ 0.5984, -0.6411],\n",
            "        [-0.7961,  1.0274],\n",
            "        [ 0.4507, -0.5311]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2280, -0.7698],\n",
            "        [ 0.7125, -0.8073],\n",
            "        [ 0.8004, -0.7212],\n",
            "        [ 0.6747, -0.8184],\n",
            "        [ 0.9186, -0.7454],\n",
            "        [-1.0887,  0.9811],\n",
            "        [-1.1330,  1.3893],\n",
            "        [ 0.4916, -0.4384],\n",
            "        [ 0.5293, -0.7416],\n",
            "        [-0.9123,  1.3998],\n",
            "        [ 0.7894, -1.0670],\n",
            "        [ 0.1490, -0.5466],\n",
            "        [ 0.5461, -0.2432],\n",
            "        [ 0.5580, -0.9503],\n",
            "        [ 0.9822, -1.0198],\n",
            "        [ 0.5111, -0.8808],\n",
            "        [ 0.8425, -0.9611],\n",
            "        [ 0.4314, -0.7085],\n",
            "        [ 0.7330, -1.0149],\n",
            "        [-0.7564,  0.9928],\n",
            "        [ 0.3025, -0.6304],\n",
            "        [ 0.5764, -1.0339],\n",
            "        [-0.5307,  0.9229],\n",
            "        [-1.0415,  1.4217],\n",
            "        [-0.8951,  0.9156],\n",
            "        [ 0.3221, -0.6484],\n",
            "        [-0.7977,  0.7409],\n",
            "        [-0.7289,  0.9829],\n",
            "        [ 0.8151, -0.8844],\n",
            "        [ 0.1637, -0.4123],\n",
            "        [-0.1469,  0.1586],\n",
            "        [-0.2629,  0.3713]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4854, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2431, -0.5699],\n",
            "        [-0.1186,  0.0456],\n",
            "        [-0.9406,  1.3707],\n",
            "        [-0.5694,  0.9699],\n",
            "        [ 0.6985, -0.8757],\n",
            "        [ 0.0484,  0.0199],\n",
            "        [ 0.4986, -0.4438],\n",
            "        [ 0.3018, -0.6294],\n",
            "        [ 0.2622, -0.8095],\n",
            "        [ 0.6512, -0.9296],\n",
            "        [ 0.2084, -0.1546],\n",
            "        [ 0.4816, -0.9423],\n",
            "        [ 0.7325, -1.0239],\n",
            "        [ 0.1050, -0.3455],\n",
            "        [-0.1970,  0.3478],\n",
            "        [-1.3582,  1.1778],\n",
            "        [ 0.7287, -0.9040],\n",
            "        [-0.9802,  1.1452],\n",
            "        [-0.7070,  0.8250],\n",
            "        [ 0.6404, -0.9044],\n",
            "        [ 0.7064, -0.9175],\n",
            "        [-0.9499,  1.0840],\n",
            "        [-1.0088,  0.8882],\n",
            "        [-0.5836,  0.4894],\n",
            "        [ 0.7317, -0.6926],\n",
            "        [ 0.6851, -0.7521],\n",
            "        [-0.5143,  0.5114],\n",
            "        [-0.2449,  0.4568],\n",
            "        [-0.5115,  0.9553],\n",
            "        [ 0.1131, -0.0139],\n",
            "        [-1.1335,  1.2890],\n",
            "        [ 0.4421, -0.4954]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3232, -0.8308],\n",
            "        [ 0.4172, -0.7136],\n",
            "        [ 0.4545, -0.4206],\n",
            "        [ 0.9020, -0.7385],\n",
            "        [-0.0522,  0.1036],\n",
            "        [-0.4596,  0.6031],\n",
            "        [ 0.6321, -0.9788],\n",
            "        [-0.1019,  0.0447],\n",
            "        [-0.7187,  0.8318],\n",
            "        [ 0.5844, -0.9338],\n",
            "        [-0.4154,  0.4028],\n",
            "        [ 0.6109, -0.7351],\n",
            "        [ 0.4531, -0.6516],\n",
            "        [ 0.9686, -0.8371],\n",
            "        [ 0.5737, -0.5438],\n",
            "        [ 0.5149, -0.8929],\n",
            "        [-1.1621,  1.2544],\n",
            "        [ 0.3875, -0.5853],\n",
            "        [-0.7845,  0.8911],\n",
            "        [-0.8907,  1.1142],\n",
            "        [-0.8790,  1.2066],\n",
            "        [ 0.5548, -0.5286],\n",
            "        [ 1.0444, -1.0672],\n",
            "        [ 0.5021, -0.9534],\n",
            "        [-0.5601,  0.6745],\n",
            "        [ 0.6183, -0.8392],\n",
            "        [-0.4605,  0.5446],\n",
            "        [-0.1434,  0.2680],\n",
            "        [ 0.5778, -1.0130],\n",
            "        [ 0.2634, -0.8330],\n",
            "        [ 0.2251, -0.7632],\n",
            "        [ 0.7145, -0.7598]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3382, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6671, -0.7259],\n",
            "        [ 0.2382, -0.0298],\n",
            "        [ 0.6058, -0.8691],\n",
            "        [ 0.1656, -0.7993],\n",
            "        [ 0.4336, -0.6629],\n",
            "        [-0.1516,  0.5056],\n",
            "        [ 0.3322, -1.0495],\n",
            "        [-0.1011,  0.0463],\n",
            "        [-0.4043,  0.4000],\n",
            "        [ 0.7333, -0.7073],\n",
            "        [-0.5479,  0.6319],\n",
            "        [ 0.5539, -0.8370],\n",
            "        [ 0.6037, -0.8618],\n",
            "        [ 0.6165, -0.9263],\n",
            "        [-0.4957,  0.5547],\n",
            "        [-0.7868,  1.0702],\n",
            "        [ 0.3875, -0.7401],\n",
            "        [ 0.3837, -0.9028],\n",
            "        [-0.1661,  0.1758],\n",
            "        [-1.0185,  1.1090],\n",
            "        [-0.6105,  0.6897],\n",
            "        [ 0.3857, -0.6390],\n",
            "        [-0.9788,  0.9630],\n",
            "        [-0.3969,  0.5826],\n",
            "        [ 0.6183, -0.7794],\n",
            "        [ 0.4362, -0.5459],\n",
            "        [ 0.5599, -0.8688],\n",
            "        [-0.8111,  1.2333],\n",
            "        [-1.0647,  1.2337],\n",
            "        [ 0.7319, -1.0287],\n",
            "        [ 0.8562, -1.0230],\n",
            "        [ 0.7933, -0.7964]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4823, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4793, -0.8053],\n",
            "        [ 0.1359,  0.1487],\n",
            "        [ 0.8654, -0.8735],\n",
            "        [ 0.4199, -0.6089],\n",
            "        [ 0.3605, -0.6544],\n",
            "        [-1.1864,  1.0670],\n",
            "        [ 0.5123, -1.0947],\n",
            "        [ 0.0856, -0.3786],\n",
            "        [ 0.6565, -0.7951],\n",
            "        [ 0.3434, -0.3729],\n",
            "        [ 0.0553,  0.0953],\n",
            "        [-1.0659,  0.8222],\n",
            "        [-0.5465,  0.7379],\n",
            "        [-0.5203,  0.7799],\n",
            "        [-0.7702,  1.4022],\n",
            "        [ 0.5037, -0.6356],\n",
            "        [-0.5101,  0.9604],\n",
            "        [-0.1083,  0.3493],\n",
            "        [ 0.6763, -0.8872],\n",
            "        [ 0.6335, -0.7108],\n",
            "        [-0.8872,  0.9414],\n",
            "        [ 0.6034, -0.8211],\n",
            "        [ 0.7634, -0.6331],\n",
            "        [-1.0944,  1.1367],\n",
            "        [ 0.3537, -0.2120],\n",
            "        [-0.9987,  1.2775],\n",
            "        [-0.8468,  0.9786],\n",
            "        [ 0.3017, -0.6186],\n",
            "        [ 0.4854, -0.3775],\n",
            "        [ 0.1017, -0.1801],\n",
            "        [ 0.2587, -0.4928],\n",
            "        [ 0.6173, -0.7174]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5118,  0.5841],\n",
            "        [ 0.8561, -1.0920],\n",
            "        [ 0.3658, -0.3855],\n",
            "        [ 0.3946, -0.6862],\n",
            "        [ 0.7748, -0.8862],\n",
            "        [-0.8677,  0.7651],\n",
            "        [-0.1599,  0.4407],\n",
            "        [-1.1419,  0.9222],\n",
            "        [ 0.8988, -0.7346],\n",
            "        [ 0.8187, -0.7344],\n",
            "        [-0.7232,  1.1185],\n",
            "        [-0.8704,  1.2551],\n",
            "        [ 0.4099, -0.6744],\n",
            "        [ 0.4962, -0.5360],\n",
            "        [ 0.9714, -0.8095],\n",
            "        [-0.5341,  1.0661],\n",
            "        [ 0.7225, -0.5264],\n",
            "        [ 0.8831, -0.8877],\n",
            "        [-0.4357,  0.3668],\n",
            "        [-0.2510,  0.0247],\n",
            "        [ 0.6755, -0.9140],\n",
            "        [ 0.5466, -0.7776],\n",
            "        [ 0.6215, -1.0231],\n",
            "        [ 1.0136, -1.0721],\n",
            "        [ 0.6557, -0.3913],\n",
            "        [ 0.2629, -0.7153],\n",
            "        [-0.8887,  1.4391],\n",
            "        [-0.1938,  0.9507],\n",
            "        [-1.0529,  1.3258],\n",
            "        [ 0.4984, -0.9716],\n",
            "        [-0.1602,  0.4230],\n",
            "        [ 0.4736, -0.3957]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4783, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2312,  1.2277],\n",
            "        [-1.0624,  1.3278],\n",
            "        [ 0.2911, -0.6797],\n",
            "        [-0.0679,  0.0906],\n",
            "        [ 0.6854, -0.8261],\n",
            "        [-0.8727,  1.1453],\n",
            "        [ 0.2391, -0.4149],\n",
            "        [ 0.2865, -0.2305],\n",
            "        [ 0.0962, -0.3813],\n",
            "        [ 0.2010, -0.4404],\n",
            "        [ 0.5108, -0.5363],\n",
            "        [ 0.3079, -0.7269],\n",
            "        [-0.9529,  1.0733],\n",
            "        [ 0.6959, -0.8586],\n",
            "        [-0.6760,  0.7134],\n",
            "        [ 0.8816, -1.0073],\n",
            "        [-0.7584,  1.0292],\n",
            "        [-0.0226, -0.0601],\n",
            "        [ 0.1814, -0.0160],\n",
            "        [-0.7928,  1.1624],\n",
            "        [-1.0770,  1.2102],\n",
            "        [ 0.5937, -0.9270],\n",
            "        [ 0.4761, -0.8627],\n",
            "        [ 0.6966, -0.7891],\n",
            "        [ 0.5892, -0.8841],\n",
            "        [-0.8319,  1.0225],\n",
            "        [ 0.6267, -0.7809],\n",
            "        [ 0.4369, -0.1572],\n",
            "        [-0.8069,  1.1573],\n",
            "        [ 0.4811, -0.8220],\n",
            "        [ 0.0098,  0.0244],\n",
            "        [ 0.7744, -0.9519]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4814,  0.4505],\n",
            "        [ 0.5490, -0.5501],\n",
            "        [ 0.7241, -1.1183],\n",
            "        [ 0.2569, -0.6862],\n",
            "        [ 0.7307, -1.0226],\n",
            "        [ 0.0781, -0.0852],\n",
            "        [-0.6922,  0.6666],\n",
            "        [ 0.1447, -0.1582],\n",
            "        [ 0.1627, -0.4030],\n",
            "        [ 0.4464, -0.8062],\n",
            "        [-0.8609,  0.7630],\n",
            "        [-0.2812,  0.4249],\n",
            "        [ 0.8839, -0.7890],\n",
            "        [ 0.8449, -1.0767],\n",
            "        [ 0.0637,  0.0677],\n",
            "        [-0.8617,  1.1102],\n",
            "        [ 0.3895, -0.5459],\n",
            "        [-1.1343,  1.4066],\n",
            "        [-1.0491,  0.9896],\n",
            "        [-1.2117,  1.2156],\n",
            "        [ 0.4863, -0.4305],\n",
            "        [ 0.0214, -0.3780],\n",
            "        [-0.8790,  1.0451],\n",
            "        [-0.8982,  1.2278],\n",
            "        [ 0.4591, -0.8144],\n",
            "        [ 0.4290, -0.5929],\n",
            "        [ 0.1902, -0.5385],\n",
            "        [-0.7143,  1.0081],\n",
            "        [-0.2217,  0.3885],\n",
            "        [-0.7352,  1.0890],\n",
            "        [-1.0963,  1.4344],\n",
            "        [ 0.4034, -0.6802]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4723, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8062,  1.1857],\n",
            "        [-0.5261,  0.9970],\n",
            "        [ 0.7359, -0.7737],\n",
            "        [-0.5340,  0.5311],\n",
            "        [ 0.6411, -0.7250],\n",
            "        [ 0.2027,  0.1650],\n",
            "        [-0.8001,  0.9516],\n",
            "        [-0.9112,  1.3954],\n",
            "        [-0.7402,  1.0497],\n",
            "        [-1.1076,  1.2288],\n",
            "        [ 0.6937, -0.4712],\n",
            "        [ 0.6110, -0.7935],\n",
            "        [-1.0662,  1.3478],\n",
            "        [ 0.3750, -0.8006],\n",
            "        [ 0.6209, -0.9221],\n",
            "        [ 0.0256,  0.1197],\n",
            "        [ 0.7730, -1.0132],\n",
            "        [ 0.0676, -0.2936],\n",
            "        [-0.3511,  0.5160],\n",
            "        [-1.1961,  1.1769],\n",
            "        [ 0.1320, -0.2367],\n",
            "        [ 0.6771, -0.9537],\n",
            "        [ 0.7118, -1.0077],\n",
            "        [ 0.7112, -0.7169],\n",
            "        [-0.2145,  0.3959],\n",
            "        [-0.5755,  1.0977],\n",
            "        [ 0.6899, -0.8613],\n",
            "        [-1.3200,  1.3100],\n",
            "        [ 0.8938, -0.7832],\n",
            "        [ 0.5915, -0.7740],\n",
            "        [-1.0091,  0.9169],\n",
            "        [ 0.7861, -1.0913]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4212, -0.5599],\n",
            "        [-1.0692,  1.0678],\n",
            "        [-0.9726,  1.1849],\n",
            "        [-0.8431,  1.0120],\n",
            "        [-0.0371, -0.0113],\n",
            "        [-0.5572,  0.9934],\n",
            "        [-0.7860,  1.0461],\n",
            "        [ 0.4350, -0.9556],\n",
            "        [-1.0211,  1.0525],\n",
            "        [ 0.4507, -0.5033],\n",
            "        [ 0.7130, -0.7059],\n",
            "        [-1.2112,  1.1912],\n",
            "        [ 0.8382, -1.1814],\n",
            "        [-1.1862,  1.2554],\n",
            "        [-0.9819,  1.0474],\n",
            "        [ 0.0135,  0.2591],\n",
            "        [ 0.4919, -0.7713],\n",
            "        [ 0.4140, -0.6006],\n",
            "        [ 0.8067, -0.8687],\n",
            "        [-0.6647,  0.4853],\n",
            "        [ 0.5591, -0.7735],\n",
            "        [ 0.2847, -0.7063],\n",
            "        [ 0.2505,  0.0425],\n",
            "        [-0.9858,  1.0224],\n",
            "        [-0.9506,  1.2970],\n",
            "        [-1.0408,  1.5078],\n",
            "        [-1.1775,  1.2469],\n",
            "        [ 0.6672, -0.7514],\n",
            "        [ 0.5163, -0.6661],\n",
            "        [ 0.4270, -0.7964],\n",
            "        [ 0.8808, -0.7342],\n",
            "        [-0.6427,  1.1811]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4868, -0.5447],\n",
            "        [ 0.8064, -0.8053],\n",
            "        [ 0.3937, -0.5888],\n",
            "        [ 0.7759, -0.6648],\n",
            "        [-0.6484,  1.0025],\n",
            "        [ 0.0076,  0.1158],\n",
            "        [-1.1255,  1.1428],\n",
            "        [ 0.9269, -1.0874],\n",
            "        [ 0.7675, -0.7016],\n",
            "        [ 0.6356, -0.5787],\n",
            "        [-0.5809,  0.7523],\n",
            "        [-0.9650,  1.3800],\n",
            "        [ 0.5991, -0.8226],\n",
            "        [ 0.4420, -0.9075],\n",
            "        [ 0.6729, -0.8859],\n",
            "        [ 0.5977, -0.8749],\n",
            "        [-0.2285,  0.2389],\n",
            "        [ 0.5642, -0.7643],\n",
            "        [ 0.5637, -0.9788],\n",
            "        [-0.9117,  1.0140],\n",
            "        [ 0.5060, -0.9020],\n",
            "        [ 0.7398, -1.1070],\n",
            "        [ 0.2681, -0.4392],\n",
            "        [-0.8323,  1.3441],\n",
            "        [ 0.9113, -0.7499],\n",
            "        [ 0.6073, -0.7521],\n",
            "        [ 0.2809, -0.3303],\n",
            "        [-1.1202,  1.3938],\n",
            "        [-0.2404,  0.4748],\n",
            "        [ 0.7731, -0.9245],\n",
            "        [ 0.4446, -0.6811],\n",
            "        [-0.9765,  1.1615]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4106, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6590,  1.1904],\n",
            "        [-0.7539,  1.3685],\n",
            "        [ 0.7097, -0.6403],\n",
            "        [-1.3275,  1.0630],\n",
            "        [-1.1476,  1.5047],\n",
            "        [ 0.1949, -0.6241],\n",
            "        [ 0.5627, -0.7049],\n",
            "        [-0.9674,  1.1797],\n",
            "        [ 0.3103,  0.0407],\n",
            "        [ 0.6905, -0.7178],\n",
            "        [-0.2901,  0.1741],\n",
            "        [ 0.6504, -1.0810],\n",
            "        [ 0.2086, -0.2468],\n",
            "        [ 0.5706, -0.8122],\n",
            "        [ 0.6576, -1.0537],\n",
            "        [-0.3518,  0.1919],\n",
            "        [-0.3209,  0.2858],\n",
            "        [-0.7297,  1.0294],\n",
            "        [-0.7071,  0.7019],\n",
            "        [ 0.6971, -0.8270],\n",
            "        [-0.9520,  1.1898],\n",
            "        [-0.3912,  0.7250],\n",
            "        [ 0.5947, -0.8339],\n",
            "        [-0.8931,  1.1158],\n",
            "        [-0.9243,  1.1523],\n",
            "        [-1.2437,  1.4379],\n",
            "        [ 0.8342, -0.8075],\n",
            "        [-0.1326,  0.4393],\n",
            "        [-0.6134,  0.9377],\n",
            "        [-0.2538,  0.7038],\n",
            "        [-0.9528,  1.1802],\n",
            "        [ 0.5302, -0.6358]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7794, -0.8000],\n",
            "        [-0.1310,  0.3894],\n",
            "        [ 0.0587, -0.3887],\n",
            "        [ 0.7643, -0.7363],\n",
            "        [-0.8010,  0.5856],\n",
            "        [ 0.2845, -0.2300],\n",
            "        [ 0.7193, -0.8834],\n",
            "        [ 0.4035, -0.6791],\n",
            "        [ 0.5004, -1.0821],\n",
            "        [ 0.6433, -0.7828],\n",
            "        [ 0.7426, -0.9305],\n",
            "        [-0.7800,  0.6653],\n",
            "        [ 0.3494, -0.3006],\n",
            "        [ 0.6950, -1.0436],\n",
            "        [-1.1366,  1.2055],\n",
            "        [ 0.7271, -1.0765],\n",
            "        [ 0.8847, -1.1473],\n",
            "        [ 0.0459, -0.3856],\n",
            "        [ 0.2633, -0.3908],\n",
            "        [ 0.6233, -1.0020],\n",
            "        [-0.9829,  0.9312],\n",
            "        [ 0.9172, -1.0032],\n",
            "        [ 0.7118, -0.9818],\n",
            "        [ 0.1918, -0.1568],\n",
            "        [ 0.1182, -0.0847],\n",
            "        [-0.2352,  0.2988],\n",
            "        [ 0.3572, -0.5670],\n",
            "        [ 0.6362, -0.9857],\n",
            "        [ 0.3875, -0.7590],\n",
            "        [ 0.9616, -1.1851],\n",
            "        [ 0.8182, -1.0621],\n",
            "        [ 0.8387, -0.9571]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4715, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7982, -1.2608],\n",
            "        [ 0.5707, -0.8844],\n",
            "        [-1.1149,  1.4239],\n",
            "        [ 0.7667, -0.5609],\n",
            "        [ 0.2109, -0.2944],\n",
            "        [ 0.7218, -0.7631],\n",
            "        [ 0.8448, -1.0614],\n",
            "        [ 0.5046, -0.5646],\n",
            "        [ 0.4506, -0.6596],\n",
            "        [ 0.4049, -0.8646],\n",
            "        [ 0.6033, -0.7691],\n",
            "        [-0.4461,  0.2533],\n",
            "        [ 0.6095, -0.7468],\n",
            "        [ 0.9049, -0.9952],\n",
            "        [ 0.5309, -0.7028],\n",
            "        [ 0.6320, -0.9761],\n",
            "        [ 0.4367, -0.0940],\n",
            "        [ 0.0053,  0.2716],\n",
            "        [ 0.4463, -0.2385],\n",
            "        [ 0.8373, -1.0775],\n",
            "        [ 0.6410, -0.7593],\n",
            "        [ 0.5559, -0.8382],\n",
            "        [-0.9696,  1.3590],\n",
            "        [ 0.7718, -1.0636],\n",
            "        [ 0.7926, -0.9357],\n",
            "        [ 0.9513, -1.0556],\n",
            "        [ 0.3591, -0.2746],\n",
            "        [-0.2050,  0.1897],\n",
            "        [ 0.6517, -0.8343],\n",
            "        [ 0.4349, -0.7080],\n",
            "        [-0.9776,  1.2500],\n",
            "        [ 0.8196, -0.9259]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4392, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4072,  0.4076],\n",
            "        [ 0.6749, -0.7211],\n",
            "        [-0.4733,  0.1802],\n",
            "        [ 0.9685, -0.9552],\n",
            "        [-0.8805,  1.2549],\n",
            "        [ 0.6084, -1.1855],\n",
            "        [-0.6940,  1.1896],\n",
            "        [ 0.7108, -0.8871],\n",
            "        [ 0.6894, -1.0074],\n",
            "        [ 0.3008, -0.8610],\n",
            "        [ 0.5027, -0.6529],\n",
            "        [ 0.6732, -1.0871],\n",
            "        [ 0.7932, -0.6498],\n",
            "        [ 0.5930, -0.7277],\n",
            "        [-0.1308,  0.2620],\n",
            "        [ 0.6167, -0.8657],\n",
            "        [ 0.1955, -0.4011],\n",
            "        [-0.8021,  1.0188],\n",
            "        [ 0.3834, -0.5996],\n",
            "        [-0.5956,  0.3856],\n",
            "        [ 0.8719, -1.0209],\n",
            "        [-0.6027,  0.8890],\n",
            "        [-0.7667,  0.9914],\n",
            "        [-0.4996,  0.6502],\n",
            "        [ 0.7410, -0.9417],\n",
            "        [ 0.6048, -0.9494],\n",
            "        [ 0.0760,  0.4337],\n",
            "        [-1.0691,  1.2182],\n",
            "        [ 0.3459, -0.6447],\n",
            "        [-0.7631,  0.7859],\n",
            "        [ 0.8858, -1.1097],\n",
            "        [ 0.6285, -0.6102]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4943, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.6327e-01, -1.0001e+00],\n",
            "        [-8.0331e-01,  1.1724e+00],\n",
            "        [ 5.5800e-01, -7.6602e-01],\n",
            "        [ 8.0631e-01, -1.0206e+00],\n",
            "        [-1.6694e-01,  1.3355e-01],\n",
            "        [-3.7176e-01,  4.7382e-01],\n",
            "        [ 3.6163e-01, -8.1325e-01],\n",
            "        [ 4.5259e-01, -8.6040e-01],\n",
            "        [ 4.0764e-01, -7.8387e-01],\n",
            "        [ 2.7620e-05, -5.4533e-01],\n",
            "        [ 6.5258e-01, -8.1679e-01],\n",
            "        [-5.6952e-01,  6.3280e-01],\n",
            "        [-7.1815e-01,  8.5939e-01],\n",
            "        [-9.1469e-01,  1.1614e+00],\n",
            "        [ 5.8373e-01, -1.0084e+00],\n",
            "        [ 4.6772e-01, -2.9797e-01],\n",
            "        [ 7.3648e-01, -9.7695e-01],\n",
            "        [ 9.8388e-02, -2.5458e-01],\n",
            "        [ 7.2624e-01, -8.8591e-01],\n",
            "        [ 5.6935e-01, -9.0152e-01],\n",
            "        [-3.6009e-01,  2.3651e-01],\n",
            "        [-6.8576e-01,  1.0972e+00],\n",
            "        [ 4.6155e-01, -4.7776e-01],\n",
            "        [-1.3357e-01, -2.3584e-01],\n",
            "        [ 2.1545e-01, -4.0666e-01],\n",
            "        [ 3.6549e-01, -1.7185e-01],\n",
            "        [ 5.7584e-01, -7.9615e-01],\n",
            "        [ 8.6194e-01, -9.6006e-01],\n",
            "        [ 8.7563e-01, -9.3798e-01],\n",
            "        [-4.7893e-01,  5.0081e-01],\n",
            "        [ 8.5456e-01, -1.2334e+00],\n",
            "        [ 1.0254e+00, -1.0689e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch   150  of    191.    Elapsed: 0:03:33.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5936, -0.6428],\n",
            "        [ 0.7367, -1.0910],\n",
            "        [ 0.7366, -0.9417],\n",
            "        [ 0.7044, -0.8641],\n",
            "        [-0.5989,  1.0051],\n",
            "        [-0.7471,  1.1409],\n",
            "        [-1.0177,  1.2859],\n",
            "        [ 0.7507, -1.0020],\n",
            "        [-0.9607,  1.1690],\n",
            "        [-0.5893,  0.6409],\n",
            "        [ 0.5691, -0.4357],\n",
            "        [ 0.6451, -0.8782],\n",
            "        [ 0.7550, -0.8499],\n",
            "        [ 0.7657, -0.9695],\n",
            "        [ 0.8203, -1.1196],\n",
            "        [-0.8583,  1.2749],\n",
            "        [ 0.6677, -0.8674],\n",
            "        [ 0.6927, -0.5569],\n",
            "        [ 0.6917, -0.6888],\n",
            "        [-0.2597,  0.1334],\n",
            "        [ 0.3610, -0.7058],\n",
            "        [ 0.2930, -0.5897],\n",
            "        [ 0.6244, -0.6936],\n",
            "        [-1.0156,  1.1190],\n",
            "        [ 0.4233, -0.9674],\n",
            "        [-1.0906,  1.2652],\n",
            "        [-0.3543,  0.4271],\n",
            "        [ 0.6140, -1.2654],\n",
            "        [ 0.9317, -1.0762],\n",
            "        [ 0.4518, -0.6687],\n",
            "        [-1.1193,  1.4355],\n",
            "        [ 0.1063, -0.1032]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4534, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-7.5017e-01,  1.0806e+00],\n",
            "        [ 4.8753e-01, -6.0930e-01],\n",
            "        [-2.8357e-01,  4.2347e-01],\n",
            "        [-2.2074e-01, -7.1148e-02],\n",
            "        [ 6.9180e-01, -6.4371e-01],\n",
            "        [ 6.1566e-01, -6.6862e-01],\n",
            "        [ 8.6426e-01, -8.8029e-01],\n",
            "        [ 5.2545e-01, -8.2972e-01],\n",
            "        [-5.4512e-01,  6.5022e-01],\n",
            "        [ 8.3030e-01, -1.0537e+00],\n",
            "        [ 8.0516e-01, -8.8272e-01],\n",
            "        [ 8.0853e-01, -7.0960e-01],\n",
            "        [-4.8140e-01,  5.1811e-01],\n",
            "        [ 4.8481e-01, -6.3521e-01],\n",
            "        [-4.7221e-01,  4.9354e-01],\n",
            "        [ 9.1413e-01, -1.1105e+00],\n",
            "        [ 4.7688e-01, -4.6457e-01],\n",
            "        [ 7.3830e-01, -1.2201e+00],\n",
            "        [-1.1048e+00,  1.3975e+00],\n",
            "        [ 7.8148e-01, -1.0747e+00],\n",
            "        [-1.0735e+00,  1.1677e+00],\n",
            "        [ 7.6380e-01, -7.3636e-01],\n",
            "        [ 2.0996e-01, -6.0944e-01],\n",
            "        [-1.0996e-03, -3.3549e-01],\n",
            "        [ 8.8568e-01, -7.9958e-01],\n",
            "        [-6.7216e-01,  5.2397e-01],\n",
            "        [ 7.9476e-01, -8.7713e-01],\n",
            "        [ 5.3773e-01, -5.1945e-01],\n",
            "        [ 2.1472e-01, -5.5750e-01],\n",
            "        [ 3.2069e-01, -1.3434e-01],\n",
            "        [ 5.2798e-01, -9.8014e-01],\n",
            "        [-9.2354e-01,  1.0592e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7198,  0.9627],\n",
            "        [ 0.8297, -0.9091],\n",
            "        [ 0.4617, -0.6297],\n",
            "        [-0.5085,  0.8155],\n",
            "        [ 0.5563, -0.5387],\n",
            "        [ 0.9968, -0.9789],\n",
            "        [ 0.0964, -0.0168],\n",
            "        [ 0.3814, -0.6503],\n",
            "        [-0.0985,  0.0138],\n",
            "        [-0.6643,  1.2786],\n",
            "        [-0.2634,  0.3146],\n",
            "        [-0.6132,  0.5389],\n",
            "        [ 0.2329, -0.2898],\n",
            "        [ 0.1900, -0.3580],\n",
            "        [-0.4147,  0.5377],\n",
            "        [ 0.4957, -0.4759],\n",
            "        [ 0.5860, -0.6304],\n",
            "        [ 0.4921, -0.6305],\n",
            "        [ 0.9300, -1.0454],\n",
            "        [-0.6485,  0.7847],\n",
            "        [ 0.8497, -1.0024],\n",
            "        [ 0.3912, -0.6265],\n",
            "        [-0.2863,  0.3882],\n",
            "        [ 0.4101, -0.7400],\n",
            "        [ 0.1429,  0.0376],\n",
            "        [ 0.3556, -0.7495],\n",
            "        [-0.9552,  0.6940],\n",
            "        [ 0.6747, -0.8901],\n",
            "        [ 0.6885, -0.4203],\n",
            "        [-0.8533,  1.1743],\n",
            "        [-0.8281,  1.0712],\n",
            "        [-1.0915,  1.4950]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9121, -1.1682],\n",
            "        [ 0.6224, -0.8777],\n",
            "        [-0.4664,  0.6202],\n",
            "        [ 0.7717, -0.8645],\n",
            "        [-0.6072,  1.0654],\n",
            "        [ 0.7207, -0.9999],\n",
            "        [ 0.0405, -0.2837],\n",
            "        [ 0.0561,  0.1000],\n",
            "        [ 0.1731, -0.1263],\n",
            "        [-0.5814,  0.7293],\n",
            "        [-0.6696,  0.8989],\n",
            "        [ 0.7082, -0.8965],\n",
            "        [-1.0078,  1.1330],\n",
            "        [-0.5503,  0.7705],\n",
            "        [ 0.3583, -0.5261],\n",
            "        [ 0.6283, -0.9748],\n",
            "        [ 0.4227, -0.6424],\n",
            "        [ 0.4215, -0.6409],\n",
            "        [-0.1851,  0.1391],\n",
            "        [-0.2464,  0.4558],\n",
            "        [ 0.1503, -0.3971],\n",
            "        [ 0.7449, -0.7940],\n",
            "        [ 0.5415, -0.9473],\n",
            "        [ 0.7576, -0.7671],\n",
            "        [ 0.5331, -1.0520],\n",
            "        [ 0.7183, -1.2249],\n",
            "        [ 0.5441, -0.8064],\n",
            "        [-0.1091,  0.0968],\n",
            "        [-0.9118,  1.1933],\n",
            "        [-0.9052,  1.1650],\n",
            "        [ 0.7534, -0.8567],\n",
            "        [ 0.6750, -0.7658]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3578, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4856, -0.4363],\n",
            "        [ 0.5795, -0.9035],\n",
            "        [-0.4920,  0.7197],\n",
            "        [ 0.2124, -0.4179],\n",
            "        [ 0.8604, -0.9278],\n",
            "        [ 0.8028, -0.9078],\n",
            "        [-0.8694,  0.9625],\n",
            "        [ 0.4282, -0.4466],\n",
            "        [-0.5393,  0.3677],\n",
            "        [ 0.5622, -0.7432],\n",
            "        [ 0.2310, -0.2292],\n",
            "        [ 0.9065, -0.9665],\n",
            "        [ 0.6554, -1.1541],\n",
            "        [-1.0593,  1.1379],\n",
            "        [ 0.6271, -0.9382],\n",
            "        [-0.6858,  1.1218],\n",
            "        [-0.7281,  0.8710],\n",
            "        [ 0.4718, -0.0239],\n",
            "        [ 0.6803, -0.8358],\n",
            "        [ 0.5964, -0.4263],\n",
            "        [ 0.7224, -0.7125],\n",
            "        [-0.1426,  0.0440],\n",
            "        [ 0.9013, -0.8282],\n",
            "        [ 0.6258, -0.8751],\n",
            "        [ 0.7074, -0.4748],\n",
            "        [ 0.7117, -0.5663],\n",
            "        [ 0.7934, -1.1487],\n",
            "        [ 0.8518, -1.1966],\n",
            "        [ 0.3929, -0.5853],\n",
            "        [ 0.4490, -0.8320],\n",
            "        [ 0.8395, -0.9938],\n",
            "        [ 0.6563, -0.7837]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8760, -0.7650],\n",
            "        [-0.2874,  0.3400],\n",
            "        [ 0.7830, -0.8961],\n",
            "        [ 0.9072, -0.9460],\n",
            "        [ 0.7976, -0.8861],\n",
            "        [-0.9270,  1.1278],\n",
            "        [ 0.4903, -0.9536],\n",
            "        [ 0.4011, -0.5686],\n",
            "        [-0.2701,  0.2387],\n",
            "        [ 0.3590, -0.1964],\n",
            "        [ 0.6749, -0.8013],\n",
            "        [ 0.3288, -0.0159],\n",
            "        [ 0.8997, -0.8228],\n",
            "        [-0.5097,  0.9700],\n",
            "        [ 0.2904, -0.2463],\n",
            "        [ 0.7973, -0.6942],\n",
            "        [ 1.0482, -1.2673],\n",
            "        [ 0.8827, -1.1661],\n",
            "        [ 0.6633, -0.9344],\n",
            "        [ 0.2187, -0.4211],\n",
            "        [ 0.0371,  0.0698],\n",
            "        [ 0.7473, -0.8715],\n",
            "        [ 0.8781, -1.1652],\n",
            "        [ 0.1232, -0.0667],\n",
            "        [ 0.3764, -0.8730],\n",
            "        [ 0.7414, -1.0355],\n",
            "        [ 0.5506, -0.6477],\n",
            "        [-1.0103,  1.2736],\n",
            "        [ 0.6433, -0.2998],\n",
            "        [ 0.4942, -0.9742],\n",
            "        [ 0.5135, -0.6733],\n",
            "        [ 0.5939, -0.8825]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4649, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6586,  0.6953],\n",
            "        [-0.3057,  0.3845],\n",
            "        [-0.4574,  0.6560],\n",
            "        [ 0.0750, -0.0135],\n",
            "        [ 0.7718, -0.9243],\n",
            "        [-0.7748,  0.7624],\n",
            "        [ 0.1180, -0.4403],\n",
            "        [ 0.7929, -0.9740],\n",
            "        [ 0.7184, -0.8173],\n",
            "        [ 0.8101, -0.9573],\n",
            "        [-1.0668,  1.2166],\n",
            "        [-0.4774,  0.4608],\n",
            "        [-0.6837,  0.6724],\n",
            "        [-0.1524,  0.2618],\n",
            "        [-0.8970,  1.0122],\n",
            "        [-0.8138,  0.8276],\n",
            "        [-1.0406,  1.2192],\n",
            "        [ 0.6499, -0.9957],\n",
            "        [-0.6107,  0.7164],\n",
            "        [ 0.6884, -0.7506],\n",
            "        [ 0.5038, -0.7747],\n",
            "        [ 0.4865, -0.4461],\n",
            "        [ 0.7265, -0.8773],\n",
            "        [-0.6916,  1.0633],\n",
            "        [ 0.2365, -0.0582],\n",
            "        [ 0.1085, -0.3456],\n",
            "        [ 0.7759, -0.6905],\n",
            "        [-0.7631,  1.0957],\n",
            "        [-0.1003,  0.3524],\n",
            "        [-1.2782,  1.2478],\n",
            "        [ 0.8211, -0.8391],\n",
            "        [ 0.7288, -0.6839]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7642, -0.9913],\n",
            "        [ 0.7654, -0.9741],\n",
            "        [ 0.5049, -0.6556],\n",
            "        [-0.9812,  1.1062],\n",
            "        [ 0.3887, -0.8185],\n",
            "        [-1.0237,  1.3132],\n",
            "        [ 0.8333, -1.1198],\n",
            "        [-0.5669,  0.7507],\n",
            "        [-0.9442,  1.0898],\n",
            "        [ 0.2772, -0.5436],\n",
            "        [ 0.7768, -0.8900],\n",
            "        [-1.2211,  1.2862],\n",
            "        [ 0.7274, -1.0261],\n",
            "        [-0.2233,  0.1446],\n",
            "        [-1.2310,  1.1806],\n",
            "        [ 0.7977, -0.9013],\n",
            "        [ 0.8004, -1.1843],\n",
            "        [-1.3882,  1.1734],\n",
            "        [-0.0469,  0.0218],\n",
            "        [ 0.4994, -0.7581],\n",
            "        [ 0.7960, -0.9125],\n",
            "        [ 0.6925, -0.8510],\n",
            "        [-1.0051,  1.1713],\n",
            "        [ 0.5583, -0.6671],\n",
            "        [-0.7996,  0.8562],\n",
            "        [ 0.7027, -1.0691],\n",
            "        [ 1.0713, -0.8449],\n",
            "        [ 0.5807, -0.9682],\n",
            "        [ 0.7642, -0.9045],\n",
            "        [ 0.1917,  0.0370],\n",
            "        [ 0.7063, -0.9393],\n",
            "        [ 0.5847, -0.8000]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7592,  1.4482],\n",
            "        [ 0.4160, -0.3504],\n",
            "        [-0.8346,  1.1891],\n",
            "        [-0.6546,  1.1604],\n",
            "        [ 0.7162, -0.8362],\n",
            "        [ 0.6314, -0.4512],\n",
            "        [ 0.5251, -0.3821],\n",
            "        [-0.7451,  0.9150],\n",
            "        [ 0.2853, -0.9288],\n",
            "        [ 0.7531, -0.8764],\n",
            "        [ 0.7017, -1.0866],\n",
            "        [-0.9556,  1.0103],\n",
            "        [-0.6606,  1.1819],\n",
            "        [ 0.7258, -0.6382],\n",
            "        [-0.9756,  1.1296],\n",
            "        [-0.8048,  1.1480],\n",
            "        [-0.3415,  0.5458],\n",
            "        [ 0.2247, -0.0151],\n",
            "        [ 0.6128, -0.7862],\n",
            "        [ 0.6334, -0.8807],\n",
            "        [ 0.3016, -0.4771],\n",
            "        [-1.0440,  1.1364],\n",
            "        [-1.3020,  1.3303],\n",
            "        [ 0.3317, -0.7984],\n",
            "        [ 0.8680, -1.0036],\n",
            "        [ 0.3817, -0.5904],\n",
            "        [ 0.6974, -1.2610],\n",
            "        [ 0.6259, -0.8511],\n",
            "        [-1.1149,  0.9552],\n",
            "        [-0.5113,  0.8239],\n",
            "        [-1.0173,  1.1647],\n",
            "        [ 0.8901, -1.0544]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2401, -0.1369],\n",
            "        [-0.8426,  1.3574],\n",
            "        [-0.5896,  0.7219],\n",
            "        [ 0.8000, -0.8611],\n",
            "        [-0.0479,  0.4136],\n",
            "        [-0.3189,  0.3749],\n",
            "        [-0.8764,  1.2813],\n",
            "        [-0.9088,  1.0880],\n",
            "        [-0.7852,  1.1420],\n",
            "        [ 0.8911, -0.7531],\n",
            "        [-0.5648,  0.7310],\n",
            "        [ 0.5943, -0.5983],\n",
            "        [ 0.8023, -1.1887],\n",
            "        [ 0.4713, -0.8130],\n",
            "        [ 0.8703, -1.1658],\n",
            "        [-0.1538,  0.4850],\n",
            "        [ 0.9892, -0.9325],\n",
            "        [ 0.8948, -0.9298],\n",
            "        [ 0.0859, -0.0363],\n",
            "        [ 0.6236, -0.9233],\n",
            "        [ 0.8055, -1.0194],\n",
            "        [-0.8859,  1.2213],\n",
            "        [ 0.6028, -0.4224],\n",
            "        [ 0.6383, -0.7822],\n",
            "        [-0.8753,  0.8168],\n",
            "        [-0.8936,  1.0463],\n",
            "        [ 0.9937, -0.9225],\n",
            "        [ 0.4105, -0.9890],\n",
            "        [ 0.9820, -1.1365],\n",
            "        [-0.1434,  0.1001],\n",
            "        [ 0.6922, -0.9820],\n",
            "        [-1.0718,  1.3435]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3474, -0.3000],\n",
            "        [ 0.7878, -0.6501],\n",
            "        [-0.6454,  0.6290],\n",
            "        [ 0.9581, -0.8855],\n",
            "        [ 1.0308, -0.9566],\n",
            "        [ 0.1057, -0.1211],\n",
            "        [ 0.2301, -0.7882],\n",
            "        [-0.9978,  1.3004],\n",
            "        [-1.0501,  1.1166],\n",
            "        [-1.1809,  1.2126],\n",
            "        [ 0.6975, -0.6854],\n",
            "        [ 0.6929, -0.9300],\n",
            "        [-0.5529,  0.8512],\n",
            "        [ 0.9799, -1.1969],\n",
            "        [-0.8026,  1.1046],\n",
            "        [ 0.4471, -0.5734],\n",
            "        [ 0.7217, -1.0350],\n",
            "        [ 0.8933, -1.0780],\n",
            "        [-0.6730,  0.7902],\n",
            "        [-0.6957,  0.6748],\n",
            "        [-1.1400,  1.0568],\n",
            "        [ 0.8130, -0.9980],\n",
            "        [-0.2318,  0.2633],\n",
            "        [-0.4559,  0.5774],\n",
            "        [-0.1891, -0.0307],\n",
            "        [ 0.1899, -0.5001],\n",
            "        [ 0.4783, -0.6338],\n",
            "        [ 0.6747, -0.9601],\n",
            "        [-0.9404,  1.1858],\n",
            "        [ 0.8002, -0.9206],\n",
            "        [ 0.8487, -1.1386],\n",
            "        [ 0.6692, -0.7473]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6719, -0.7616],\n",
            "        [-1.1741,  0.9929],\n",
            "        [-0.4927,  0.9180],\n",
            "        [ 0.7592, -0.8606],\n",
            "        [ 0.4742, -0.3509],\n",
            "        [ 0.6644, -1.0392],\n",
            "        [ 0.2710, -0.2979],\n",
            "        [ 0.4140, -0.4351],\n",
            "        [ 0.0859,  0.1197],\n",
            "        [ 0.7331, -0.9420],\n",
            "        [-0.0208,  0.1108],\n",
            "        [ 0.5808, -0.6995],\n",
            "        [ 0.7935, -0.5905],\n",
            "        [ 0.8520, -0.9352],\n",
            "        [ 0.7282, -0.9686],\n",
            "        [ 0.2494, -0.5913],\n",
            "        [-0.4764,  0.7482],\n",
            "        [ 0.3297, -0.4394],\n",
            "        [-0.7063,  0.9132],\n",
            "        [-0.4132, -0.0748],\n",
            "        [ 0.4977, -1.0363],\n",
            "        [ 0.7839, -0.7954],\n",
            "        [ 0.6835, -0.8417],\n",
            "        [ 0.0428,  0.0144],\n",
            "        [-0.9985,  1.2513],\n",
            "        [-1.0459,  1.2384],\n",
            "        [ 0.2812, -0.3805],\n",
            "        [ 0.1447,  0.0411],\n",
            "        [-0.7813,  0.9100],\n",
            "        [-1.2469,  1.5065],\n",
            "        [ 0.6344, -0.8683],\n",
            "        [ 0.7887, -0.9103]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0809, -0.1528],\n",
            "        [ 0.9780, -1.0199],\n",
            "        [ 0.8701, -0.9667],\n",
            "        [ 0.8605, -1.2116],\n",
            "        [-0.8322,  0.9195],\n",
            "        [ 0.9648, -0.9687],\n",
            "        [-0.7531,  1.0389],\n",
            "        [-0.6134,  0.5690],\n",
            "        [ 0.6890, -1.0202],\n",
            "        [ 0.9019, -1.0022],\n",
            "        [ 0.0123, -0.3212],\n",
            "        [ 0.6600, -0.9709],\n",
            "        [-0.5690,  1.0639],\n",
            "        [ 0.6174, -1.0792],\n",
            "        [-0.5371,  0.6893],\n",
            "        [ 0.6056, -0.7477],\n",
            "        [-0.7277,  0.9612],\n",
            "        [ 0.0183,  0.0472],\n",
            "        [ 0.9534, -1.0950],\n",
            "        [-0.3764,  0.7386],\n",
            "        [-0.5364,  0.8090],\n",
            "        [ 0.7240, -1.0005],\n",
            "        [-0.6737,  0.7471],\n",
            "        [-0.1762,  0.1709],\n",
            "        [-0.5483,  0.3681],\n",
            "        [ 0.8225, -1.1136],\n",
            "        [-0.5460,  0.7348],\n",
            "        [ 0.4910, -0.8419],\n",
            "        [ 0.8183, -1.2265],\n",
            "        [-0.8547,  1.0487],\n",
            "        [ 0.6461, -0.6202],\n",
            "        [ 0.8025, -0.8020]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0354, -0.7677],\n",
            "        [ 0.6814, -0.7777],\n",
            "        [ 0.7971, -1.1696],\n",
            "        [ 0.5880, -1.1593],\n",
            "        [ 0.8192, -0.7752],\n",
            "        [ 0.7550, -0.9623],\n",
            "        [ 0.5343, -0.8180],\n",
            "        [-0.3462,  0.6134],\n",
            "        [ 0.2743, -0.3608],\n",
            "        [ 1.0026, -1.1194],\n",
            "        [ 0.5150, -0.9014],\n",
            "        [ 0.6949, -0.8008],\n",
            "        [-0.5829,  0.9817],\n",
            "        [-0.6594,  1.1141],\n",
            "        [-1.0793,  1.3620],\n",
            "        [-1.0522,  1.1228],\n",
            "        [ 0.2103, -0.1468],\n",
            "        [ 0.5436, -0.8619],\n",
            "        [ 0.5564, -0.7389],\n",
            "        [ 0.4793, -0.4346],\n",
            "        [ 0.5463, -0.8986],\n",
            "        [ 0.8770, -1.2061],\n",
            "        [ 0.8005, -1.1384],\n",
            "        [ 0.5810, -0.2771],\n",
            "        [ 0.7339, -0.8990],\n",
            "        [ 0.6630, -0.7928],\n",
            "        [ 0.2111, -0.1997],\n",
            "        [-0.4129,  0.5770],\n",
            "        [ 0.6400, -1.1566],\n",
            "        [ 0.4365, -0.8691],\n",
            "        [ 0.7278, -0.9232],\n",
            "        [ 0.8250, -0.9653]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7708, -0.8693],\n",
            "        [-0.9507,  1.0632],\n",
            "        [ 1.1218, -1.2132],\n",
            "        [-0.5724,  0.6813],\n",
            "        [ 0.7626, -0.9067],\n",
            "        [ 0.7067, -0.6081],\n",
            "        [-0.7570,  1.0739],\n",
            "        [-0.7078,  0.6569],\n",
            "        [ 0.5529, -0.9092],\n",
            "        [ 0.4782, -0.6894],\n",
            "        [ 0.4631, -0.5170],\n",
            "        [ 0.7100, -0.9998],\n",
            "        [-0.7082,  1.0319],\n",
            "        [-0.9085,  1.0263],\n",
            "        [ 0.8774, -1.0064],\n",
            "        [ 0.8502, -1.0482],\n",
            "        [ 0.5384, -0.6234],\n",
            "        [ 0.6447, -1.2706],\n",
            "        [ 0.7796, -0.6688],\n",
            "        [ 0.7062, -0.7110],\n",
            "        [ 0.7364, -0.9401],\n",
            "        [ 0.5442, -1.1321],\n",
            "        [ 0.8275, -0.8777],\n",
            "        [ 0.5571, -0.4850],\n",
            "        [ 0.5946, -0.8445],\n",
            "        [ 0.8879, -0.8835],\n",
            "        [ 0.8107, -0.8464],\n",
            "        [-1.1097,  1.4330],\n",
            "        [-0.3996,  0.4154],\n",
            "        [ 0.6207, -0.9177],\n",
            "        [-0.9173,  0.7887],\n",
            "        [-0.0415,  0.2229]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3743, -0.1677],\n",
            "        [-0.4048,  0.4566],\n",
            "        [ 0.0610, -0.1075],\n",
            "        [ 0.7159, -0.6116],\n",
            "        [-0.1668, -0.0473],\n",
            "        [ 0.7162, -0.8368],\n",
            "        [ 0.6483, -0.6419],\n",
            "        [ 0.9371, -0.9965],\n",
            "        [ 1.0192, -0.7762],\n",
            "        [-1.1200,  1.5425],\n",
            "        [-0.5632,  0.6099],\n",
            "        [-0.9386,  0.9413],\n",
            "        [-0.3359,  0.2414],\n",
            "        [ 0.9754, -0.8895],\n",
            "        [ 0.6347, -1.0293],\n",
            "        [ 0.7611, -1.0015],\n",
            "        [-0.5235,  0.8038],\n",
            "        [ 0.7771, -0.6433],\n",
            "        [ 0.6449, -0.9482],\n",
            "        [ 0.5240, -0.1953],\n",
            "        [ 0.7471, -0.9705],\n",
            "        [-0.0825,  0.1786],\n",
            "        [ 0.8123, -1.0347],\n",
            "        [-1.0098,  1.3336],\n",
            "        [ 0.7283, -0.9723],\n",
            "        [ 0.7537, -0.8856],\n",
            "        [-0.7443,  1.0334],\n",
            "        [-0.4075,  0.5069],\n",
            "        [-0.6998,  0.9420],\n",
            "        [-0.8885,  1.1579],\n",
            "        [ 0.6184, -1.0582],\n",
            "        [ 0.7094, -0.8040]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8151, -0.9939],\n",
            "        [-0.2606,  0.1560],\n",
            "        [ 0.8349, -1.0647],\n",
            "        [-0.6738,  0.9880],\n",
            "        [ 0.6606, -0.7511],\n",
            "        [-0.6995,  0.6715],\n",
            "        [-1.2870,  1.4787],\n",
            "        [ 0.8932, -0.9805],\n",
            "        [-1.0928,  1.2054],\n",
            "        [ 0.2317, -0.1307],\n",
            "        [-0.9573,  1.0747],\n",
            "        [ 0.6979, -0.9189],\n",
            "        [ 0.6678, -0.8281],\n",
            "        [-0.5228,  0.5106],\n",
            "        [ 0.5956, -0.8652],\n",
            "        [ 0.5323, -0.2169],\n",
            "        [ 0.6527, -0.2381],\n",
            "        [-0.0632,  0.0082],\n",
            "        [ 0.8075, -0.8609],\n",
            "        [-0.6014,  0.6994],\n",
            "        [ 0.0695,  0.2158],\n",
            "        [ 0.7628, -1.0231],\n",
            "        [-0.8711,  1.1633],\n",
            "        [ 0.2252, -0.6584],\n",
            "        [ 0.6934, -0.7680],\n",
            "        [ 0.8539, -0.6785],\n",
            "        [ 0.5697, -0.7471],\n",
            "        [ 0.6403, -0.8695],\n",
            "        [ 0.3306, -0.4674],\n",
            "        [-0.9887,  1.0955],\n",
            "        [ 0.4472, -0.2977],\n",
            "        [ 0.7669, -1.2384]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4528, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4460, -0.7594],\n",
            "        [-0.4518,  0.5944],\n",
            "        [ 0.8317, -0.6810],\n",
            "        [ 0.7365, -0.8530],\n",
            "        [ 0.4867, -0.9047],\n",
            "        [ 0.7192, -0.7576],\n",
            "        [ 0.5242, -0.7769],\n",
            "        [ 0.7002, -0.8058],\n",
            "        [ 0.7125, -0.8921],\n",
            "        [ 0.6931, -0.9691],\n",
            "        [-0.7251,  0.5365],\n",
            "        [-0.7300,  0.7594],\n",
            "        [ 0.3161, -0.5165],\n",
            "        [ 0.9910, -1.0728],\n",
            "        [ 0.8531, -1.1053],\n",
            "        [ 0.0383, -0.2046],\n",
            "        [ 0.5198, -1.2431],\n",
            "        [ 0.4655, -0.4378],\n",
            "        [ 0.5941, -0.4029],\n",
            "        [ 0.2651, -0.3492],\n",
            "        [-0.9604,  1.1667],\n",
            "        [ 0.9211, -0.7907],\n",
            "        [-0.2115,  0.3585],\n",
            "        [ 0.8501, -0.7833],\n",
            "        [-0.7507,  0.8399],\n",
            "        [ 0.4851, -0.7390],\n",
            "        [-0.5955,  0.7921],\n",
            "        [ 0.5907, -0.7456],\n",
            "        [-0.6242,  0.6647],\n",
            "        [ 0.6619, -1.0054],\n",
            "        [-0.5807,  0.5196],\n",
            "        [ 0.8696, -0.8400]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9595, -1.0617],\n",
            "        [ 0.5364, -0.8893],\n",
            "        [ 0.8279, -1.0875],\n",
            "        [ 0.2502, -0.5617],\n",
            "        [-0.9092,  1.2816],\n",
            "        [ 0.5444, -0.7186],\n",
            "        [-0.5255,  0.5112],\n",
            "        [ 0.6636, -0.9590],\n",
            "        [ 0.6528, -1.0520],\n",
            "        [ 0.6095, -0.9458],\n",
            "        [-0.1063,  0.1722],\n",
            "        [ 0.6129, -0.8588],\n",
            "        [ 0.9635, -1.2447],\n",
            "        [-1.0884,  1.4243],\n",
            "        [ 0.5123, -0.6936],\n",
            "        [-0.8128,  1.1116],\n",
            "        [ 0.7213, -0.7977],\n",
            "        [ 0.8298, -1.0114],\n",
            "        [ 0.7178, -0.7069],\n",
            "        [-1.0296,  1.2241],\n",
            "        [ 0.8658, -0.9483],\n",
            "        [-0.6977,  0.8550],\n",
            "        [ 0.5360, -0.7274],\n",
            "        [ 0.7018, -1.0004],\n",
            "        [-0.8582,  1.0826],\n",
            "        [-0.2425,  0.4104],\n",
            "        [ 0.8358, -1.0201],\n",
            "        [-0.9066,  0.8825],\n",
            "        [ 0.5341, -0.9020],\n",
            "        [ 0.7463, -1.0094],\n",
            "        [-0.9906,  1.3057],\n",
            "        [ 0.7819, -0.9335]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2082,  1.2285],\n",
            "        [-0.3068,  0.0102],\n",
            "        [ 0.6736, -0.4081],\n",
            "        [ 0.5692, -0.9157],\n",
            "        [-0.5676,  0.7822],\n",
            "        [ 0.3743, -0.9288],\n",
            "        [ 0.5835, -1.0409],\n",
            "        [-0.9804,  1.3828],\n",
            "        [ 0.1852, -0.5601],\n",
            "        [ 0.5744, -0.8532],\n",
            "        [-0.6862,  0.8796],\n",
            "        [-0.5378,  0.6560],\n",
            "        [ 0.7347, -1.0218],\n",
            "        [ 0.5746, -1.3026],\n",
            "        [-0.1117, -0.0258],\n",
            "        [ 0.7980, -0.6848],\n",
            "        [-1.1569,  1.4113],\n",
            "        [-0.1294,  0.0263],\n",
            "        [ 0.9418, -1.0822],\n",
            "        [ 0.1008, -0.2883],\n",
            "        [ 0.6635, -0.7195],\n",
            "        [ 0.6387, -0.7052],\n",
            "        [ 0.7516, -0.7961],\n",
            "        [ 0.5052, -0.8554],\n",
            "        [-0.8474,  1.0622],\n",
            "        [-0.8593,  0.9306],\n",
            "        [ 0.8131, -1.1236],\n",
            "        [ 0.7397, -0.8295],\n",
            "        [ 0.7025, -0.9653],\n",
            "        [ 0.4098, -0.1701],\n",
            "        [-0.4519,  0.4496],\n",
            "        [-1.1681,  1.1920]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6561, -0.8932],\n",
            "        [ 0.2681, -0.3269],\n",
            "        [ 0.7688, -1.0935],\n",
            "        [ 0.0998, -0.7902],\n",
            "        [ 0.3529, -0.4048],\n",
            "        [-1.0375,  1.2049],\n",
            "        [ 0.3603, -0.5280],\n",
            "        [-0.9991,  1.1314],\n",
            "        [ 0.8803, -0.9644],\n",
            "        [ 0.9411, -0.9711],\n",
            "        [ 0.6896, -0.7262],\n",
            "        [-0.8912,  1.1396],\n",
            "        [-0.9636,  1.1980],\n",
            "        [-1.1566,  1.2105],\n",
            "        [ 0.7891, -0.9571],\n",
            "        [ 0.7700, -0.6474],\n",
            "        [ 0.5551, -0.8437],\n",
            "        [ 0.8324, -0.9323],\n",
            "        [ 0.7439, -1.1750],\n",
            "        [ 0.1110,  0.1859],\n",
            "        [ 0.6378, -0.6542],\n",
            "        [ 0.0482,  0.0492],\n",
            "        [-1.2090,  1.3468],\n",
            "        [ 0.3065, -0.1674],\n",
            "        [ 0.7194, -1.0062],\n",
            "        [ 0.7947, -1.1067],\n",
            "        [ 0.4456, -0.6722],\n",
            "        [ 0.8951, -0.8863],\n",
            "        [ 0.8743, -0.6476],\n",
            "        [ 0.6642, -0.7475],\n",
            "        [-0.9616,  1.1811],\n",
            "        [ 0.6084, -0.7563]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7951, -0.9312],\n",
            "        [ 0.3270, -0.3046],\n",
            "        [ 0.0587,  0.1771],\n",
            "        [ 0.5871, -0.5387],\n",
            "        [-0.4008,  0.4874],\n",
            "        [ 0.8454, -0.8642],\n",
            "        [ 0.6805, -0.4656],\n",
            "        [ 0.5403, -0.4969],\n",
            "        [ 0.4644, -0.9902],\n",
            "        [ 0.9221, -0.6800],\n",
            "        [ 0.5417, -1.2250],\n",
            "        [-0.6327,  0.9709],\n",
            "        [ 0.0675,  0.0056],\n",
            "        [ 0.5782, -1.0260],\n",
            "        [ 0.4976, -0.8724],\n",
            "        [-0.2212,  0.2530],\n",
            "        [ 0.6950, -0.7194],\n",
            "        [-0.2981,  0.4061],\n",
            "        [-0.2283,  0.0285],\n",
            "        [ 0.7318, -0.7368],\n",
            "        [ 0.5403, -0.2440],\n",
            "        [ 0.6384, -0.5826],\n",
            "        [-0.4223,  0.9305],\n",
            "        [ 0.8415, -1.0724],\n",
            "        [ 0.3481, -1.1031],\n",
            "        [ 1.0441, -0.9358],\n",
            "        [ 0.1575, -0.1714],\n",
            "        [-0.5237,  0.5462],\n",
            "        [ 0.1652,  0.0529],\n",
            "        [ 0.7448, -0.7374],\n",
            "        [-1.2648,  1.1241],\n",
            "        [-0.3501,  0.3500]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3194, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3931, -0.4640],\n",
            "        [-1.1850,  1.0307],\n",
            "        [ 0.4312, -0.7080],\n",
            "        [ 0.5892, -0.8943],\n",
            "        [ 0.7863, -1.3124],\n",
            "        [-1.2351,  1.0387],\n",
            "        [ 0.6163, -0.8297],\n",
            "        [ 0.6456, -0.9263],\n",
            "        [ 0.2003,  0.0249],\n",
            "        [ 0.9485, -1.0078],\n",
            "        [ 0.7975, -1.0171],\n",
            "        [ 0.5489, -0.7466],\n",
            "        [ 0.0583, -0.0171],\n",
            "        [-1.3130,  1.3127],\n",
            "        [ 0.7202, -0.9401],\n",
            "        [-0.8855,  0.8455],\n",
            "        [ 0.4995, -0.4986],\n",
            "        [ 0.7121, -0.9131],\n",
            "        [-0.5575,  0.7107],\n",
            "        [ 0.7213, -0.7556],\n",
            "        [ 0.3349, -0.5255],\n",
            "        [ 0.4582, -0.7474],\n",
            "        [-0.5828,  0.9542],\n",
            "        [ 0.2013, -0.6273],\n",
            "        [ 0.5939, -0.6006],\n",
            "        [ 0.6525, -0.5661],\n",
            "        [ 0.9541, -1.0364],\n",
            "        [-0.8045,  1.2557],\n",
            "        [ 0.7731, -0.7944],\n",
            "        [ 0.7807, -0.8884],\n",
            "        [-0.4524,  0.7079],\n",
            "        [ 0.4191, -0.7841]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3344, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1451, -0.3853],\n",
            "        [ 0.7359, -0.4620],\n",
            "        [ 0.9421, -1.0643],\n",
            "        [ 0.6486, -1.1092],\n",
            "        [ 0.7539, -0.7372],\n",
            "        [ 0.7081, -0.8630],\n",
            "        [-0.5290,  0.3822],\n",
            "        [ 0.2615, -0.2045],\n",
            "        [-1.1403,  1.3358],\n",
            "        [ 0.6424, -0.8973],\n",
            "        [ 0.7309, -0.8387],\n",
            "        [ 0.7463, -1.1289],\n",
            "        [ 0.7049, -0.6440],\n",
            "        [ 0.8298, -0.9941],\n",
            "        [ 0.6730, -0.7288],\n",
            "        [ 0.8660, -0.8824],\n",
            "        [ 0.6888, -0.6323],\n",
            "        [-1.2137,  1.2127],\n",
            "        [ 0.8236, -0.7476],\n",
            "        [ 0.5241, -0.7708],\n",
            "        [ 0.8449, -0.9560],\n",
            "        [-0.1485,  0.3444],\n",
            "        [ 0.4494, -0.9147],\n",
            "        [-0.2173,  0.3949],\n",
            "        [ 0.5662, -0.7981],\n",
            "        [-0.8486,  0.7327],\n",
            "        [ 0.6416, -0.9769],\n",
            "        [ 0.4305, -0.6113],\n",
            "        [ 0.6314, -0.8464],\n",
            "        [-0.0120, -0.1224],\n",
            "        [-0.7779,  0.7893],\n",
            "        [ 0.5237, -0.6536]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4226, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6229,  0.8151],\n",
            "        [-0.0024,  0.1938],\n",
            "        [-1.5088,  1.6038],\n",
            "        [-0.7649,  0.8548],\n",
            "        [-0.7035,  0.9967],\n",
            "        [ 0.7618, -0.9871],\n",
            "        [-0.2505,  0.1568],\n",
            "        [ 0.2920, -0.4623],\n",
            "        [ 0.6098, -0.9979],\n",
            "        [-0.7706,  0.8364],\n",
            "        [ 0.7701, -0.8446],\n",
            "        [ 0.9291, -0.8027],\n",
            "        [-0.1665,  0.0775],\n",
            "        [-0.1790, -0.0244],\n",
            "        [ 0.6095, -0.8613],\n",
            "        [-1.1330,  1.1179],\n",
            "        [ 0.5104, -0.8413],\n",
            "        [ 1.0256, -0.9557],\n",
            "        [ 0.4949, -0.7535],\n",
            "        [ 0.4053, -0.2848],\n",
            "        [ 0.5701, -0.7743],\n",
            "        [ 0.9009, -0.8096],\n",
            "        [ 0.7822, -1.0669],\n",
            "        [ 0.3372, -0.7085],\n",
            "        [-1.0862,  1.1293],\n",
            "        [ 0.4162, -0.5274],\n",
            "        [-1.0841,  1.4426],\n",
            "        [-0.5588,  0.6568],\n",
            "        [-0.8178,  0.8557],\n",
            "        [ 0.8172, -0.9709],\n",
            "        [ 0.1932, -0.0660],\n",
            "        [ 0.0083,  0.1763]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0792,  1.2254],\n",
            "        [-0.9739,  1.1481],\n",
            "        [ 0.7604, -0.9695],\n",
            "        [ 0.2118, -0.3993],\n",
            "        [-0.3088,  0.1954],\n",
            "        [ 0.5911, -0.5596],\n",
            "        [ 0.6513, -0.8504],\n",
            "        [-0.7053,  0.9500],\n",
            "        [ 0.9726, -1.1256],\n",
            "        [ 0.7899, -0.7485],\n",
            "        [ 0.8086, -0.8730],\n",
            "        [ 0.0298, -0.1358],\n",
            "        [ 0.5889, -0.9962],\n",
            "        [-0.2931,  0.2320],\n",
            "        [-1.1159,  0.9362],\n",
            "        [ 0.2951, -0.5628],\n",
            "        [ 0.8077, -1.0112],\n",
            "        [-0.5640,  0.8328],\n",
            "        [ 0.5706, -0.9383],\n",
            "        [-0.9242,  1.2457],\n",
            "        [ 0.6979, -0.7968],\n",
            "        [ 0.9355, -0.7979],\n",
            "        [-0.9337,  0.9846],\n",
            "        [ 0.4175, -0.6050],\n",
            "        [ 0.7514, -0.7236],\n",
            "        [ 0.8371, -0.9178],\n",
            "        [-0.2265,  0.3889],\n",
            "        [ 0.4684, -0.4205],\n",
            "        [ 0.8427, -1.0666],\n",
            "        [ 0.2504, -0.6346],\n",
            "        [-1.3069,  1.1982],\n",
            "        [ 0.9264, -0.9711]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7090, -0.8415],\n",
            "        [ 1.0051, -1.1128],\n",
            "        [ 0.8587, -0.9673],\n",
            "        [-0.6281,  0.6610],\n",
            "        [ 0.7886, -0.8320],\n",
            "        [ 0.4379, -0.5378],\n",
            "        [ 0.6752, -1.3614],\n",
            "        [ 0.6677, -0.7153],\n",
            "        [-0.2762,  0.1979],\n",
            "        [ 0.0984,  0.0691],\n",
            "        [ 0.4036, -0.7612],\n",
            "        [-1.0683,  1.4331],\n",
            "        [-1.1764,  1.2985],\n",
            "        [ 0.7138, -0.5659],\n",
            "        [-0.6523,  0.5862],\n",
            "        [ 0.7696, -0.9381],\n",
            "        [ 0.8879, -0.4802],\n",
            "        [ 0.5714, -0.6597],\n",
            "        [ 0.6437, -0.9695],\n",
            "        [ 0.0338, -0.2202],\n",
            "        [-1.0473,  1.1899],\n",
            "        [-1.1752,  1.3847],\n",
            "        [ 0.9451, -1.0469],\n",
            "        [ 0.8684, -0.8937],\n",
            "        [ 0.7003, -0.6003],\n",
            "        [-1.3874,  1.3067],\n",
            "        [ 0.7173, -0.9169],\n",
            "        [ 0.8490, -0.7877],\n",
            "        [ 0.6816, -1.0184],\n",
            "        [-0.8567,  1.0237],\n",
            "        [-0.8573,  1.3481],\n",
            "        [-0.9916,  1.1833]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4696, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1186,  1.3854],\n",
            "        [ 0.7674, -0.5499],\n",
            "        [ 0.7946, -1.0457],\n",
            "        [-1.0592,  1.0696],\n",
            "        [ 0.5490, -0.8260],\n",
            "        [ 0.4994, -0.6870],\n",
            "        [ 0.6419, -0.8027],\n",
            "        [ 0.6091, -0.0563],\n",
            "        [-0.8682,  0.8233],\n",
            "        [ 0.0033, -0.0470],\n",
            "        [ 0.6473, -0.6923],\n",
            "        [ 0.5036, -0.4976],\n",
            "        [-0.2283,  0.4261],\n",
            "        [ 0.6097, -1.0966],\n",
            "        [ 0.7676, -0.8216],\n",
            "        [ 0.6340, -0.5838],\n",
            "        [ 0.2991, -0.5738],\n",
            "        [ 0.7193, -0.9321],\n",
            "        [ 0.7297, -0.9385],\n",
            "        [-1.0652,  1.1693],\n",
            "        [ 0.7325, -1.1251],\n",
            "        [-1.1969,  1.3657],\n",
            "        [ 0.6607, -1.2121],\n",
            "        [ 0.3385, -0.5570],\n",
            "        [ 0.0467, -0.5344],\n",
            "        [-0.7885,  0.6813],\n",
            "        [-0.3357,  0.3907],\n",
            "        [ 0.5370, -0.9256],\n",
            "        [ 0.5401, -0.3628],\n",
            "        [ 0.8599, -1.2231],\n",
            "        [ 0.6045, -1.1756],\n",
            "        [-1.0587,  0.9654]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2341,  0.4499],\n",
            "        [-1.1119,  1.6593],\n",
            "        [ 0.0762,  0.1959],\n",
            "        [ 0.8735, -0.9079],\n",
            "        [ 0.7189, -0.6946],\n",
            "        [ 0.9000, -0.8746],\n",
            "        [-0.2633,  0.1630],\n",
            "        [-0.5044,  0.3648],\n",
            "        [ 0.9662, -1.2131],\n",
            "        [-0.2681,  0.5228],\n",
            "        [ 0.3989, -0.3482],\n",
            "        [-0.9917,  1.1500],\n",
            "        [ 0.8452, -0.7494],\n",
            "        [ 0.0143,  0.0254],\n",
            "        [-1.3001,  1.2192],\n",
            "        [ 0.0243,  0.0976],\n",
            "        [ 0.4955, -0.8545],\n",
            "        [ 0.9990, -0.8887],\n",
            "        [-0.7852,  0.9565],\n",
            "        [ 0.8394, -1.0073],\n",
            "        [ 0.5725, -0.7233],\n",
            "        [ 0.7116, -0.2893],\n",
            "        [ 0.6130, -1.1846],\n",
            "        [ 0.6238, -0.7611],\n",
            "        [-0.9203,  0.9772],\n",
            "        [ 0.9459, -1.1324],\n",
            "        [ 0.6233, -0.9954],\n",
            "        [-0.5125,  0.6695],\n",
            "        [ 0.8418, -0.8653],\n",
            "        [ 0.5465, -0.7472],\n",
            "        [-0.9545,  1.2979],\n",
            "        [ 0.0408, -0.3103]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4598,  0.5991],\n",
            "        [ 0.8766, -0.8489],\n",
            "        [-0.2243,  0.1659],\n",
            "        [ 0.5019, -0.5720],\n",
            "        [ 0.3314, -0.5870],\n",
            "        [ 0.8111, -0.9773],\n",
            "        [ 0.0385, -0.3000],\n",
            "        [ 0.1225, -0.1395],\n",
            "        [-1.0452,  1.3155],\n",
            "        [ 0.6376, -0.8427],\n",
            "        [-0.5864,  0.9351],\n",
            "        [ 0.3324, -0.3912],\n",
            "        [ 0.4109, -0.5521],\n",
            "        [-1.1686,  1.2260],\n",
            "        [ 0.6152, -0.8371],\n",
            "        [ 0.7190, -0.9745],\n",
            "        [-0.1301,  0.0657],\n",
            "        [-0.7280,  0.8981],\n",
            "        [ 0.4049, -0.7375],\n",
            "        [-0.8960,  1.0742],\n",
            "        [ 0.0719, -0.7247],\n",
            "        [ 0.2354, -0.1929],\n",
            "        [ 0.8787, -0.9679],\n",
            "        [-1.0605,  1.0353],\n",
            "        [-0.6557,  1.0720],\n",
            "        [ 0.8223, -0.8938],\n",
            "        [ 0.6080, -0.7600],\n",
            "        [ 0.5851, -0.7142],\n",
            "        [ 0.0177, -0.1556],\n",
            "        [-1.3024,  1.3170],\n",
            "        [ 0.6322, -0.8230],\n",
            "        [ 0.7356, -0.8613]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0281, -0.1278],\n",
            "        [-1.0545,  1.0132],\n",
            "        [ 0.3954, -0.6970],\n",
            "        [ 0.8562, -1.1050],\n",
            "        [ 0.1805, -0.1490],\n",
            "        [ 0.7424, -0.9219],\n",
            "        [ 0.8436, -0.7394],\n",
            "        [-0.1837,  0.3448],\n",
            "        [ 0.7725, -1.0214],\n",
            "        [-0.2791,  0.1948],\n",
            "        [ 0.7891, -0.6308],\n",
            "        [-1.1415,  1.5750],\n",
            "        [-1.0027,  1.2444],\n",
            "        [ 0.9432, -0.8687],\n",
            "        [ 0.8309, -0.8488],\n",
            "        [-0.4872,  0.6815],\n",
            "        [ 0.9445, -0.9698],\n",
            "        [ 0.5340, -0.2919],\n",
            "        [-1.0775,  0.8459],\n",
            "        [-0.9792,  1.0639],\n",
            "        [-1.1920,  1.3312],\n",
            "        [ 0.7070, -1.0264],\n",
            "        [ 0.5975, -0.7479],\n",
            "        [-1.0872,  1.2527],\n",
            "        [ 1.0237, -1.1673],\n",
            "        [ 0.5472, -0.9579],\n",
            "        [ 1.0137, -0.9426],\n",
            "        [ 0.2400, -0.5978],\n",
            "        [-1.0605,  1.3715],\n",
            "        [ 0.4092, -0.6654],\n",
            "        [ 0.9195, -1.0689],\n",
            "        [-0.4342,  0.5003]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4583, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6072, -0.9228],\n",
            "        [ 0.8354, -1.0721],\n",
            "        [ 0.9400, -0.9106],\n",
            "        [ 0.6961, -1.0000],\n",
            "        [-1.0461,  0.9595],\n",
            "        [ 0.3453, -0.6177],\n",
            "        [ 0.3947, -0.2609],\n",
            "        [-0.8306,  0.9646],\n",
            "        [ 0.7170, -0.7757],\n",
            "        [-0.6709,  0.7826],\n",
            "        [ 0.5415, -0.4052],\n",
            "        [ 0.7377, -0.9855],\n",
            "        [-0.6941,  0.8924],\n",
            "        [-1.1754,  1.2272],\n",
            "        [ 0.7517, -0.9319],\n",
            "        [ 0.8247, -0.9273],\n",
            "        [ 0.7532, -0.6068],\n",
            "        [ 0.4944, -0.6322],\n",
            "        [ 0.5527, -0.9319],\n",
            "        [-1.0992,  1.3846],\n",
            "        [ 0.0480, -0.0879],\n",
            "        [ 0.5368, -0.6030],\n",
            "        [ 0.5813, -0.9122],\n",
            "        [ 0.5927, -1.0007],\n",
            "        [-1.2812,  1.2196],\n",
            "        [-0.5784,  0.6805],\n",
            "        [ 0.3424, -0.0852],\n",
            "        [ 0.5751, -0.4757],\n",
            "        [ 0.5381, -0.5438],\n",
            "        [ 0.8737, -0.5762],\n",
            "        [-0.0991,  0.1604],\n",
            "        [ 0.3571, -0.7147]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2823,  0.3837],\n",
            "        [ 0.6629, -1.0308],\n",
            "        [ 0.0769, -0.1099],\n",
            "        [ 1.0217, -0.9164],\n",
            "        [ 0.3465, -0.7058],\n",
            "        [-1.0688,  1.4684],\n",
            "        [ 0.6678, -0.9702],\n",
            "        [ 0.8174, -0.6437],\n",
            "        [-1.1874,  1.5162],\n",
            "        [ 0.4460, -0.5170],\n",
            "        [ 0.5280, -0.8417],\n",
            "        [ 0.2514, -0.5208],\n",
            "        [ 0.7691, -1.0028],\n",
            "        [-0.5327,  0.3670],\n",
            "        [ 0.4699, -0.7225],\n",
            "        [-1.2763,  1.4797],\n",
            "        [ 0.6645, -0.7212],\n",
            "        [ 0.5041, -0.4541],\n",
            "        [ 0.5108, -1.1146],\n",
            "        [-0.9736,  1.4110],\n",
            "        [ 0.6547, -0.7153],\n",
            "        [-1.3583,  1.2260],\n",
            "        [-0.8759,  1.0315],\n",
            "        [-0.1008,  0.2736],\n",
            "        [-1.0946,  1.2861],\n",
            "        [ 0.3423, -0.4091],\n",
            "        [ 0.1917, -0.2216],\n",
            "        [ 0.7791, -0.9643],\n",
            "        [-1.3152,  1.5489],\n",
            "        [ 0.0607, -0.2879],\n",
            "        [ 0.7274, -0.7896],\n",
            "        [-1.3806,  1.3839]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4696, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1973, -0.1917],\n",
            "        [-0.4805,  0.5620],\n",
            "        [-1.3939,  1.3641],\n",
            "        [ 0.5884, -1.0288],\n",
            "        [ 0.7874, -0.9246],\n",
            "        [ 0.6029, -0.8969],\n",
            "        [ 0.7424, -0.7303],\n",
            "        [ 0.1360, -0.5652],\n",
            "        [-1.1131,  1.4948],\n",
            "        [-0.1411,  0.3172],\n",
            "        [-1.1532,  1.5183],\n",
            "        [ 0.7435, -0.6659],\n",
            "        [ 0.8625, -1.0913],\n",
            "        [-1.1634,  1.2440],\n",
            "        [ 0.6585, -0.7585],\n",
            "        [ 0.8744, -0.9156],\n",
            "        [ 0.4148, -1.0919],\n",
            "        [-0.9035,  1.1785],\n",
            "        [ 0.4517, -0.6069],\n",
            "        [ 1.0448, -1.1943],\n",
            "        [ 0.3620, -0.7141],\n",
            "        [-1.1319,  1.4391],\n",
            "        [ 0.4864, -1.0720],\n",
            "        [ 0.7122, -0.5773],\n",
            "        [ 0.5281, -0.8241],\n",
            "        [ 0.4453, -0.6999],\n",
            "        [ 0.6707, -0.7682],\n",
            "        [ 0.7344, -0.8943],\n",
            "        [-0.7637,  0.9689],\n",
            "        [-0.6872,  0.7766],\n",
            "        [-0.1965,  0.3649],\n",
            "        [-0.5345,  0.7037]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3297, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8805, -0.9000],\n",
            "        [ 0.8147, -0.8816],\n",
            "        [ 0.0539,  0.3621],\n",
            "        [-0.6107,  1.1141],\n",
            "        [ 1.0463, -1.2174],\n",
            "        [ 0.2247, -0.7330],\n",
            "        [ 0.4026, -0.8670],\n",
            "        [ 0.6612, -0.7453],\n",
            "        [-1.1621,  1.3423],\n",
            "        [ 0.3732, -0.7842],\n",
            "        [-1.0441,  1.0568],\n",
            "        [ 0.7138, -1.1230],\n",
            "        [-0.8191,  0.7610],\n",
            "        [-1.2884,  1.1520],\n",
            "        [-1.0580,  0.7322],\n",
            "        [-0.7538,  1.0460],\n",
            "        [-1.2338,  1.3767],\n",
            "        [-1.0853,  1.2701],\n",
            "        [ 0.6159, -0.5265],\n",
            "        [ 0.0564, -0.3947],\n",
            "        [ 0.5985, -1.0443],\n",
            "        [-0.9823,  1.1452],\n",
            "        [-0.6442,  0.5655],\n",
            "        [ 0.6956, -0.6956],\n",
            "        [ 0.7102, -0.8903],\n",
            "        [-0.8934,  1.1346],\n",
            "        [ 0.6820, -1.0128],\n",
            "        [-1.1142,  1.2191],\n",
            "        [ 0.6502, -1.0638],\n",
            "        [-0.4271,  0.3258],\n",
            "        [-1.0255,  1.4987],\n",
            "        [ 0.9793, -0.9728]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8501, -1.0607],\n",
            "        [ 0.4433, -0.7880],\n",
            "        [-0.5805,  1.1129],\n",
            "        [ 0.9544, -0.8471],\n",
            "        [ 0.7410, -0.9181],\n",
            "        [ 0.1126, -0.4788],\n",
            "        [ 0.9987, -1.1749],\n",
            "        [-1.0337,  0.9766],\n",
            "        [ 0.3638, -0.4915],\n",
            "        [ 0.7733, -0.9350],\n",
            "        [-0.9852,  0.9226],\n",
            "        [-0.6452,  0.4640],\n",
            "        [-1.2505,  1.3499],\n",
            "        [ 0.6936, -0.8386],\n",
            "        [-0.4499,  0.2388],\n",
            "        [ 0.8597, -1.3961],\n",
            "        [-1.0619,  1.2786],\n",
            "        [-0.7008,  0.8702],\n",
            "        [ 1.1462, -1.0621],\n",
            "        [ 0.6494, -1.0597],\n",
            "        [ 0.5078, -0.4625],\n",
            "        [ 0.2638, -0.6361],\n",
            "        [ 0.1715, -0.5429],\n",
            "        [ 0.6551, -0.8056],\n",
            "        [ 0.7524, -0.6064],\n",
            "        [-1.0684,  1.3810],\n",
            "        [ 0.6461, -1.1243],\n",
            "        [-1.1887,  1.4073],\n",
            "        [ 0.9332, -0.9957],\n",
            "        [ 0.0969, -0.2364],\n",
            "        [ 0.5512, -0.8032],\n",
            "        [-1.2666,  1.1859]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 4.0160e-01, -7.5242e-01],\n",
            "        [-1.0650e+00,  1.1070e+00],\n",
            "        [-1.2961e+00,  1.5048e+00],\n",
            "        [-8.4765e-01,  1.5567e+00],\n",
            "        [-8.2038e-01,  8.3215e-01],\n",
            "        [ 2.9293e-01, -3.2361e-01],\n",
            "        [ 9.3503e-01, -7.8013e-01],\n",
            "        [ 6.1721e-01, -1.0068e+00],\n",
            "        [ 7.0039e-01, -7.1539e-01],\n",
            "        [ 6.7212e-01, -1.0243e+00],\n",
            "        [ 7.0097e-01, -8.8922e-01],\n",
            "        [-1.4595e-01,  7.9878e-02],\n",
            "        [-3.2455e-01,  5.3475e-01],\n",
            "        [ 5.9114e-01, -5.9076e-01],\n",
            "        [ 4.1860e-01, -9.5597e-01],\n",
            "        [-2.1982e-01, -3.5920e-02],\n",
            "        [ 7.4498e-01, -1.1224e+00],\n",
            "        [-1.1555e+00,  1.2194e+00],\n",
            "        [-5.7953e-01,  6.8693e-01],\n",
            "        [ 9.1961e-01, -1.1263e+00],\n",
            "        [ 7.7260e-01, -7.9248e-01],\n",
            "        [ 6.9757e-01, -1.1418e+00],\n",
            "        [-5.2411e-01,  4.6790e-01],\n",
            "        [-1.2945e+00,  1.4635e+00],\n",
            "        [-7.5427e-01,  9.8563e-01],\n",
            "        [ 3.6454e-01, -3.3678e-01],\n",
            "        [ 1.9562e-01, -3.8219e-01],\n",
            "        [ 2.5016e-01, -6.2030e-04],\n",
            "        [ 6.8505e-01, -9.8210e-01],\n",
            "        [-6.2285e-02,  9.1482e-02],\n",
            "        [ 4.0239e-01, -8.4656e-01],\n",
            "        [-7.6542e-01,  6.9858e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5022, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9857,  1.2275],\n",
            "        [ 0.4859, -0.7522],\n",
            "        [-1.1832,  1.3018],\n",
            "        [ 0.4858, -0.5970],\n",
            "        [-1.2518,  1.3277],\n",
            "        [ 0.3901, -0.2353],\n",
            "        [ 0.9119, -1.0517],\n",
            "        [ 0.6240, -0.6395],\n",
            "        [ 0.6619, -0.7656],\n",
            "        [-0.2870,  0.6159],\n",
            "        [ 0.8025, -1.1131],\n",
            "        [ 0.9372, -0.8409],\n",
            "        [-0.0807,  0.3232],\n",
            "        [ 0.5467, -0.8404],\n",
            "        [ 0.5997, -0.9825],\n",
            "        [-1.2076,  1.2067],\n",
            "        [ 0.6122, -0.7676],\n",
            "        [-1.3866,  1.6273],\n",
            "        [ 0.5436, -1.1451],\n",
            "        [-0.7258,  0.9717],\n",
            "        [ 0.7969, -0.8566],\n",
            "        [ 0.7756, -0.7745],\n",
            "        [ 0.5266, -0.7241],\n",
            "        [ 0.6703, -0.9670],\n",
            "        [ 0.6175, -0.7584],\n",
            "        [-0.8864,  0.8538],\n",
            "        [ 0.5038, -0.5884],\n",
            "        [-0.0938,  0.4220],\n",
            "        [ 0.6247, -0.9060],\n",
            "        [ 0.3038, -0.3000],\n",
            "        [ 0.7375, -0.7599],\n",
            "        [ 0.5684, -0.8603]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5976, -0.8068],\n",
            "        [ 0.7628, -0.7735],\n",
            "        [ 0.5188, -0.8957],\n",
            "        [-0.6311,  0.9932],\n",
            "        [ 0.8410, -0.7478],\n",
            "        [ 0.3884, -0.4828],\n",
            "        [ 0.3446, -0.6713],\n",
            "        [-0.8215,  1.2352],\n",
            "        [-1.2811,  1.4060],\n",
            "        [-0.0853, -0.2202],\n",
            "        [-0.6802,  0.4684],\n",
            "        [ 0.4484, -0.1941],\n",
            "        [-1.3276,  1.4410],\n",
            "        [-0.1191,  0.2485],\n",
            "        [-0.7978,  0.9868],\n",
            "        [ 0.8788, -1.1997],\n",
            "        [ 0.3184, -0.6261],\n",
            "        [ 0.6696, -0.9645],\n",
            "        [-0.0326, -0.2266],\n",
            "        [ 0.9541, -0.9725],\n",
            "        [ 0.6273, -0.9856],\n",
            "        [ 0.7322, -0.8534],\n",
            "        [-0.4491,  0.3846],\n",
            "        [-0.5219,  0.5610],\n",
            "        [ 0.0181, -0.2107],\n",
            "        [-0.5799,  0.9315],\n",
            "        [-1.1062,  1.2831],\n",
            "        [-0.7796,  1.0244],\n",
            "        [ 0.9271, -1.0537],\n",
            "        [ 0.0834, -0.2402],\n",
            "        [-1.3514,  1.4121],\n",
            "        [-1.0151,  1.3474]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7723, -0.7688],\n",
            "        [ 0.7026, -0.7081],\n",
            "        [ 0.2766, -0.2084],\n",
            "        [-1.0058,  1.2322],\n",
            "        [ 0.2898, -0.6169],\n",
            "        [ 0.6159, -0.5550],\n",
            "        [ 0.8457, -1.2868],\n",
            "        [-1.2052,  1.3137],\n",
            "        [-0.9647,  1.0288],\n",
            "        [ 0.7202, -0.8729],\n",
            "        [-1.0632,  1.3696],\n",
            "        [-0.6022,  1.1259],\n",
            "        [-1.2940,  1.3245],\n",
            "        [-0.1795,  0.3370],\n",
            "        [ 0.9645, -0.6640],\n",
            "        [-0.0489, -0.0829],\n",
            "        [ 0.6979, -1.0442],\n",
            "        [ 0.4755, -1.0247],\n",
            "        [-0.9612,  1.1056],\n",
            "        [ 0.7896, -1.0443],\n",
            "        [-1.0870,  1.3068],\n",
            "        [ 0.7771, -0.9660],\n",
            "        [-0.3284,  0.1096],\n",
            "        [-0.2275, -0.0854],\n",
            "        [-1.0617,  1.3295],\n",
            "        [ 0.1867, -0.3483],\n",
            "        [-0.7484,  0.8183],\n",
            "        [-1.0708,  1.1231],\n",
            "        [ 0.5596, -0.7576],\n",
            "        [ 0.4868, -0.2038],\n",
            "        [ 0.7649, -0.8543],\n",
            "        [ 0.8044, -0.8185]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1156, -0.1507],\n",
            "        [ 0.7985, -1.0294],\n",
            "        [-0.3529,  0.4935],\n",
            "        [-0.8225,  0.9409],\n",
            "        [ 0.9282, -0.7324],\n",
            "        [ 0.0776,  0.0154],\n",
            "        [-1.3389,  1.4983],\n",
            "        [ 0.1882, -0.1645],\n",
            "        [-1.1111,  1.3118],\n",
            "        [ 0.6666, -0.9426]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epoch took: 0:04:28\n",
            "\n",
            "Running Validation...\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6380, device='cuda:0'), logits=tensor([[ 0.5038, -0.6616],\n",
            "        [-1.0237,  1.2677],\n",
            "        [-0.9148,  1.0897],\n",
            "        [ 0.1542, -0.3224],\n",
            "        [ 0.1218, -0.3325],\n",
            "        [-1.0825,  1.1885],\n",
            "        [-0.7963,  1.0950],\n",
            "        [ 0.8393, -0.8931],\n",
            "        [ 0.8205, -1.0359],\n",
            "        [ 0.9413, -1.0081],\n",
            "        [-1.1014,  1.3654],\n",
            "        [-1.2308,  1.3627],\n",
            "        [-1.1138,  1.3599],\n",
            "        [-1.0444,  1.1402],\n",
            "        [-1.1817,  1.4567],\n",
            "        [ 0.7160, -0.8543],\n",
            "        [ 0.3974, -0.6549],\n",
            "        [ 0.8361, -1.0205],\n",
            "        [ 0.0302,  0.2896],\n",
            "        [ 0.9243, -1.0751],\n",
            "        [-0.9637,  1.2875],\n",
            "        [-0.7153,  0.8253],\n",
            "        [-1.1846,  1.3618],\n",
            "        [ 0.8308, -1.0192],\n",
            "        [-0.9630,  1.1252],\n",
            "        [ 0.7986, -1.0254],\n",
            "        [-1.2559,  1.4824],\n",
            "        [-0.6037,  0.7583],\n",
            "        [-1.1734,  1.4579],\n",
            "        [-0.5604,  0.7242],\n",
            "        [-0.8402,  0.8579],\n",
            "        [ 0.9670, -1.0221]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3163, device='cuda:0'), logits=tensor([[-0.7773,  1.0428],\n",
            "        [ 0.4863, -0.8276],\n",
            "        [-0.4482,  0.4882],\n",
            "        [ 0.7242, -0.7095],\n",
            "        [-1.1244,  1.3018],\n",
            "        [ 0.8053, -0.9126],\n",
            "        [ 0.4808, -0.6184],\n",
            "        [ 0.4847, -0.7576],\n",
            "        [ 0.4960, -0.7783],\n",
            "        [ 0.8172, -1.0339],\n",
            "        [ 0.7729, -0.9649],\n",
            "        [-0.3752,  0.5733],\n",
            "        [ 0.8131, -0.9365],\n",
            "        [ 0.6594, -0.9210],\n",
            "        [-0.8011,  0.9064],\n",
            "        [ 0.8355, -0.9043],\n",
            "        [ 0.6646, -0.8038],\n",
            "        [-1.0289,  1.2663],\n",
            "        [-0.7880,  0.8434],\n",
            "        [ 0.7673, -1.0374],\n",
            "        [-0.9076,  1.1208],\n",
            "        [-0.9418,  0.9661],\n",
            "        [-0.5305,  0.6634],\n",
            "        [ 0.5065, -0.8152],\n",
            "        [-0.8733,  1.1831],\n",
            "        [ 0.2073, -0.1153],\n",
            "        [ 0.9029, -1.0583],\n",
            "        [ 0.4986, -0.7961],\n",
            "        [ 0.6588, -0.7698],\n",
            "        [-0.1186,  0.1216],\n",
            "        [ 0.7674, -1.0187],\n",
            "        [-0.5098,  0.8845]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3878, device='cuda:0'), logits=tensor([[ 0.8597, -1.0167],\n",
            "        [ 0.8225, -0.8975],\n",
            "        [ 0.0403, -0.0200],\n",
            "        [-1.1574,  1.2883],\n",
            "        [-1.1056,  1.2747],\n",
            "        [-1.2934,  1.4753],\n",
            "        [ 0.7786, -0.8696],\n",
            "        [ 0.8075, -1.0190],\n",
            "        [ 0.8562, -0.8790],\n",
            "        [ 0.4692, -0.6342],\n",
            "        [-0.6837,  0.7100],\n",
            "        [ 0.4642, -0.8013],\n",
            "        [ 0.5625, -0.7773],\n",
            "        [ 0.9022, -0.9968],\n",
            "        [-1.2352,  1.2748],\n",
            "        [-0.7106,  0.7436],\n",
            "        [-1.2000,  1.2967],\n",
            "        [ 0.6911, -0.7613],\n",
            "        [ 0.0180, -0.0481],\n",
            "        [ 0.8161, -0.9903],\n",
            "        [ 0.7885, -1.0035],\n",
            "        [-1.1020,  1.3995],\n",
            "        [ 0.7702, -1.0132],\n",
            "        [ 0.7611, -0.9259],\n",
            "        [ 0.7992, -0.9720],\n",
            "        [-0.1279,  0.1873],\n",
            "        [ 0.8754, -0.9699],\n",
            "        [ 0.6471, -0.8240],\n",
            "        [-1.2960,  1.4249],\n",
            "        [-1.2208,  1.4236],\n",
            "        [ 0.6577, -0.8061],\n",
            "        [-1.1259,  1.3135]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3998, device='cuda:0'), logits=tensor([[ 0.5105, -0.6123],\n",
            "        [-0.9573,  1.2486],\n",
            "        [-1.0187,  0.9807],\n",
            "        [ 0.8056, -1.0694],\n",
            "        [ 0.7785, -1.0379],\n",
            "        [ 0.7341, -0.9295],\n",
            "        [-1.2947,  1.4141],\n",
            "        [-0.7408,  1.0769],\n",
            "        [ 0.7558, -0.9557],\n",
            "        [ 0.6953, -0.8251],\n",
            "        [ 0.6443, -0.7513],\n",
            "        [-1.1526,  1.4306],\n",
            "        [-1.2119,  1.3148],\n",
            "        [ 0.7334, -0.8179],\n",
            "        [-0.1225,  0.2256],\n",
            "        [-0.8726,  1.1579],\n",
            "        [-1.1438,  1.4104],\n",
            "        [ 0.8936, -1.0406],\n",
            "        [-1.1057,  1.3819],\n",
            "        [-1.2532,  1.4218],\n",
            "        [-0.0357,  0.1950],\n",
            "        [-0.5199,  0.6628],\n",
            "        [ 0.7739, -0.8234],\n",
            "        [-0.0893,  0.0584],\n",
            "        [ 0.3402, -0.3848],\n",
            "        [ 0.0104, -0.1408],\n",
            "        [ 0.7239, -0.8129],\n",
            "        [-0.3207,  0.6535],\n",
            "        [-0.0615,  0.0641],\n",
            "        [ 0.7548, -0.8540],\n",
            "        [-1.3391,  1.4347],\n",
            "        [-1.1404,  1.3851]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2413, device='cuda:0'), logits=tensor([[ 0.5881, -0.8869],\n",
            "        [ 0.2571, -0.4888],\n",
            "        [-1.2069,  1.5028],\n",
            "        [-1.3275,  1.4392],\n",
            "        [ 0.8348, -0.9864],\n",
            "        [ 0.0070,  0.0805],\n",
            "        [ 0.4823, -0.5934],\n",
            "        [-1.2524,  1.4479],\n",
            "        [-1.2729,  1.3698],\n",
            "        [-0.6831,  0.8568],\n",
            "        [ 0.3521, -0.3403],\n",
            "        [-1.1036,  1.3884],\n",
            "        [-1.3066,  1.3250],\n",
            "        [ 0.8626, -1.0289],\n",
            "        [-0.6818,  0.8936],\n",
            "        [-1.1355,  1.3934],\n",
            "        [ 0.5763, -0.5133],\n",
            "        [-0.9503,  1.1332],\n",
            "        [ 0.7570, -0.9102],\n",
            "        [ 0.8454, -0.8939],\n",
            "        [-0.0393,  0.2274],\n",
            "        [-0.6947,  0.8595],\n",
            "        [-0.6011,  0.7239],\n",
            "        [ 0.7697, -0.8751],\n",
            "        [ 0.8399, -0.9970],\n",
            "        [ 0.8283, -0.8573],\n",
            "        [ 0.8239, -1.0681],\n",
            "        [ 0.2549, -0.4179],\n",
            "        [ 0.6357, -0.9087],\n",
            "        [ 0.7108, -0.9626],\n",
            "        [ 0.7699, -0.9704],\n",
            "        [ 0.6617, -0.7723]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3439, device='cuda:0'), logits=tensor([[-1.2841,  1.4387],\n",
            "        [-0.9132,  1.1053],\n",
            "        [ 0.1059, -0.1168],\n",
            "        [ 0.2326, -0.1717],\n",
            "        [ 0.5503, -0.7568],\n",
            "        [ 0.8080, -0.8878],\n",
            "        [-1.3137,  1.4712],\n",
            "        [-1.0819,  1.3654],\n",
            "        [-1.1707,  1.4359],\n",
            "        [ 0.8304, -0.9852],\n",
            "        [-1.2653,  1.3915],\n",
            "        [ 0.8327, -1.0366],\n",
            "        [-1.0829,  1.1489],\n",
            "        [-1.1146,  1.3561],\n",
            "        [-0.7930,  1.0612],\n",
            "        [ 0.7142, -0.8263],\n",
            "        [-0.1407,  0.4427],\n",
            "        [-0.0540, -0.0743],\n",
            "        [-1.1002,  1.2952],\n",
            "        [ 0.1335, -0.4077],\n",
            "        [ 0.8321, -1.0104],\n",
            "        [ 0.0495, -0.2436],\n",
            "        [-0.8038,  0.9753],\n",
            "        [-1.2187,  1.3426],\n",
            "        [ 0.6810, -0.8947],\n",
            "        [-1.3228,  1.3999],\n",
            "        [ 0.9200, -1.0803],\n",
            "        [ 0.5200, -0.4086],\n",
            "        [-0.8606,  1.1962],\n",
            "        [ 0.8124, -1.0166],\n",
            "        [ 0.7595, -0.7597],\n",
            "        [ 0.5066, -0.7705]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4684, device='cuda:0'), logits=tensor([[ 0.5965, -0.8274],\n",
            "        [-1.1289,  1.4354],\n",
            "        [ 0.8331, -0.8055],\n",
            "        [-0.7047,  0.9844],\n",
            "        [ 0.8039, -1.0226],\n",
            "        [-1.1526,  1.3048],\n",
            "        [-1.3481,  1.4001],\n",
            "        [-1.3538,  1.4011],\n",
            "        [ 0.2729, -0.5574],\n",
            "        [-0.6873,  0.7982],\n",
            "        [ 0.9312, -1.1305],\n",
            "        [ 0.5675, -0.4432],\n",
            "        [-1.1447,  1.3003],\n",
            "        [ 0.0619, -0.3699],\n",
            "        [ 0.7240, -1.0231],\n",
            "        [ 0.6421, -0.8856],\n",
            "        [-1.0956,  1.3204],\n",
            "        [-1.1575,  1.2893],\n",
            "        [ 0.6440, -0.9357],\n",
            "        [-1.2522,  1.4539],\n",
            "        [-0.1483,  0.0481],\n",
            "        [ 0.2946, -0.5917],\n",
            "        [ 0.8493, -1.0225],\n",
            "        [ 0.0022, -0.2441],\n",
            "        [-0.9076,  1.2096],\n",
            "        [-1.1317,  1.3427],\n",
            "        [ 0.8543, -0.9383],\n",
            "        [ 0.5255, -0.5762],\n",
            "        [ 0.3392, -0.0860],\n",
            "        [ 0.7704, -0.9317],\n",
            "        [ 0.6979, -0.8448],\n",
            "        [ 0.5230, -0.4709]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4861, device='cuda:0'), logits=tensor([[ 0.5102, -0.7326],\n",
            "        [ 0.7846, -1.0229],\n",
            "        [-1.1475,  1.3462],\n",
            "        [-1.2102,  1.4506],\n",
            "        [ 0.7170, -0.9085],\n",
            "        [ 0.7523, -0.8883],\n",
            "        [ 0.5820, -0.7102],\n",
            "        [ 0.3713, -0.6615],\n",
            "        [ 0.2608, -0.4175],\n",
            "        [-0.2276,  0.4869],\n",
            "        [ 0.5013, -0.6233],\n",
            "        [ 0.6770, -0.4798],\n",
            "        [ 0.7836, -1.0257],\n",
            "        [-1.3278,  1.4039],\n",
            "        [-1.1794,  1.4460],\n",
            "        [ 0.8828, -0.9844],\n",
            "        [-0.3114,  0.6714],\n",
            "        [ 0.4669, -0.7154],\n",
            "        [-0.0387, -0.3959],\n",
            "        [ 0.6830, -0.8737],\n",
            "        [-1.2825,  1.4104],\n",
            "        [-1.0043,  1.1849],\n",
            "        [-0.5144,  0.4897],\n",
            "        [ 0.8142, -0.9934],\n",
            "        [-1.1750,  1.3810],\n",
            "        [ 0.8353, -0.9224],\n",
            "        [ 0.8870, -1.0603],\n",
            "        [ 0.7579, -1.0811],\n",
            "        [-1.1336,  1.3609],\n",
            "        [-1.0787,  1.2824],\n",
            "        [-0.6530,  0.6717],\n",
            "        [ 0.8058, -1.0446]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3751, device='cuda:0'), logits=tensor([[-0.5110,  0.5351],\n",
            "        [ 0.7588, -0.8697],\n",
            "        [ 0.5598, -0.7070],\n",
            "        [ 0.7710, -1.0379],\n",
            "        [-0.9677,  1.1513],\n",
            "        [ 0.4706, -0.4646],\n",
            "        [-0.1251,  0.3176],\n",
            "        [-0.8396,  1.0926],\n",
            "        [-1.2839,  1.4394],\n",
            "        [-1.2158,  1.3561],\n",
            "        [ 0.8614, -0.9904],\n",
            "        [ 0.4244, -0.7633],\n",
            "        [-0.7250,  0.6413],\n",
            "        [ 0.6858, -0.9462],\n",
            "        [-0.2065, -0.0693],\n",
            "        [ 0.7211, -0.8801],\n",
            "        [ 0.4969, -0.6488],\n",
            "        [ 0.3451, -0.2821],\n",
            "        [ 0.1060, -0.0680],\n",
            "        [ 0.5690, -0.6184],\n",
            "        [-0.8751,  0.8213],\n",
            "        [-1.0259,  1.3886],\n",
            "        [-1.2975,  1.4024],\n",
            "        [-0.7368,  0.6779],\n",
            "        [ 0.3528, -0.5207],\n",
            "        [-0.3090,  0.4575],\n",
            "        [-0.9413,  1.1168],\n",
            "        [ 0.7848, -1.0129],\n",
            "        [-0.6494,  1.0057],\n",
            "        [ 0.6421, -0.9706],\n",
            "        [ 0.3499, -0.5033],\n",
            "        [ 0.5350, -0.6854]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4162, device='cuda:0'), logits=tensor([[ 0.0754, -0.0345],\n",
            "        [-1.1569,  1.3960],\n",
            "        [-1.2792,  1.4642],\n",
            "        [ 0.7817, -0.9722],\n",
            "        [-0.5221,  0.9599],\n",
            "        [-1.2960,  1.4610],\n",
            "        [-1.0052,  1.0830],\n",
            "        [-1.3297,  1.4777],\n",
            "        [-0.4602,  0.6333],\n",
            "        [-1.2555,  1.4078],\n",
            "        [-0.4433,  0.3360],\n",
            "        [ 0.0434, -0.0156],\n",
            "        [-0.0146, -0.2670],\n",
            "        [-1.2363,  1.3950],\n",
            "        [ 0.4591, -0.0053],\n",
            "        [ 0.5624, -0.4499],\n",
            "        [-1.2197,  1.2915],\n",
            "        [ 0.7884, -0.8883],\n",
            "        [-0.6104,  0.6302],\n",
            "        [-1.2470,  1.2310],\n",
            "        [-0.6564,  0.7117],\n",
            "        [ 0.8022, -0.9525],\n",
            "        [-0.5581,  0.6525],\n",
            "        [-1.2231,  1.2545],\n",
            "        [-0.8134,  0.9350],\n",
            "        [-1.2325,  1.4625],\n",
            "        [ 0.6022, -0.9536],\n",
            "        [ 0.7093, -0.8506],\n",
            "        [-1.1784,  1.4138],\n",
            "        [ 0.2624, -0.2093],\n",
            "        [ 0.6715, -0.6774],\n",
            "        [ 0.5554, -0.6915]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5092, device='cuda:0'), logits=tensor([[-1.1771,  1.3934],\n",
            "        [ 0.1195,  0.0532],\n",
            "        [ 0.7258, -0.8547],\n",
            "        [ 0.7421, -0.8047],\n",
            "        [-1.1308,  1.3934],\n",
            "        [-0.3643,  0.0575],\n",
            "        [-1.2587,  1.4982],\n",
            "        [ 0.2549, -0.5695],\n",
            "        [ 0.8186, -1.0419],\n",
            "        [ 0.8257, -0.9561],\n",
            "        [-0.6684,  0.8159],\n",
            "        [-1.2531,  1.4426],\n",
            "        [ 0.6606, -0.8399],\n",
            "        [ 0.5232, -0.6286],\n",
            "        [-0.4517,  0.3850],\n",
            "        [ 0.7303, -0.9150],\n",
            "        [ 0.6770, -1.0051],\n",
            "        [-0.8448,  1.0274],\n",
            "        [-1.0958,  1.4737],\n",
            "        [ 0.1613, -0.2876],\n",
            "        [-1.3046,  1.4375],\n",
            "        [ 0.6970, -0.7139],\n",
            "        [-1.1746,  1.4115],\n",
            "        [ 0.1733, -0.1145],\n",
            "        [-0.7654,  1.0592],\n",
            "        [ 0.8559, -1.0744],\n",
            "        [ 0.7257, -0.5108],\n",
            "        [-1.1802,  1.4773],\n",
            "        [-0.4772,  0.7991],\n",
            "        [ 0.4829, -0.6383],\n",
            "        [ 0.6618, -0.7686],\n",
            "        [-0.3698,  0.6057]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4792, device='cuda:0'), logits=tensor([[ 0.5533, -0.7314],\n",
            "        [-1.1946,  1.3319],\n",
            "        [-0.8866,  0.7518],\n",
            "        [ 0.5514, -0.7015],\n",
            "        [-1.1351,  1.3334],\n",
            "        [-0.4890,  0.6868],\n",
            "        [-0.6728,  0.7988],\n",
            "        [ 0.8433, -1.0307],\n",
            "        [ 0.2878, -0.5245],\n",
            "        [-1.2299,  1.3863],\n",
            "        [-0.2297,  0.1464],\n",
            "        [-1.1901,  1.3774],\n",
            "        [-1.1544,  1.3773],\n",
            "        [ 0.7794, -0.9725],\n",
            "        [-0.0591,  0.0579],\n",
            "        [-1.1042,  1.3153],\n",
            "        [ 0.8655, -1.0054],\n",
            "        [ 0.6015, -0.7042],\n",
            "        [ 0.8259, -1.0835],\n",
            "        [ 0.6010, -0.6870],\n",
            "        [-1.2180,  1.3942],\n",
            "        [ 0.8905, -1.0732],\n",
            "        [ 0.7556, -1.0690],\n",
            "        [ 0.7787, -0.9242],\n",
            "        [-1.2884,  1.3998],\n",
            "        [-0.1876,  0.3410],\n",
            "        [ 0.9225, -1.0386],\n",
            "        [ 0.8142, -0.9562],\n",
            "        [ 0.7613, -0.9993],\n",
            "        [-0.3664,  0.4513],\n",
            "        [ 0.8513, -0.9159],\n",
            "        [-1.2324,  1.4645]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4572, device='cuda:0'), logits=tensor([[-0.5459,  0.6077],\n",
            "        [ 0.0355, -0.1655],\n",
            "        [-0.9811,  1.2323],\n",
            "        [ 0.7930, -1.0149],\n",
            "        [-0.1776,  0.2675],\n",
            "        [ 0.7464, -0.8973],\n",
            "        [ 0.8846, -0.9938],\n",
            "        [ 0.5147, -0.7431],\n",
            "        [ 0.7668, -0.9500],\n",
            "        [-1.2241,  1.4277],\n",
            "        [ 0.4816, -0.7706],\n",
            "        [-1.0242,  1.3440],\n",
            "        [ 0.6527, -0.8066],\n",
            "        [ 0.6485, -1.0493],\n",
            "        [ 0.7848, -0.9392],\n",
            "        [-1.0048,  1.1562],\n",
            "        [ 0.8337, -0.8698],\n",
            "        [ 0.8584, -0.9603],\n",
            "        [ 0.8306, -0.9765],\n",
            "        [ 0.7799, -1.0974],\n",
            "        [ 0.8708, -1.1300],\n",
            "        [-0.9347,  1.1072],\n",
            "        [ 0.6912, -0.9073],\n",
            "        [ 0.5012, -0.7534],\n",
            "        [-0.8817,  1.0274],\n",
            "        [ 0.7825, -0.8993],\n",
            "        [ 0.6197, -0.8988],\n",
            "        [-1.2511,  1.3802],\n",
            "        [ 0.9024, -1.0133],\n",
            "        [ 0.5397, -0.8132],\n",
            "        [ 0.4306, -0.7151],\n",
            "        [-0.1858,  0.4308]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4606, device='cuda:0'), logits=tensor([[-1.2152,  1.4261],\n",
            "        [-0.0769,  0.0394],\n",
            "        [ 0.2825, -0.5057],\n",
            "        [-1.2817,  1.4268],\n",
            "        [ 0.6527, -0.8794],\n",
            "        [ 0.5616, -0.8280],\n",
            "        [-1.1155,  1.2285],\n",
            "        [-1.2117,  1.4377],\n",
            "        [ 0.7776, -1.0588],\n",
            "        [-1.2184,  1.4180],\n",
            "        [-0.1201, -0.0387],\n",
            "        [ 0.2198, -0.5466],\n",
            "        [ 0.8050, -0.9222],\n",
            "        [ 0.7009, -0.9798],\n",
            "        [ 0.4576, -0.3036],\n",
            "        [-1.1598,  1.3859],\n",
            "        [ 0.2257, -0.6063],\n",
            "        [-0.9997,  1.3589],\n",
            "        [-0.5903,  0.7872],\n",
            "        [-0.0827,  0.0504],\n",
            "        [-0.8042,  0.8920],\n",
            "        [ 0.8774, -0.9869],\n",
            "        [-1.1931,  1.3240],\n",
            "        [ 0.2631, -0.6440],\n",
            "        [ 0.5200, -0.6807],\n",
            "        [ 0.6733, -0.8638],\n",
            "        [-1.1675,  1.3840],\n",
            "        [ 0.7248, -0.8419],\n",
            "        [ 0.8837, -0.9992],\n",
            "        [ 0.6914, -0.7263],\n",
            "        [ 0.0056,  0.1555],\n",
            "        [ 0.7236, -0.8784]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4324, device='cuda:0'), logits=tensor([[-1.0961,  1.3208],\n",
            "        [-0.8741,  0.9921],\n",
            "        [ 0.7939, -1.0576],\n",
            "        [ 0.6712, -0.5459],\n",
            "        [ 0.3453, -0.2625],\n",
            "        [ 0.5012, -0.2896],\n",
            "        [-0.7070,  0.8892],\n",
            "        [-0.9835,  1.3729],\n",
            "        [-0.6458,  0.7087],\n",
            "        [ 0.5751, -0.5645],\n",
            "        [ 0.2089, -0.1667],\n",
            "        [-1.1586,  1.4065],\n",
            "        [ 0.3596, -0.2110],\n",
            "        [ 0.8853, -1.0042],\n",
            "        [ 0.3904, -0.4910],\n",
            "        [-1.1343,  1.4347],\n",
            "        [-0.1967,  0.4249],\n",
            "        [ 0.7044, -0.5764],\n",
            "        [-1.2676,  1.3925],\n",
            "        [ 0.7136, -0.8818],\n",
            "        [-1.1829,  1.4888],\n",
            "        [-1.2291,  1.2499],\n",
            "        [ 0.7736, -0.9370],\n",
            "        [ 0.7770, -0.8711],\n",
            "        [ 0.7825, -1.1127],\n",
            "        [-0.8651,  1.1426],\n",
            "        [-1.1135,  1.3286],\n",
            "        [-0.6884,  1.0195],\n",
            "        [ 0.7072, -0.9476],\n",
            "        [ 0.7755, -1.0000],\n",
            "        [-1.2164,  1.2831],\n",
            "        [ 0.7146, -0.9130]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5777, device='cuda:0'), logits=tensor([[ 0.6339, -0.9399],\n",
            "        [-1.0694,  1.2835],\n",
            "        [-1.3392,  1.4004],\n",
            "        [ 0.7077, -0.8917],\n",
            "        [ 0.6100, -0.5982],\n",
            "        [-0.5539,  0.7051],\n",
            "        [-0.2332,  0.2074],\n",
            "        [ 0.7853, -1.1294],\n",
            "        [-0.6330,  0.7686],\n",
            "        [-0.7756,  1.0088],\n",
            "        [ 0.5713, -0.9026],\n",
            "        [ 0.4581, -0.4292],\n",
            "        [ 0.7844, -0.9815],\n",
            "        [ 0.6099, -0.8408],\n",
            "        [-1.1986,  1.4624],\n",
            "        [-1.1911,  1.4323],\n",
            "        [ 0.6405, -0.5382],\n",
            "        [-0.3389,  0.2935],\n",
            "        [-1.2901,  1.4211],\n",
            "        [ 0.7453, -0.7880],\n",
            "        [ 0.6523, -0.3824],\n",
            "        [ 0.9043, -1.0182],\n",
            "        [ 0.8059, -1.0409],\n",
            "        [-0.8854,  1.1281],\n",
            "        [-0.9174,  1.1268],\n",
            "        [ 0.3064, -0.5032],\n",
            "        [-0.1173,  0.1340],\n",
            "        [ 0.1942,  0.0294],\n",
            "        [ 0.5074, -0.5851],\n",
            "        [-0.2465,  0.0644],\n",
            "        [ 0.4386, -0.6096],\n",
            "        [ 0.9345, -1.0750]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4395, device='cuda:0'), logits=tensor([[-0.8004,  1.0636],\n",
            "        [-1.2675,  1.3917],\n",
            "        [-1.0891,  1.3695],\n",
            "        [ 0.4016, -0.3589],\n",
            "        [-1.1852,  1.4032],\n",
            "        [-1.1999,  1.2685],\n",
            "        [-1.2399,  1.4278],\n",
            "        [ 0.8824, -1.0250],\n",
            "        [ 0.4344, -0.3579],\n",
            "        [ 0.7046, -0.9392],\n",
            "        [-1.1841,  1.4586],\n",
            "        [-0.6329,  0.7507],\n",
            "        [-0.9757,  0.9044],\n",
            "        [ 0.3620, -0.2829],\n",
            "        [ 0.7741, -0.7379],\n",
            "        [ 0.7877, -0.9623],\n",
            "        [-0.2834,  0.3313],\n",
            "        [-1.1986,  1.2799],\n",
            "        [ 0.7447, -0.9503],\n",
            "        [-0.6444,  0.8267],\n",
            "        [ 0.5270, -0.8034],\n",
            "        [ 0.8230, -1.0705],\n",
            "        [-1.1468,  1.1680],\n",
            "        [ 0.4630, -0.3815],\n",
            "        [ 0.8105, -0.9207],\n",
            "        [ 0.6488, -0.5867],\n",
            "        [-0.7973,  1.2381],\n",
            "        [ 0.6651, -0.7224],\n",
            "        [ 0.7097, -0.8170],\n",
            "        [-0.1896,  0.1782],\n",
            "        [-1.0310,  1.3235],\n",
            "        [ 0.6931, -1.0316]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4877, device='cuda:0'), logits=tensor([[-1.1279,  1.2241],\n",
            "        [ 0.8015, -1.0757],\n",
            "        [-1.2304,  1.3691],\n",
            "        [ 0.8081, -0.9639],\n",
            "        [-0.2356,  0.2177],\n",
            "        [-1.2680,  1.3653],\n",
            "        [ 0.8318, -1.0834],\n",
            "        [ 0.2028, -0.4563],\n",
            "        [ 0.8887, -1.0273],\n",
            "        [-1.2231,  1.3940],\n",
            "        [ 0.7051, -0.7470],\n",
            "        [-0.1517,  0.0643],\n",
            "        [ 0.9160, -1.0274],\n",
            "        [ 0.5096, -0.7767],\n",
            "        [-1.2565,  1.4388],\n",
            "        [ 0.7952, -0.9519],\n",
            "        [ 0.4944, -0.7608],\n",
            "        [-1.3123,  1.4079],\n",
            "        [-1.1908,  1.2057],\n",
            "        [ 0.8120, -1.0357],\n",
            "        [ 0.4654, -0.4666],\n",
            "        [ 0.0445,  0.0355],\n",
            "        [ 0.5834, -0.7417],\n",
            "        [-1.0823,  1.3791],\n",
            "        [ 0.6449, -0.8575],\n",
            "        [ 0.7853, -0.7778],\n",
            "        [ 0.6003, -0.6021],\n",
            "        [ 0.6215, -0.4620],\n",
            "        [ 0.7674, -0.9851],\n",
            "        [ 0.7439, -0.9282],\n",
            "        [ 0.8090, -1.0369],\n",
            "        [-1.1410,  1.4732]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3997, device='cuda:0'), logits=tensor([[ 0.6995, -0.7688],\n",
            "        [-0.2875,  0.3069],\n",
            "        [ 0.7470, -0.9129],\n",
            "        [ 0.7572, -1.0497],\n",
            "        [-0.9903,  1.0265],\n",
            "        [-1.1293,  1.4579],\n",
            "        [ 0.1454, -0.5027],\n",
            "        [ 0.0427, -0.1338],\n",
            "        [-0.6107,  1.0529],\n",
            "        [-1.1214,  1.3454],\n",
            "        [ 0.6764, -0.7635],\n",
            "        [ 0.6163, -0.7351],\n",
            "        [-1.0649,  1.3697],\n",
            "        [ 0.7047, -0.8614],\n",
            "        [ 0.1308, -0.2327],\n",
            "        [ 0.5465, -0.7951],\n",
            "        [-0.7552,  0.8184],\n",
            "        [-1.2146,  1.3467],\n",
            "        [ 0.7947, -1.0671],\n",
            "        [-1.2166,  1.3635],\n",
            "        [-1.2585,  1.3206],\n",
            "        [ 0.6706, -0.7543],\n",
            "        [-0.9313,  1.0839],\n",
            "        [-1.1242,  1.4870],\n",
            "        [-0.1259,  0.0450],\n",
            "        [ 0.5464, -0.7911],\n",
            "        [ 0.1711, -0.1095],\n",
            "        [ 0.7255, -0.9220],\n",
            "        [-1.3236,  1.3938],\n",
            "        [ 0.4742, -0.6762],\n",
            "        [-0.0251,  0.0552],\n",
            "        [-0.6449,  0.6868]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3951, device='cuda:0'), logits=tensor([[-1.1978,  1.4441],\n",
            "        [ 0.2123, -0.5732],\n",
            "        [-0.1558,  0.3812],\n",
            "        [ 0.8679, -1.0352],\n",
            "        [-1.2327,  1.4844],\n",
            "        [-0.6094,  0.6403],\n",
            "        [-1.2580,  1.4998],\n",
            "        [ 0.8610, -0.9730],\n",
            "        [ 0.9207, -0.9959],\n",
            "        [ 0.1899, -0.4166],\n",
            "        [ 0.3872, -0.3561],\n",
            "        [ 0.6007, -0.8496],\n",
            "        [-1.2320,  1.4811],\n",
            "        [-0.1050,  0.0735],\n",
            "        [ 0.5397, -0.6116],\n",
            "        [ 0.2206, -0.2390],\n",
            "        [ 0.8315, -0.8744],\n",
            "        [-1.1790,  1.3095],\n",
            "        [-0.1140,  0.2013],\n",
            "        [-1.2117,  1.3464],\n",
            "        [-1.0331,  1.2651],\n",
            "        [ 0.8237, -0.9967],\n",
            "        [ 0.7613, -0.9932],\n",
            "        [ 0.6441, -0.6576],\n",
            "        [-0.1544,  0.2548],\n",
            "        [ 0.4521, -0.6791],\n",
            "        [ 0.6441, -0.6774],\n",
            "        [-0.6739,  0.8685],\n",
            "        [-0.3032,  0.4112],\n",
            "        [-0.2942,  0.4202],\n",
            "        [-1.1448,  1.3987],\n",
            "        [-1.1488,  1.4170]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3573, device='cuda:0'), logits=tensor([[-1.2328,  1.2720],\n",
            "        [-0.1278,  0.2713],\n",
            "        [ 0.8438, -0.9872],\n",
            "        [-1.2412,  1.3596],\n",
            "        [-0.6993,  0.8056],\n",
            "        [ 0.9147, -0.9964],\n",
            "        [-0.9376,  0.9875],\n",
            "        [ 0.8710, -1.0324],\n",
            "        [ 0.6965, -0.8524],\n",
            "        [ 0.1665, -0.3011],\n",
            "        [ 0.7473, -0.8786],\n",
            "        [ 0.3852, -0.2920],\n",
            "        [ 0.2754, -0.5889],\n",
            "        [-1.2127,  1.3769],\n",
            "        [ 0.7854, -1.0238],\n",
            "        [ 0.8597, -0.9552],\n",
            "        [ 0.7051, -0.9544],\n",
            "        [ 0.1335, -0.3770],\n",
            "        [ 0.9374, -1.0992],\n",
            "        [-1.0205,  1.2075],\n",
            "        [-0.5186,  0.4881],\n",
            "        [-1.2373,  1.4059],\n",
            "        [-1.3100,  1.4019],\n",
            "        [-0.7672,  0.5529],\n",
            "        [ 0.4077, -0.2783],\n",
            "        [-1.1661,  1.3685],\n",
            "        [ 0.0032, -0.1192],\n",
            "        [ 0.8032, -0.9968],\n",
            "        [ 0.2629, -0.3222],\n",
            "        [ 0.2858, -0.1650],\n",
            "        [ 0.7395, -0.9881],\n",
            "        [ 0.3712, -0.4236]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4012, device='cuda:0'), logits=tensor([[-1.2807,  1.4952],\n",
            "        [ 0.6100, -0.5849],\n",
            "        [ 0.6623, -0.8523],\n",
            "        [ 0.7268, -0.7236],\n",
            "        [ 0.8575, -0.9573],\n",
            "        [-1.1631,  1.2656],\n",
            "        [ 0.8291, -0.9380],\n",
            "        [ 0.7862, -0.9682],\n",
            "        [-1.0783,  1.3496],\n",
            "        [ 0.8198, -1.0626],\n",
            "        [-0.7923,  0.9076],\n",
            "        [-0.0120, -0.0700],\n",
            "        [-1.0342,  1.2620],\n",
            "        [-0.3199,  0.6990],\n",
            "        [-0.9608,  1.2622],\n",
            "        [-1.2585,  1.4664],\n",
            "        [-1.2556,  1.3623],\n",
            "        [-0.5164,  0.5279],\n",
            "        [ 0.8127, -0.9834],\n",
            "        [ 0.5372, -0.8565],\n",
            "        [ 0.4810, -0.8109],\n",
            "        [-1.0876,  1.3488],\n",
            "        [ 0.7880, -0.9815],\n",
            "        [-1.2049,  1.2038],\n",
            "        [-1.0743,  1.3392],\n",
            "        [-1.3049,  1.5188],\n",
            "        [-1.0605,  1.2239],\n",
            "        [-0.7572,  0.9368],\n",
            "        [-1.2482,  1.4110],\n",
            "        [ 0.2279, -0.5614],\n",
            "        [-1.0980,  1.2852],\n",
            "        [ 0.4710, -0.7255]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.7006, device='cuda:0'), logits=tensor([[ 0.8498, -1.0708],\n",
            "        [-0.8021,  0.7576],\n",
            "        [ 0.6719, -0.9954],\n",
            "        [-0.3788,  0.3573],\n",
            "        [-1.2377,  1.4992],\n",
            "        [-0.7451,  1.0559],\n",
            "        [ 0.7148, -0.9080],\n",
            "        [-0.4303,  0.4468],\n",
            "        [-1.3248,  1.3267],\n",
            "        [-0.4718,  0.3787],\n",
            "        [ 0.2445, -0.6288],\n",
            "        [ 0.4576, -0.5844],\n",
            "        [-0.8553,  1.0571],\n",
            "        [-1.1363,  1.1626],\n",
            "        [ 0.9542, -1.0992],\n",
            "        [-1.1739,  1.3650],\n",
            "        [ 0.7708, -0.8896],\n",
            "        [ 0.7519, -1.0975],\n",
            "        [-1.2834,  1.3549],\n",
            "        [-1.2911,  1.4107],\n",
            "        [-0.9192,  1.0274],\n",
            "        [ 0.8249, -0.9373],\n",
            "        [ 0.4088, -0.5544],\n",
            "        [-1.0497,  1.2382],\n",
            "        [ 0.6025, -0.9228],\n",
            "        [-0.9648,  1.1060],\n",
            "        [-0.5914,  1.0133],\n",
            "        [ 0.8206, -0.9629],\n",
            "        [-0.0621, -0.1341],\n",
            "        [ 0.8798, -1.0008],\n",
            "        [ 0.6946, -0.9391],\n",
            "        [-0.1362,  0.2350]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3324, device='cuda:0'), logits=tensor([[-0.4876,  0.7126],\n",
            "        [ 0.7057, -0.8519],\n",
            "        [-1.1813,  1.3501],\n",
            "        [ 0.6350, -0.5125],\n",
            "        [-1.2195,  1.3396],\n",
            "        [ 0.7285, -0.8789],\n",
            "        [ 0.7133, -0.9014],\n",
            "        [ 0.6887, -0.7947],\n",
            "        [ 0.4519, -0.6856],\n",
            "        [-1.3032,  1.4704],\n",
            "        [ 0.8600, -1.0835],\n",
            "        [-0.9460,  1.2659],\n",
            "        [ 0.2773, -0.1991],\n",
            "        [-1.2867,  1.4317],\n",
            "        [-0.1243,  0.1468],\n",
            "        [ 0.7677, -0.9290],\n",
            "        [ 0.6300, -0.4913],\n",
            "        [-1.3194,  1.4553],\n",
            "        [-0.7671,  0.7188],\n",
            "        [-0.7900,  0.8334],\n",
            "        [ 0.8883, -0.9580],\n",
            "        [ 0.3487, -0.2770],\n",
            "        [ 0.7067, -0.9827],\n",
            "        [ 0.5353, -0.7704],\n",
            "        [ 0.2598, -0.0160],\n",
            "        [-0.9031,  0.8187],\n",
            "        [-0.2685,  0.2347],\n",
            "        [-1.3403,  1.3800],\n",
            "        [-1.1759,  1.3057],\n",
            "        [ 0.4598, -0.4271],\n",
            "        [ 0.5911, -0.7486],\n",
            "        [-1.2630,  1.5081]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3630, device='cuda:0'), logits=tensor([[-0.6160,  0.5661],\n",
            "        [ 0.7456, -0.9765],\n",
            "        [ 0.8623, -1.1334],\n",
            "        [-1.1852,  1.3193],\n",
            "        [ 0.7819, -1.0796],\n",
            "        [-0.1122,  0.0362],\n",
            "        [ 0.2459, -0.4591],\n",
            "        [-0.6233,  0.8966],\n",
            "        [-1.1841,  1.4333],\n",
            "        [-1.2698,  1.3757],\n",
            "        [ 0.1080, -0.1812],\n",
            "        [-0.6465,  0.8388],\n",
            "        [ 0.0680,  0.1806],\n",
            "        [ 0.7681, -0.9100],\n",
            "        [-0.6832,  0.8001],\n",
            "        [ 0.5120, -0.8195],\n",
            "        [-1.1651,  1.3006],\n",
            "        [ 0.8084, -1.0302],\n",
            "        [ 0.7946, -0.9632],\n",
            "        [-1.1494,  1.3484],\n",
            "        [-1.0750,  1.2491],\n",
            "        [ 0.7354, -0.9503],\n",
            "        [-1.1652,  1.3839],\n",
            "        [ 0.8237, -0.9796],\n",
            "        [-1.1619,  1.4247],\n",
            "        [ 0.1091, -0.0775],\n",
            "        [ 0.7946, -0.9144],\n",
            "        [ 0.8816, -1.0389],\n",
            "        [-0.0103, -0.1458],\n",
            "        [ 0.7495, -0.8001],\n",
            "        [ 0.2568, -0.4998],\n",
            "        [-1.1321,  1.4159]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3394, device='cuda:0'), logits=tensor([[ 0.6547, -0.8524],\n",
            "        [ 0.7929, -1.1003],\n",
            "        [ 0.6619, -0.8810],\n",
            "        [ 0.1637, -0.1817],\n",
            "        [ 0.7428, -0.9731],\n",
            "        [ 0.7141, -0.7995],\n",
            "        [ 0.9260, -1.1247],\n",
            "        [ 0.7317, -0.9051],\n",
            "        [ 0.7614, -0.9815],\n",
            "        [ 0.7949, -1.0171],\n",
            "        [ 0.5239, -0.4872],\n",
            "        [ 0.2943, -0.2919],\n",
            "        [ 0.9458, -1.0706],\n",
            "        [-1.2502,  1.4529],\n",
            "        [-1.0055,  1.2425],\n",
            "        [ 0.6141, -1.0104],\n",
            "        [ 0.5151, -0.7602],\n",
            "        [-1.2169,  1.4161],\n",
            "        [ 0.5110, -0.8416],\n",
            "        [-1.2956,  1.3345],\n",
            "        [ 0.8480, -0.9750],\n",
            "        [ 0.6518, -1.0221],\n",
            "        [-0.7810,  0.8074],\n",
            "        [-1.1292,  1.2997],\n",
            "        [-1.1296,  1.3461],\n",
            "        [-0.0344, -0.3510],\n",
            "        [ 0.6898, -0.9407],\n",
            "        [ 0.7133, -0.9128],\n",
            "        [-1.0506,  1.3959],\n",
            "        [ 0.2951, -0.5274],\n",
            "        [ 0.8211, -0.9474],\n",
            "        [-0.5850,  0.8322]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4411, device='cuda:0'), logits=tensor([[-0.9788,  1.3205],\n",
            "        [-1.0048,  1.1019],\n",
            "        [-0.2663,  0.2522],\n",
            "        [ 0.8442, -0.9000],\n",
            "        [ 0.8172, -1.0396],\n",
            "        [ 0.6928, -1.0114],\n",
            "        [ 0.8161, -1.0324],\n",
            "        [ 0.7489, -1.0111],\n",
            "        [ 0.6398, -0.7661],\n",
            "        [-0.8609,  0.8180],\n",
            "        [ 0.8706, -1.1022],\n",
            "        [ 0.8637, -1.0796],\n",
            "        [ 0.5256, -0.7692],\n",
            "        [-1.0230,  1.2297],\n",
            "        [-0.3021,  0.2999],\n",
            "        [ 0.6927, -0.9921],\n",
            "        [-0.8248,  1.0666],\n",
            "        [-1.2473,  1.4491],\n",
            "        [-1.2548,  1.4378],\n",
            "        [-0.0073,  0.2052],\n",
            "        [ 0.2166, -0.5237],\n",
            "        [-1.1807,  1.4394],\n",
            "        [ 0.7690, -0.9810],\n",
            "        [ 0.9207, -0.9560],\n",
            "        [ 0.8031, -0.9957],\n",
            "        [-1.2306,  1.5074],\n",
            "        [ 0.8443, -1.0449],\n",
            "        [ 0.2796, -0.4174],\n",
            "        [-1.3348,  1.5111],\n",
            "        [-0.4004,  0.6228],\n",
            "        [ 0.1930, -0.0920],\n",
            "        [-0.3392,  0.5805]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4770, device='cuda:0'), logits=tensor([[ 0.8933, -1.1061],\n",
            "        [ 0.8200, -0.9687],\n",
            "        [-1.2383,  1.3828],\n",
            "        [ 0.7061, -1.0033],\n",
            "        [ 0.8181, -0.9889],\n",
            "        [ 0.8688, -0.9198],\n",
            "        [ 0.6740, -0.9494],\n",
            "        [-0.5980,  0.8912],\n",
            "        [-1.0401,  1.2360],\n",
            "        [-0.2685,  0.1651],\n",
            "        [ 0.8176, -1.0916],\n",
            "        [ 0.7828, -1.0427],\n",
            "        [ 0.8178, -0.9348],\n",
            "        [-1.2401,  1.4245],\n",
            "        [-0.2681,  0.3196],\n",
            "        [-1.1588,  1.3489],\n",
            "        [-0.1148,  0.0659],\n",
            "        [ 0.9246, -0.9711],\n",
            "        [ 0.7919, -0.9496],\n",
            "        [ 0.5815, -0.9089],\n",
            "        [-1.0027,  1.2312],\n",
            "        [ 0.8367, -1.0874],\n",
            "        [ 0.8739, -1.0408],\n",
            "        [-1.1378,  1.3299],\n",
            "        [ 0.2826, -0.5931],\n",
            "        [-0.1726,  0.1416],\n",
            "        [ 0.8336, -1.0384],\n",
            "        [-1.2133,  1.3656],\n",
            "        [-1.2874,  1.5135],\n",
            "        [ 0.2178, -0.5349],\n",
            "        [ 0.3923, -0.5069],\n",
            "        [-0.9710,  1.3098]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4682, device='cuda:0'), logits=tensor([[-0.5009,  0.5986],\n",
            "        [ 0.7852, -0.9213],\n",
            "        [-1.1190,  1.4372],\n",
            "        [ 0.6751, -0.7056],\n",
            "        [ 0.6164, -0.8953],\n",
            "        [ 0.4221, -0.6251],\n",
            "        [-0.8993,  1.1504],\n",
            "        [ 0.8197, -0.9084],\n",
            "        [ 0.5343, -0.7271],\n",
            "        [ 0.8835, -1.0323],\n",
            "        [ 0.7366, -1.0211],\n",
            "        [-0.9836,  1.1482],\n",
            "        [-1.1815,  1.4440],\n",
            "        [-0.7414,  1.0534],\n",
            "        [-1.1390,  1.3999],\n",
            "        [ 0.7377, -0.7830],\n",
            "        [-1.1745,  1.4117],\n",
            "        [ 0.6395, -0.9018],\n",
            "        [ 0.3760, -0.7649],\n",
            "        [-1.0303,  1.3048],\n",
            "        [ 0.7610, -0.6719],\n",
            "        [ 0.9068, -0.9329],\n",
            "        [-1.2707,  1.2781],\n",
            "        [ 0.5895, -0.7138],\n",
            "        [-0.0080, -0.0103],\n",
            "        [ 0.8552, -0.7633],\n",
            "        [-1.1959,  1.3701],\n",
            "        [-0.4867,  0.6380],\n",
            "        [-1.0814,  1.0409],\n",
            "        [-1.1336,  1.4353],\n",
            "        [ 0.1161, -0.0982],\n",
            "        [-1.2402,  1.3974]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4003, device='cuda:0'), logits=tensor([[ 0.3719, -0.7137],\n",
            "        [-0.7347,  0.9796],\n",
            "        [ 0.8417, -1.0607],\n",
            "        [-1.1883,  1.4173],\n",
            "        [-0.4550,  0.5067],\n",
            "        [-1.3561,  1.4876],\n",
            "        [-0.5641,  0.8095],\n",
            "        [ 0.7618, -0.9510],\n",
            "        [-1.3725,  1.4658],\n",
            "        [ 0.3909, -0.5734],\n",
            "        [-0.8102,  1.0066],\n",
            "        [-0.2566,  0.2478],\n",
            "        [-1.0367,  1.2554],\n",
            "        [-0.0356, -0.0712],\n",
            "        [ 0.3414, -0.5645],\n",
            "        [-0.8165,  1.1315],\n",
            "        [-0.8043,  1.0595],\n",
            "        [ 0.8680, -0.9594],\n",
            "        [-0.4074,  0.4215],\n",
            "        [ 0.3471, -0.7355],\n",
            "        [ 0.6438, -0.8735],\n",
            "        [ 0.4856, -0.4019],\n",
            "        [ 0.7251, -0.8261],\n",
            "        [ 0.8515, -1.1099],\n",
            "        [-1.1473,  1.4138],\n",
            "        [ 0.7891, -0.8909],\n",
            "        [-0.5481,  0.6725],\n",
            "        [ 0.6910, -0.9212],\n",
            "        [ 0.3903, -0.0462],\n",
            "        [ 0.8321, -1.0184],\n",
            "        [ 0.8740, -1.0056],\n",
            "        [ 0.7582, -0.7828]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4767, device='cuda:0'), logits=tensor([[ 0.8647, -1.0760],\n",
            "        [ 0.8235, -1.1283],\n",
            "        [ 0.7680, -0.9676],\n",
            "        [ 0.0184, -0.3634],\n",
            "        [-0.2271,  0.4158],\n",
            "        [-0.2197,  0.2758],\n",
            "        [ 0.7903, -0.8529],\n",
            "        [ 0.3060, -0.3335],\n",
            "        [-1.0751,  1.2910],\n",
            "        [ 0.7863, -1.0642],\n",
            "        [ 0.4793, -0.7308],\n",
            "        [ 0.7776, -0.8851],\n",
            "        [-0.6303,  0.7623],\n",
            "        [ 0.6286, -0.4667],\n",
            "        [-0.8160,  0.9221],\n",
            "        [ 0.9408, -0.9973],\n",
            "        [ 0.6813, -0.9746],\n",
            "        [ 0.6712, -0.9635],\n",
            "        [ 0.7987, -0.9164],\n",
            "        [-0.3554,  0.5100],\n",
            "        [ 0.2929, -0.1660],\n",
            "        [ 0.8454, -1.0699],\n",
            "        [ 0.6302, -0.7281],\n",
            "        [-0.0299,  0.3377],\n",
            "        [ 0.9186, -1.0636],\n",
            "        [-1.0671,  1.1050],\n",
            "        [-1.2094,  1.3714],\n",
            "        [-1.2138,  1.3600],\n",
            "        [ 0.8893, -1.0194],\n",
            "        [ 0.6910, -0.9027],\n",
            "        [ 0.4015, -0.4887],\n",
            "        [ 0.8163, -0.9459]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3434, device='cuda:0'), logits=tensor([[ 0.6526, -0.8195],\n",
            "        [ 0.7547, -0.9855],\n",
            "        [ 0.8580, -1.0126],\n",
            "        [-1.3178,  1.4685],\n",
            "        [ 0.6894, -0.9767],\n",
            "        [-0.7027,  0.8307],\n",
            "        [ 0.7781, -0.8563],\n",
            "        [-1.3519,  1.4454],\n",
            "        [-0.6081,  0.6718],\n",
            "        [-0.6577,  0.8546],\n",
            "        [ 0.1198, -0.3977],\n",
            "        [-0.7867,  0.9155],\n",
            "        [ 0.5533, -0.3693],\n",
            "        [ 0.7190, -0.9174],\n",
            "        [ 0.8233, -0.9689],\n",
            "        [-0.1445, -0.2767],\n",
            "        [ 0.8969, -1.0043],\n",
            "        [ 0.5311, -0.6502],\n",
            "        [-0.3656,  0.5729],\n",
            "        [ 0.5108, -0.4374],\n",
            "        [ 0.3275, -0.7242],\n",
            "        [-0.0115, -0.0188],\n",
            "        [ 0.8308, -1.0705],\n",
            "        [-1.1781,  1.3160],\n",
            "        [-1.0174,  1.2884],\n",
            "        [-1.2026,  1.3726],\n",
            "        [ 0.7818, -0.9327],\n",
            "        [ 0.8416, -0.9685],\n",
            "        [-1.2298,  1.2758],\n",
            "        [ 0.4271, -0.6683],\n",
            "        [-0.4164,  0.4643],\n",
            "        [ 0.8828, -1.0187]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5990, device='cuda:0'), logits=tensor([[ 0.5789, -0.5576],\n",
            "        [-1.2364,  1.4145],\n",
            "        [ 0.7775, -0.8252],\n",
            "        [-1.0995,  1.4555],\n",
            "        [ 0.7149, -0.8595],\n",
            "        [ 0.7797, -1.0429],\n",
            "        [ 0.5068, -0.8648],\n",
            "        [ 0.4740, -0.2668],\n",
            "        [ 0.7694, -1.0467],\n",
            "        [-1.2256,  1.3436],\n",
            "        [ 0.4942, -0.7150],\n",
            "        [ 0.2981, -0.5501],\n",
            "        [-1.2992,  1.4567],\n",
            "        [ 0.9895, -1.0132],\n",
            "        [-0.4238,  0.2378],\n",
            "        [ 0.8545, -1.0554],\n",
            "        [ 0.4097, -0.7589],\n",
            "        [-0.8985,  1.0285],\n",
            "        [ 0.7629, -1.0233],\n",
            "        [-0.0701, -0.0441],\n",
            "        [-1.0698,  1.3186],\n",
            "        [ 0.8416, -0.9628],\n",
            "        [-1.2935,  1.3371],\n",
            "        [-1.1971,  1.3420],\n",
            "        [ 0.6172, -0.8286],\n",
            "        [-1.0048,  0.9383],\n",
            "        [-0.2722,  0.4911],\n",
            "        [-1.0343,  1.3349],\n",
            "        [ 0.1232,  0.0339],\n",
            "        [-0.7139,  1.0590],\n",
            "        [ 0.7624, -0.9800],\n",
            "        [ 0.6828, -0.7862]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2845, device='cuda:0'), logits=tensor([[ 0.8143, -0.9896],\n",
            "        [-0.9042,  1.2161],\n",
            "        [ 0.5277, -0.6772],\n",
            "        [-0.4542,  0.4560],\n",
            "        [-0.7216,  0.9131],\n",
            "        [ 0.0949, -0.0820],\n",
            "        [-0.0948,  0.1143],\n",
            "        [-1.2855,  1.3904],\n",
            "        [ 0.3682, -0.6059],\n",
            "        [ 0.7316, -0.8781],\n",
            "        [ 0.8620, -0.9766],\n",
            "        [ 0.6016, -0.6227],\n",
            "        [ 0.9064, -0.9950],\n",
            "        [ 0.2965, -0.4747],\n",
            "        [-1.2183,  1.3903],\n",
            "        [ 0.6171, -0.7160],\n",
            "        [ 0.2489, -0.5796],\n",
            "        [-1.3182,  1.4639],\n",
            "        [ 0.7915, -1.0041],\n",
            "        [ 0.6953, -0.9438],\n",
            "        [-0.6778,  0.8184],\n",
            "        [ 0.6306, -0.9059],\n",
            "        [ 0.4535, -0.6365],\n",
            "        [-1.1165,  1.4215],\n",
            "        [-1.0266,  1.1555],\n",
            "        [-1.1660,  1.3195],\n",
            "        [ 0.7720, -1.1028],\n",
            "        [ 0.5804, -0.8534],\n",
            "        [ 0.8085, -0.9936],\n",
            "        [-1.1002,  1.3924],\n",
            "        [-0.8893,  1.2274],\n",
            "        [ 0.5347, -0.8583]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3805, device='cuda:0'), logits=tensor([[ 0.9060, -1.1038],\n",
            "        [-0.4795,  0.5775],\n",
            "        [ 0.3008, -0.6352],\n",
            "        [ 0.6246, -0.9399],\n",
            "        [ 0.6039, -0.9563],\n",
            "        [-1.2321,  1.3741],\n",
            "        [-1.1967,  1.4664],\n",
            "        [ 0.5942, -0.7743],\n",
            "        [-1.1928,  1.2722],\n",
            "        [-0.3647,  0.4990],\n",
            "        [-1.2244,  1.4017],\n",
            "        [ 0.7405, -1.0619],\n",
            "        [-1.2224,  1.2787],\n",
            "        [ 0.6256, -0.7502],\n",
            "        [ 0.0874,  0.0733],\n",
            "        [ 0.8390, -0.9243],\n",
            "        [ 0.2852, -0.5028],\n",
            "        [-1.2638,  1.4457],\n",
            "        [-1.1512,  1.3477],\n",
            "        [ 0.1001, -0.0480],\n",
            "        [-0.9527,  1.1181],\n",
            "        [ 0.4042, -0.6040],\n",
            "        [-1.0344,  1.3753],\n",
            "        [ 0.5979, -0.7704],\n",
            "        [-0.9555,  1.3084],\n",
            "        [ 0.3846, -0.2735],\n",
            "        [-0.2172,  0.2102],\n",
            "        [ 0.8255, -1.0040],\n",
            "        [ 0.8054, -1.0415],\n",
            "        [ 0.6399, -0.8360],\n",
            "        [-0.3579,  0.3890],\n",
            "        [ 0.7894, -0.9058]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4514, device='cuda:0'), logits=tensor([[ 0.8693, -1.0588],\n",
            "        [-0.9728,  1.1193],\n",
            "        [ 0.3096, -0.5631],\n",
            "        [ 0.8757, -1.0195],\n",
            "        [ 0.8115, -0.9522],\n",
            "        [ 0.5928, -0.8477],\n",
            "        [ 0.3748, -0.6773],\n",
            "        [-1.3188,  1.4258],\n",
            "        [-1.1112,  1.3881],\n",
            "        [-1.1105,  1.3582],\n",
            "        [-1.2704,  1.4052],\n",
            "        [-0.7117,  0.8031],\n",
            "        [-1.1053,  1.2767],\n",
            "        [-0.8384,  1.0835],\n",
            "        [ 0.9090, -1.0073],\n",
            "        [-1.3110,  1.3928],\n",
            "        [ 0.7594, -0.8463],\n",
            "        [-0.3760,  0.6583],\n",
            "        [ 0.6159, -0.9056],\n",
            "        [-0.7317,  0.6097],\n",
            "        [-0.2688,  0.4819],\n",
            "        [-0.8307,  1.1090],\n",
            "        [ 0.2179, -0.6256],\n",
            "        [-0.2378,  0.3060],\n",
            "        [ 0.5308, -0.8602],\n",
            "        [-0.5231,  0.8423],\n",
            "        [ 0.8068, -0.8507],\n",
            "        [ 0.5875, -0.7856],\n",
            "        [-1.2566,  1.4764],\n",
            "        [-0.6077,  0.7472],\n",
            "        [-1.2134,  1.4895],\n",
            "        [ 0.7362, -1.0001]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2658, device='cuda:0'), logits=tensor([[ 0.4960, -0.5668],\n",
            "        [ 0.3351, -0.5621],\n",
            "        [-1.1213,  1.4598],\n",
            "        [-1.2545,  1.4393],\n",
            "        [ 0.8107, -0.9136],\n",
            "        [-1.1390,  1.2977],\n",
            "        [ 0.7974, -1.0442],\n",
            "        [ 0.8872, -1.0442],\n",
            "        [-1.1542,  1.4537],\n",
            "        [-1.0069,  1.2930],\n",
            "        [ 0.9129, -1.0949],\n",
            "        [-0.2420,  0.1942],\n",
            "        [-1.3091,  1.3943],\n",
            "        [ 0.1468, -0.3609],\n",
            "        [ 0.7114, -0.9290],\n",
            "        [ 0.6958, -0.8485],\n",
            "        [-1.2816,  1.4807],\n",
            "        [ 0.1519, -0.1796],\n",
            "        [ 0.4440, -0.6808],\n",
            "        [-1.1631,  1.4245],\n",
            "        [-1.1859,  1.5015],\n",
            "        [-1.2139,  1.4230],\n",
            "        [ 0.6467, -0.9957],\n",
            "        [ 0.7040, -1.0078],\n",
            "        [ 0.8359, -0.8171],\n",
            "        [ 0.4164, -0.7108],\n",
            "        [-1.2994,  1.3226],\n",
            "        [-1.1625,  1.3837],\n",
            "        [ 0.8202, -1.0077],\n",
            "        [-0.8313,  1.0315],\n",
            "        [-1.2243,  1.4352],\n",
            "        [-0.0071, -0.2669]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6139, device='cuda:0'), logits=tensor([[ 0.8473, -1.0612],\n",
            "        [ 0.0435, -0.2096],\n",
            "        [ 0.2646, -0.2731],\n",
            "        [-1.1385,  1.3902],\n",
            "        [ 0.4532, -0.1079],\n",
            "        [ 0.3381, -0.5781],\n",
            "        [ 0.8837, -0.9373],\n",
            "        [ 0.4490, -0.5849],\n",
            "        [-1.1568,  1.3548],\n",
            "        [ 0.7735, -0.8347],\n",
            "        [-0.3793,  0.4991],\n",
            "        [-1.1512,  1.2873],\n",
            "        [ 0.6764, -1.0027],\n",
            "        [ 0.8444, -1.0373],\n",
            "        [ 0.2257, -0.6063],\n",
            "        [-1.2273,  1.4458],\n",
            "        [-0.6574,  0.7547],\n",
            "        [ 0.6033, -0.5147],\n",
            "        [ 0.5064, -0.7943],\n",
            "        [-0.4507,  0.6848],\n",
            "        [ 0.2107, -0.4798],\n",
            "        [ 0.3871, -0.6425],\n",
            "        [-1.1316,  1.3836],\n",
            "        [-0.8119,  1.0358],\n",
            "        [ 0.9071, -0.9524],\n",
            "        [ 0.4116, -0.5683],\n",
            "        [ 0.4978, -0.7075],\n",
            "        [ 0.8548, -1.1531],\n",
            "        [-1.1552,  1.3216],\n",
            "        [-0.5867,  0.7195],\n",
            "        [ 0.3824, -0.6575],\n",
            "        [ 0.2698, -0.5960]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4882, device='cuda:0'), logits=tensor([[ 0.6257, -0.7012],\n",
            "        [-0.8429,  1.2216],\n",
            "        [-0.9849,  1.1013],\n",
            "        [ 0.6126, -0.6792],\n",
            "        [ 0.7674, -1.0750],\n",
            "        [ 0.8790, -1.1069],\n",
            "        [ 0.7709, -0.7414],\n",
            "        [ 0.4550, -0.3293],\n",
            "        [-0.7627,  0.9159],\n",
            "        [ 0.1519, -0.2336],\n",
            "        [ 0.7993, -0.9273],\n",
            "        [-1.0266,  1.2197],\n",
            "        [ 0.6270, -0.8771],\n",
            "        [ 0.3138, -0.5393],\n",
            "        [-1.2349,  1.3961],\n",
            "        [ 0.6399, -0.4405],\n",
            "        [ 0.3263, -0.1159],\n",
            "        [ 0.4673, -0.6739],\n",
            "        [-0.7658,  0.8825],\n",
            "        [ 0.7311, -1.0000],\n",
            "        [-1.1751,  1.4357],\n",
            "        [-0.8313,  0.9480],\n",
            "        [-0.9789,  1.2572],\n",
            "        [ 0.6725, -0.7556],\n",
            "        [ 0.8679, -1.0463],\n",
            "        [ 0.7505, -0.9910],\n",
            "        [-0.0935, -0.0398],\n",
            "        [-1.1429,  1.4294],\n",
            "        [-0.0144,  0.2707],\n",
            "        [ 0.3226, -0.4693],\n",
            "        [ 0.4691, -0.6751],\n",
            "        [ 0.7962, -0.9111]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3783, device='cuda:0'), logits=tensor([[-0.2625,  0.2993],\n",
            "        [-1.1343,  1.4347],\n",
            "        [-0.9289,  1.0374],\n",
            "        [ 0.7655, -0.8213],\n",
            "        [ 0.7276, -0.8417],\n",
            "        [ 0.4899, -0.8610],\n",
            "        [ 0.5303, -0.7899],\n",
            "        [-1.1874,  1.4694],\n",
            "        [ 0.6103, -0.7004],\n",
            "        [ 0.5631, -0.6653],\n",
            "        [ 0.8388, -0.9681],\n",
            "        [ 0.4819, -0.4002],\n",
            "        [ 0.6308, -0.5915],\n",
            "        [-0.7536,  0.6767],\n",
            "        [ 0.3333, -0.5871],\n",
            "        [-0.6179,  0.8185],\n",
            "        [-1.1682,  1.4401],\n",
            "        [ 0.6457, -0.9066],\n",
            "        [-0.7036,  0.7328],\n",
            "        [ 0.7911, -0.9431],\n",
            "        [ 0.3132, -0.5636],\n",
            "        [-1.0851,  1.3363],\n",
            "        [ 0.4956, -0.5870],\n",
            "        [ 0.1701, -0.5176],\n",
            "        [ 0.8589, -1.0595],\n",
            "        [-0.6509,  0.8126],\n",
            "        [-0.4448,  0.3843],\n",
            "        [ 0.6520, -0.9457],\n",
            "        [-1.2322,  1.2777],\n",
            "        [ 0.8093, -1.0364],\n",
            "        [-1.3249,  1.4666],\n",
            "        [ 0.7117, -0.7204]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5303, device='cuda:0'), logits=tensor([[ 0.8283, -1.0509],\n",
            "        [-0.1803,  0.0882],\n",
            "        [ 0.8348, -1.0718],\n",
            "        [-1.1386,  1.4041],\n",
            "        [ 0.6398, -0.9646],\n",
            "        [ 0.2710, -0.6251],\n",
            "        [ 0.5420, -0.7286],\n",
            "        [ 0.7952, -0.9728],\n",
            "        [ 0.0017,  0.0149],\n",
            "        [-1.2800,  1.4963],\n",
            "        [ 0.8692, -1.0082],\n",
            "        [ 0.7007, -1.0277],\n",
            "        [-0.8476,  1.2027],\n",
            "        [ 0.4241, -0.6200],\n",
            "        [-1.0046,  1.1599],\n",
            "        [-1.1086,  1.3667],\n",
            "        [-1.1706,  1.2637],\n",
            "        [-1.0782,  1.3145],\n",
            "        [-1.1862,  1.4186],\n",
            "        [-0.6037,  0.7583],\n",
            "        [-1.2214,  1.3070],\n",
            "        [ 0.5980, -0.7595],\n",
            "        [ 0.4914, -0.7866],\n",
            "        [ 0.7455, -0.7962],\n",
            "        [ 0.5950, -0.8116],\n",
            "        [ 0.7134, -0.8509],\n",
            "        [ 0.0776, -0.1020],\n",
            "        [ 0.7218, -0.8864],\n",
            "        [-0.8701,  0.9563],\n",
            "        [ 0.4734, -0.8081],\n",
            "        [ 0.6591, -0.8820],\n",
            "        [ 0.5225, -0.8148]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3388, device='cuda:0'), logits=tensor([[-0.9189,  1.0923],\n",
            "        [-0.9633,  1.0807],\n",
            "        [-0.2292,  0.4813],\n",
            "        [-1.2577,  1.3955],\n",
            "        [ 0.3431, -0.5813],\n",
            "        [ 0.3843, -0.6052],\n",
            "        [-0.0928,  0.1706],\n",
            "        [ 0.6765, -0.5951],\n",
            "        [ 0.4522, -0.6741],\n",
            "        [-1.0239,  0.9565],\n",
            "        [ 0.3225, -0.6211],\n",
            "        [ 0.9340, -1.0882],\n",
            "        [-1.2222,  1.4788],\n",
            "        [ 0.8751, -0.9780],\n",
            "        [-1.2192,  1.3420],\n",
            "        [-0.7780,  0.7448],\n",
            "        [ 0.5776, -0.8111],\n",
            "        [-1.0979,  1.3766],\n",
            "        [ 0.6054, -0.9038],\n",
            "        [ 0.5670, -0.8313],\n",
            "        [ 0.3184, -0.1486],\n",
            "        [ 0.7902, -1.0026],\n",
            "        [-0.4738,  0.5301],\n",
            "        [-0.2955,  0.3634],\n",
            "        [-0.1124,  0.1166],\n",
            "        [-0.6848,  0.8205],\n",
            "        [-1.2417,  1.4073],\n",
            "        [-1.2416,  1.3858],\n",
            "        [ 0.8105, -0.9656],\n",
            "        [ 0.4475, -0.6243],\n",
            "        [-1.0302,  1.1793],\n",
            "        [-1.0917,  1.2784]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3488, device='cuda:0'), logits=tensor([[ 0.9271, -1.1236],\n",
            "        [ 0.8869, -1.1107],\n",
            "        [ 0.1586, -0.1317],\n",
            "        [ 0.9014, -1.1078],\n",
            "        [-0.7936,  0.9788],\n",
            "        [ 0.7464, -0.8735],\n",
            "        [ 0.8114, -0.9291],\n",
            "        [-0.0621, -0.1183],\n",
            "        [-1.1172,  1.4661],\n",
            "        [ 0.5828, -0.7995],\n",
            "        [ 0.3566, -0.5364],\n",
            "        [ 0.2453, -0.1323],\n",
            "        [-0.9882,  1.3251],\n",
            "        [ 0.6632, -0.9444],\n",
            "        [-1.0626,  1.2163],\n",
            "        [-1.2594,  1.4803],\n",
            "        [ 0.4876, -0.5270],\n",
            "        [ 0.7897, -0.9023],\n",
            "        [ 0.6846, -0.7912],\n",
            "        [ 0.7756, -1.0502],\n",
            "        [ 0.4610, -0.6235],\n",
            "        [-0.1816,  0.0524],\n",
            "        [ 0.7165, -0.9741],\n",
            "        [-0.5954,  0.5578],\n",
            "        [-0.7269,  0.8148],\n",
            "        [ 0.5370, -0.7952],\n",
            "        [-0.0318,  0.2174],\n",
            "        [-0.7918,  1.0965],\n",
            "        [ 0.8859, -1.1562],\n",
            "        [-1.2266,  1.4273],\n",
            "        [ 0.7844, -1.0126],\n",
            "        [ 0.7481, -0.9609]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5724, device='cuda:0'), logits=tensor([[-1.1828,  1.4505],\n",
            "        [ 0.7080, -0.8324],\n",
            "        [ 0.6945, -0.8709],\n",
            "        [-1.3269,  1.4283],\n",
            "        [ 0.0520, -0.4472],\n",
            "        [ 0.8051, -0.8317],\n",
            "        [ 0.5596, -0.6215],\n",
            "        [ 0.5578, -0.6443],\n",
            "        [ 0.3967, -0.6640],\n",
            "        [ 0.8595, -1.0757],\n",
            "        [-0.9399,  1.1329],\n",
            "        [-0.1685,  0.1109],\n",
            "        [ 0.5276, -0.1310],\n",
            "        [ 0.5217, -0.5782],\n",
            "        [ 0.4054, -0.5529],\n",
            "        [ 0.0411, -0.1000],\n",
            "        [ 0.1233, -0.1492],\n",
            "        [ 0.2670, -0.4715],\n",
            "        [-0.8458,  0.9944],\n",
            "        [-1.1924,  1.3608],\n",
            "        [ 0.4502, -0.5850],\n",
            "        [ 0.4104, -0.4730],\n",
            "        [ 0.5896, -0.7645],\n",
            "        [-0.3860,  0.5080],\n",
            "        [-0.4628,  0.3722],\n",
            "        [-0.2633,  0.1959],\n",
            "        [ 0.8179, -1.0301],\n",
            "        [ 0.6509, -0.8103],\n",
            "        [-0.9970,  1.2519],\n",
            "        [-0.9393,  1.0696],\n",
            "        [-1.0495,  1.1565],\n",
            "        [-1.1918,  1.4764]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3090, device='cuda:0'), logits=tensor([[-1.1354,  1.3954],\n",
            "        [ 0.8835, -1.0854],\n",
            "        [ 0.4219, -0.2064],\n",
            "        [ 0.9230, -1.0178],\n",
            "        [ 0.7570, -0.9992],\n",
            "        [-0.8804,  1.2340],\n",
            "        [ 0.8452, -0.9740],\n",
            "        [ 0.7677, -1.1161],\n",
            "        [-1.1469,  1.2493],\n",
            "        [-0.4313,  0.3609],\n",
            "        [ 0.6225, -0.6782],\n",
            "        [ 0.5927, -0.8605],\n",
            "        [-0.9316,  1.1904],\n",
            "        [ 0.7288, -0.9462],\n",
            "        [ 0.7212, -0.6821],\n",
            "        [-1.0570,  1.1937],\n",
            "        [ 0.9290, -1.1059],\n",
            "        [ 0.2430, -0.3661],\n",
            "        [ 0.8021, -0.8985],\n",
            "        [ 0.0597, -0.1332],\n",
            "        [-1.1469,  1.2493],\n",
            "        [-0.5250,  0.6427],\n",
            "        [ 0.7950, -1.0441],\n",
            "        [-1.2114,  1.2687],\n",
            "        [-0.7821,  1.0004],\n",
            "        [-1.1671,  1.3173],\n",
            "        [-1.1988,  1.2382],\n",
            "        [-0.8962,  1.1358],\n",
            "        [ 0.7105, -0.9069],\n",
            "        [ 0.8631, -1.0855],\n",
            "        [-1.1947,  1.3057],\n",
            "        [ 0.6080, -0.4586]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4961, device='cuda:0'), logits=tensor([[-0.4057,  0.6272],\n",
            "        [ 0.8145, -1.1015],\n",
            "        [-1.2264,  1.4550],\n",
            "        [ 0.6378, -0.9361],\n",
            "        [ 0.2770, -0.7036],\n",
            "        [ 0.8769, -0.9667],\n",
            "        [-1.1922,  1.4204],\n",
            "        [ 0.5788, -0.9072],\n",
            "        [-0.6118,  0.6513],\n",
            "        [-1.0603,  1.0316],\n",
            "        [ 0.6543, -0.6368],\n",
            "        [ 0.2738, -0.1016],\n",
            "        [ 0.4983, -0.7990],\n",
            "        [ 0.2945, -0.4772],\n",
            "        [ 0.6260, -0.9901],\n",
            "        [ 0.5528, -0.7447],\n",
            "        [ 0.6209, -0.8054],\n",
            "        [-0.9949,  1.3508],\n",
            "        [ 0.7725, -0.8762],\n",
            "        [ 0.5943, -0.4672],\n",
            "        [ 0.8128, -0.9753],\n",
            "        [ 0.7853, -1.0012],\n",
            "        [ 0.7781, -0.8747],\n",
            "        [-0.0118,  0.0702],\n",
            "        [ 0.8220, -1.0604],\n",
            "        [ 0.6051, -0.8572],\n",
            "        [-0.4756,  0.4439],\n",
            "        [ 0.2001, -0.4519],\n",
            "        [-0.1873,  0.1627],\n",
            "        [ 0.9225, -0.9291],\n",
            "        [ 0.7368, -0.9782],\n",
            "        [ 0.4888, -0.7831]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5664, device='cuda:0'), logits=tensor([[-0.1156,  0.1463],\n",
            "        [-1.1268,  1.4974],\n",
            "        [-1.1053,  1.2162],\n",
            "        [-0.7271,  1.0363],\n",
            "        [ 0.4223, -0.8338],\n",
            "        [-0.3905,  0.4702],\n",
            "        [-0.0227, -0.2917],\n",
            "        [ 0.2912, -0.5988],\n",
            "        [ 0.5920, -0.7316],\n",
            "        [-0.7452,  0.9300],\n",
            "        [-1.0891,  1.4279],\n",
            "        [ 0.6895, -0.8490],\n",
            "        [ 0.6594, -0.7392],\n",
            "        [ 0.8944, -0.9994],\n",
            "        [-0.8616,  1.0071],\n",
            "        [ 0.7556, -0.9214],\n",
            "        [ 0.7697, -1.0049],\n",
            "        [ 0.3732, -0.6579],\n",
            "        [ 0.7762, -0.9063],\n",
            "        [ 0.7296, -0.8944],\n",
            "        [ 0.8264, -1.0135],\n",
            "        [-1.2142,  1.4845],\n",
            "        [-0.7918,  0.9846],\n",
            "        [-1.3163,  1.3981],\n",
            "        [-1.2966,  1.3977],\n",
            "        [ 0.8365, -0.8305],\n",
            "        [ 0.6646, -0.9189],\n",
            "        [-0.5320,  0.4328],\n",
            "        [ 0.2016, -0.5425],\n",
            "        [ 0.6871, -1.0158],\n",
            "        [ 0.6263, -0.8545],\n",
            "        [ 0.7972, -0.9590]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6187, device='cuda:0'), logits=tensor([[-0.9808,  1.1086],\n",
            "        [ 0.2939, -0.0584],\n",
            "        [ 0.7767, -0.7026],\n",
            "        [ 0.0804, -0.0492],\n",
            "        [ 0.7486, -0.6841],\n",
            "        [ 0.8899, -0.9102],\n",
            "        [ 0.7896, -1.0138],\n",
            "        [ 0.7465, -1.0534],\n",
            "        [-0.0452, -0.2690],\n",
            "        [ 0.8922, -0.9608],\n",
            "        [ 0.6814, -0.8469],\n",
            "        [-1.1577,  1.4428],\n",
            "        [ 0.3514, -0.3497],\n",
            "        [ 0.7151, -0.7698],\n",
            "        [ 0.8658, -1.0897],\n",
            "        [ 0.3474, -0.1994],\n",
            "        [-0.9393,  1.0696],\n",
            "        [ 0.5818, -0.5859],\n",
            "        [ 0.7898, -1.0114]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "  Accuracy: 0.82\n",
            "  F1: 0.78\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:20\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7685, -1.1838],\n",
            "        [ 0.7065, -0.7327],\n",
            "        [ 0.8137, -1.0305],\n",
            "        [-0.5066,  0.7929],\n",
            "        [ 0.7708, -0.9568],\n",
            "        [ 0.7341, -0.8929],\n",
            "        [ 0.3170, -0.0469],\n",
            "        [-0.7264,  0.7411],\n",
            "        [ 0.8579, -0.9559],\n",
            "        [ 0.6134, -0.8626],\n",
            "        [ 0.5662, -1.0991],\n",
            "        [-0.9993,  1.4629],\n",
            "        [ 0.6260, -1.0907],\n",
            "        [ 0.4127, -0.4140],\n",
            "        [ 0.6502, -0.9186],\n",
            "        [ 0.3715, -0.4527],\n",
            "        [-1.1321,  1.4790],\n",
            "        [ 0.8196, -0.8910],\n",
            "        [ 0.5229, -0.4570],\n",
            "        [ 0.0247, -0.1243],\n",
            "        [ 0.5586, -0.4143],\n",
            "        [-1.0933,  1.2170],\n",
            "        [-0.7730,  0.7515],\n",
            "        [-0.0025,  0.0864],\n",
            "        [ 0.8935, -1.0059],\n",
            "        [-1.0896,  1.3041],\n",
            "        [ 0.7004, -0.9164],\n",
            "        [ 0.4771, -0.6719],\n",
            "        [ 0.3540, -0.6509],\n",
            "        [-0.8388,  0.9529],\n",
            "        [ 0.8637, -1.2464],\n",
            "        [ 0.9782, -1.0150]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5233, -0.4846],\n",
            "        [ 0.6752, -0.7769],\n",
            "        [-1.2616,  1.2949],\n",
            "        [-0.0790,  0.0265],\n",
            "        [ 0.7405, -0.9850],\n",
            "        [ 0.9107, -0.8307],\n",
            "        [-0.1658, -0.1420],\n",
            "        [-1.0518,  1.2773],\n",
            "        [ 0.8804, -1.0525],\n",
            "        [ 0.1817, -0.3257],\n",
            "        [ 0.8123, -0.9648],\n",
            "        [-0.7741,  0.8848],\n",
            "        [ 0.7620, -0.9374],\n",
            "        [-1.2233,  1.1397],\n",
            "        [ 0.9014, -0.7849],\n",
            "        [ 0.8852, -0.9245],\n",
            "        [ 0.4916, -0.8139],\n",
            "        [ 0.8985, -0.7660],\n",
            "        [ 0.4833, -0.8049],\n",
            "        [ 0.9019, -1.0084],\n",
            "        [ 0.1409, -0.2308],\n",
            "        [-0.7381,  1.1301],\n",
            "        [-0.5094,  0.3926],\n",
            "        [ 0.6500, -0.6013],\n",
            "        [ 0.4136, -0.2739],\n",
            "        [-1.2196,  1.3742],\n",
            "        [ 0.6089, -0.7945],\n",
            "        [-0.8422,  0.8997],\n",
            "        [-1.0342,  1.5641],\n",
            "        [-0.4321,  0.5467],\n",
            "        [-1.4898,  1.4436],\n",
            "        [ 0.3040, -0.3497]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0071, -0.9745],\n",
            "        [ 0.8334, -0.7693],\n",
            "        [-0.5990,  0.9550],\n",
            "        [ 0.7420, -0.7380],\n",
            "        [ 0.4623, -0.7451],\n",
            "        [ 0.5638, -0.5133],\n",
            "        [ 0.8453, -0.9519],\n",
            "        [ 0.8808, -1.0134],\n",
            "        [ 0.4321, -0.7522],\n",
            "        [ 0.8500, -0.6354],\n",
            "        [ 0.6010, -1.0349],\n",
            "        [-0.2701,  0.4260],\n",
            "        [ 0.9502, -1.1655],\n",
            "        [ 0.9281, -0.9549],\n",
            "        [-1.3112,  1.2503],\n",
            "        [-0.9435,  1.0473],\n",
            "        [-0.6316,  0.5572],\n",
            "        [ 0.4898, -0.7074],\n",
            "        [-0.9003,  1.1197],\n",
            "        [ 0.0699,  0.0043],\n",
            "        [ 0.3393,  0.0971],\n",
            "        [ 0.1415, -0.1331],\n",
            "        [-0.9077,  0.8700],\n",
            "        [-0.8942,  1.1488],\n",
            "        [ 0.6221, -0.8786],\n",
            "        [ 0.7982, -0.9541],\n",
            "        [ 0.8590, -0.9455],\n",
            "        [ 0.6542, -0.8350],\n",
            "        [ 0.8688, -1.1687],\n",
            "        [-0.0370, -0.1547],\n",
            "        [-0.0656, -0.1132],\n",
            "        [-0.8645,  1.2306]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3848, -0.2634],\n",
            "        [ 0.6247, -0.9335],\n",
            "        [ 0.6142, -0.7165],\n",
            "        [-1.0945,  1.0662],\n",
            "        [-0.4636,  0.5619],\n",
            "        [ 0.0607, -0.2983],\n",
            "        [ 0.6920, -0.9854],\n",
            "        [-1.0503,  1.2606],\n",
            "        [ 0.7739, -0.7612],\n",
            "        [ 0.5291, -0.6919],\n",
            "        [-1.1224,  1.4497],\n",
            "        [ 0.8198, -0.8669],\n",
            "        [-1.2330,  0.9435],\n",
            "        [ 0.8259, -1.0779],\n",
            "        [ 0.1859, -0.4818],\n",
            "        [-1.0500,  1.2390],\n",
            "        [ 0.8767, -0.9914],\n",
            "        [-0.3179,  0.3399],\n",
            "        [ 0.9949, -1.0351],\n",
            "        [-1.0877,  1.2871],\n",
            "        [-0.9891,  0.8230],\n",
            "        [ 0.7315, -0.9123],\n",
            "        [-0.9227,  1.0839],\n",
            "        [-0.1306,  0.1795],\n",
            "        [-0.0694, -0.2094],\n",
            "        [-0.8241,  1.0722],\n",
            "        [ 0.9355, -1.2891],\n",
            "        [ 0.3470, -0.4212],\n",
            "        [ 0.6442, -1.2118],\n",
            "        [ 0.8085, -0.9717],\n",
            "        [ 0.2441, -0.6535],\n",
            "        [ 0.3069, -0.6911]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4076,  0.6957],\n",
            "        [-1.0875,  1.0525],\n",
            "        [ 0.8209, -0.8650],\n",
            "        [ 0.7087, -1.0105],\n",
            "        [ 0.2634, -0.1517],\n",
            "        [ 0.7289, -1.2127],\n",
            "        [-1.1435,  1.3146],\n",
            "        [ 0.0555, -0.3876],\n",
            "        [-1.3387,  1.2035],\n",
            "        [-1.2692,  1.2599],\n",
            "        [ 0.1081, -0.2761],\n",
            "        [ 0.7143, -0.6917],\n",
            "        [ 0.3572, -0.0942],\n",
            "        [ 0.6102, -1.1069],\n",
            "        [ 0.8618, -1.0513],\n",
            "        [ 0.6604, -1.3874],\n",
            "        [-1.1488,  1.3291],\n",
            "        [-1.1420,  1.3583],\n",
            "        [ 0.6090, -0.9877],\n",
            "        [-1.2477,  1.3950],\n",
            "        [ 0.9739, -0.9824],\n",
            "        [ 0.1703, -0.6021],\n",
            "        [ 0.5495, -0.2608],\n",
            "        [ 0.9146, -0.9679],\n",
            "        [-0.0169,  0.1336],\n",
            "        [-1.1098,  1.3152],\n",
            "        [ 0.7290, -0.9860],\n",
            "        [ 0.2779, -0.3044],\n",
            "        [ 0.6746, -0.9133],\n",
            "        [ 0.8041, -0.9726],\n",
            "        [ 0.7655, -0.9024],\n",
            "        [ 0.8009, -1.0400]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8246, -1.0295],\n",
            "        [ 0.5981, -0.7451],\n",
            "        [ 0.7695, -0.7444],\n",
            "        [ 0.7783, -1.0162],\n",
            "        [ 0.8626, -0.7124],\n",
            "        [ 0.8310, -0.7607],\n",
            "        [ 0.4550, -0.6150],\n",
            "        [ 0.7413, -1.1881],\n",
            "        [ 0.5388, -0.7153],\n",
            "        [-0.7594,  0.9548],\n",
            "        [-0.8058,  1.2896],\n",
            "        [-1.1673,  0.9650],\n",
            "        [ 0.2364, -0.3036],\n",
            "        [ 0.8009, -1.0567],\n",
            "        [ 0.8724, -0.9933],\n",
            "        [ 0.6021, -0.7189],\n",
            "        [ 0.9349, -1.1623],\n",
            "        [-0.2576, -0.0542],\n",
            "        [ 0.5014, -0.4007],\n",
            "        [ 0.9022, -1.0549],\n",
            "        [-0.2437,  0.1852],\n",
            "        [ 0.1950, -0.2156],\n",
            "        [-0.8492,  0.9988],\n",
            "        [ 0.8597, -1.2976],\n",
            "        [ 0.9819, -0.9964],\n",
            "        [-1.2269,  1.3832],\n",
            "        [ 0.8748, -1.1057],\n",
            "        [ 0.6680, -0.9323],\n",
            "        [-0.9534,  1.2572],\n",
            "        [ 0.7013, -0.9854],\n",
            "        [ 0.8588, -0.9720],\n",
            "        [ 0.3623, -0.9000]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4445,  0.1713],\n",
            "        [-1.0444,  1.4024],\n",
            "        [ 0.2131, -0.4693],\n",
            "        [ 0.1530, -0.0136],\n",
            "        [ 0.6370, -0.6361],\n",
            "        [ 0.7081, -0.7312],\n",
            "        [ 0.9623, -0.7946],\n",
            "        [-1.3050,  1.1504],\n",
            "        [ 0.5004, -0.5810],\n",
            "        [ 0.9786, -0.9166],\n",
            "        [ 0.4802, -0.8817],\n",
            "        [ 0.4119, -0.6774],\n",
            "        [ 0.1168, -0.0844],\n",
            "        [ 1.0582, -1.0490],\n",
            "        [-1.1058,  1.2834],\n",
            "        [ 0.5538, -0.8013],\n",
            "        [ 0.1268, -0.1261],\n",
            "        [ 0.7423, -0.8287],\n",
            "        [-0.4513,  0.1307],\n",
            "        [-1.3076,  1.2934],\n",
            "        [ 0.4976, -0.6728],\n",
            "        [ 0.3783, -0.2140],\n",
            "        [ 0.6452, -0.9927],\n",
            "        [ 0.7663, -0.9848],\n",
            "        [ 0.8195, -1.1408],\n",
            "        [ 0.9721, -1.0169],\n",
            "        [ 1.0172, -0.9743],\n",
            "        [ 0.4790, -0.0801],\n",
            "        [-1.1230,  1.1480],\n",
            "        [ 0.9048, -0.8813],\n",
            "        [-0.9939,  1.0733],\n",
            "        [ 0.5950, -1.1808]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6154, -0.9173],\n",
            "        [ 0.6516, -0.8960],\n",
            "        [-0.1786,  0.1359],\n",
            "        [-1.0612,  1.4650],\n",
            "        [ 0.5950, -0.8851],\n",
            "        [ 0.7034, -0.8159],\n",
            "        [ 0.7072, -0.7883],\n",
            "        [-1.0984,  1.1711],\n",
            "        [-0.0340,  0.0391],\n",
            "        [ 0.8427, -0.8347],\n",
            "        [ 0.7029, -0.9271],\n",
            "        [ 0.4257, -0.4774],\n",
            "        [ 0.7538, -0.8027],\n",
            "        [ 0.6588, -0.9685],\n",
            "        [ 0.7243, -0.7038],\n",
            "        [ 0.9408, -1.0284],\n",
            "        [ 1.1135, -0.8252],\n",
            "        [-0.9323,  1.1321],\n",
            "        [ 0.8725, -0.9360],\n",
            "        [ 0.2232, -0.1509],\n",
            "        [ 0.5457, -0.2686],\n",
            "        [-1.2374,  1.5198],\n",
            "        [ 0.9370, -0.8713],\n",
            "        [ 0.0540, -0.3414],\n",
            "        [-1.0007,  1.2792],\n",
            "        [ 0.5961, -0.9950],\n",
            "        [ 0.7454, -0.8124],\n",
            "        [-0.5464,  0.3588],\n",
            "        [ 0.6746, -1.1986],\n",
            "        [-1.4152,  1.3491],\n",
            "        [ 0.4507, -0.5427],\n",
            "        [ 0.1147, -0.6752]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1199,  1.1345],\n",
            "        [-1.2972,  1.2083],\n",
            "        [-0.8556,  0.9046],\n",
            "        [ 0.3777, -0.4514],\n",
            "        [ 0.6482, -1.1572],\n",
            "        [ 0.6783, -0.9291],\n",
            "        [-1.1434,  1.3080],\n",
            "        [-1.2872,  1.3310],\n",
            "        [-1.2459,  1.3323],\n",
            "        [-1.2253,  1.4051],\n",
            "        [ 0.8645, -0.9234],\n",
            "        [-0.7711,  0.6537],\n",
            "        [ 0.8939, -1.0195],\n",
            "        [ 0.4939, -0.6680],\n",
            "        [ 0.6089, -0.9734],\n",
            "        [ 0.1581, -0.0581],\n",
            "        [ 0.8778, -1.1303],\n",
            "        [ 0.8046, -1.0235],\n",
            "        [ 0.0038, -0.0912],\n",
            "        [-0.1539,  0.0167],\n",
            "        [ 0.4355, -0.4655],\n",
            "        [ 0.6919, -0.6590],\n",
            "        [ 0.3829, -0.7844],\n",
            "        [-0.0075, -0.1805],\n",
            "        [ 0.8096, -0.9472],\n",
            "        [ 0.7849, -0.8960],\n",
            "        [-1.3007,  1.4030],\n",
            "        [ 0.7654, -0.8377],\n",
            "        [-1.0319,  1.4102],\n",
            "        [ 0.4430, -0.6576],\n",
            "        [-1.2068,  1.2026],\n",
            "        [ 0.8017, -1.1267]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0860e+00, -8.4317e-01],\n",
            "        [ 8.0186e-01, -9.1002e-01],\n",
            "        [-2.6020e-01,  3.6424e-01],\n",
            "        [ 7.3510e-01, -1.0859e+00],\n",
            "        [ 3.8850e-01, -3.6568e-01],\n",
            "        [ 7.6418e-01, -7.8337e-01],\n",
            "        [ 1.3221e-01, -1.3208e-01],\n",
            "        [-1.1679e+00,  1.1896e+00],\n",
            "        [ 4.4775e-02, -1.3380e-01],\n",
            "        [ 7.8518e-01, -6.7434e-01],\n",
            "        [ 7.6589e-01, -7.7612e-01],\n",
            "        [-2.4720e-01,  2.5332e-01],\n",
            "        [-1.8418e-01,  2.1594e-01],\n",
            "        [-9.3409e-01,  1.3746e+00],\n",
            "        [-1.0294e+00,  1.4692e+00],\n",
            "        [ 3.8946e-01, -4.3550e-01],\n",
            "        [ 3.6440e-01, -5.6190e-01],\n",
            "        [ 7.4178e-01, -6.0875e-01],\n",
            "        [ 6.8732e-01, -1.0497e+00],\n",
            "        [ 4.8276e-01, -4.2146e-01],\n",
            "        [ 7.6215e-01, -1.0775e+00],\n",
            "        [-9.3762e-01,  1.4036e+00],\n",
            "        [ 7.9430e-01, -1.0554e+00],\n",
            "        [ 2.9771e-01, -2.8675e-01],\n",
            "        [ 2.8009e-01, -3.8106e-01],\n",
            "        [ 7.0184e-01, -9.1343e-01],\n",
            "        [-1.1065e+00,  1.5282e+00],\n",
            "        [ 8.4508e-01, -1.0166e+00],\n",
            "        [-1.0649e+00,  1.4878e+00],\n",
            "        [ 8.9671e-04, -3.0998e-01],\n",
            "        [ 2.7062e-01, -1.8826e-01],\n",
            "        [-1.2788e+00,  1.3920e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0171,  1.2944],\n",
            "        [-0.5918,  0.6292],\n",
            "        [ 0.7288, -1.2892],\n",
            "        [ 0.6534, -0.9584],\n",
            "        [ 0.8622, -1.0957],\n",
            "        [-0.4362,  0.2396],\n",
            "        [ 1.0575, -0.5744],\n",
            "        [-0.1808,  0.2094],\n",
            "        [ 0.4316, -0.3118],\n",
            "        [-1.2078,  1.3074],\n",
            "        [ 1.0523, -1.1645],\n",
            "        [ 0.8278, -0.8919],\n",
            "        [-1.1878,  1.1155],\n",
            "        [-0.9822,  1.2914],\n",
            "        [ 0.9204, -0.7851],\n",
            "        [ 0.9096, -1.1866],\n",
            "        [ 0.8497, -0.8311],\n",
            "        [ 0.4836, -0.9588],\n",
            "        [-0.9948,  1.2805],\n",
            "        [-1.3178,  1.1926],\n",
            "        [ 0.7211, -0.8531],\n",
            "        [ 0.2287, -0.6145],\n",
            "        [-1.3305,  1.2035],\n",
            "        [ 0.8546, -1.0556],\n",
            "        [ 0.5577, -0.9356],\n",
            "        [-1.4646,  1.5184],\n",
            "        [ 0.7777, -0.8945],\n",
            "        [ 0.8473, -1.0543],\n",
            "        [-1.1834,  1.3774],\n",
            "        [-1.0481,  1.3982],\n",
            "        [ 0.7588, -1.0669],\n",
            "        [ 0.8397, -1.0595]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6977,  0.5575],\n",
            "        [ 0.6150, -0.5860],\n",
            "        [-0.2979,  0.4094],\n",
            "        [-1.2198,  1.3030],\n",
            "        [ 0.6802, -0.5395],\n",
            "        [ 0.6390, -0.7358],\n",
            "        [ 0.4355, -0.3946],\n",
            "        [ 0.7988, -1.0862],\n",
            "        [ 0.8939, -0.8947],\n",
            "        [-0.3670,  0.1975],\n",
            "        [-1.0770,  1.1645],\n",
            "        [-1.2705,  1.3647],\n",
            "        [ 0.6831, -0.9938],\n",
            "        [ 0.3456, -0.3121],\n",
            "        [ 0.5967, -0.7737],\n",
            "        [ 0.5702, -0.8944],\n",
            "        [-0.0654,  0.0207],\n",
            "        [ 0.8371, -1.1874],\n",
            "        [ 0.7647, -1.0560],\n",
            "        [-1.2025,  1.3960],\n",
            "        [ 0.8811, -1.0108],\n",
            "        [ 0.7867, -0.5802],\n",
            "        [-1.3136,  1.4340],\n",
            "        [-0.3997,  0.7248],\n",
            "        [ 0.7184, -1.0423],\n",
            "        [-0.5292,  0.3921],\n",
            "        [ 0.4582, -0.7723],\n",
            "        [ 1.0392, -1.1177],\n",
            "        [ 1.0328, -0.7138],\n",
            "        [ 1.0272, -1.1413],\n",
            "        [-0.8582,  1.2100],\n",
            "        [-1.1860,  1.2275]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4907, -0.5032],\n",
            "        [-0.6558,  0.8741],\n",
            "        [-1.0155,  1.3176],\n",
            "        [ 0.6076, -0.7481],\n",
            "        [-0.2589,  0.6404],\n",
            "        [ 0.5931, -0.2720],\n",
            "        [-1.1589,  1.4334],\n",
            "        [-1.3564,  1.4222],\n",
            "        [ 0.0741,  0.0908],\n",
            "        [ 0.2574, -0.7321],\n",
            "        [-1.1341,  1.3802],\n",
            "        [-0.0686,  0.2117],\n",
            "        [ 0.8840, -1.0700],\n",
            "        [-0.9914,  1.3440],\n",
            "        [ 0.8720, -1.1135],\n",
            "        [ 0.4858, -0.6931],\n",
            "        [ 0.7109, -0.5093],\n",
            "        [-0.1041,  0.1696],\n",
            "        [-1.1748,  1.2867],\n",
            "        [ 0.8452, -0.9383],\n",
            "        [ 1.0108, -0.8457],\n",
            "        [-1.4580,  1.2128],\n",
            "        [-0.7921,  1.1642],\n",
            "        [-1.0201,  1.3141],\n",
            "        [ 0.8244, -1.0316],\n",
            "        [ 0.0600, -0.1779],\n",
            "        [-1.2120,  1.3953],\n",
            "        [-0.4844,  0.6620],\n",
            "        [ 0.8859, -0.9663],\n",
            "        [-1.1957,  1.0552],\n",
            "        [ 0.6851, -1.0409],\n",
            "        [ 0.4639, -0.5353]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4267, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4985, -0.6498],\n",
            "        [ 0.7044, -1.1016],\n",
            "        [ 0.7324, -1.0108],\n",
            "        [ 0.7054, -1.0043],\n",
            "        [ 0.8180, -0.8940],\n",
            "        [ 0.3622, -0.8519],\n",
            "        [-0.9236,  1.0121],\n",
            "        [ 0.8379, -0.9114],\n",
            "        [ 1.0339, -1.0416],\n",
            "        [-1.0488,  0.7715],\n",
            "        [ 0.9346, -0.8042],\n",
            "        [ 0.6676, -0.9402],\n",
            "        [-0.3623,  0.5814],\n",
            "        [-0.2237,  0.5050],\n",
            "        [ 0.7821, -0.7758],\n",
            "        [-0.6890,  0.6489],\n",
            "        [-0.3808,  0.1976],\n",
            "        [ 0.7925, -0.9474],\n",
            "        [-1.3682,  1.4507],\n",
            "        [ 0.7180, -1.1722],\n",
            "        [ 0.5820, -1.1253],\n",
            "        [ 0.7212, -0.9272],\n",
            "        [-0.7611,  0.9528],\n",
            "        [ 0.8559, -1.1615],\n",
            "        [ 0.6283, -0.9903],\n",
            "        [-1.1985,  1.1869],\n",
            "        [ 0.0052,  0.1977],\n",
            "        [ 0.2853, -0.6669],\n",
            "        [ 0.0415,  0.1611],\n",
            "        [-0.4078,  0.4070],\n",
            "        [ 0.8808, -0.9420],\n",
            "        [ 0.0619, -0.3355]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5555, -0.6381],\n",
            "        [-0.8554,  0.8074],\n",
            "        [-0.5884,  0.7856],\n",
            "        [-1.1603,  1.2323],\n",
            "        [-0.9459,  1.4662],\n",
            "        [-1.1235,  0.9811],\n",
            "        [-1.3934,  1.4975],\n",
            "        [-1.1454,  1.4052],\n",
            "        [-1.0680,  0.9720],\n",
            "        [-1.3582,  1.5022],\n",
            "        [ 0.7945, -0.6726],\n",
            "        [-1.0537,  1.3007],\n",
            "        [ 1.0677, -1.0348],\n",
            "        [-1.2671,  1.3803],\n",
            "        [ 0.5388, -0.9782],\n",
            "        [ 0.9473, -0.9618],\n",
            "        [ 1.0283, -0.8178],\n",
            "        [ 0.4248, -0.4810],\n",
            "        [ 0.8861, -0.9584],\n",
            "        [-0.9829,  1.3543],\n",
            "        [ 0.3040, -0.6304],\n",
            "        [-0.1165,  0.1917],\n",
            "        [ 0.7153, -1.0008],\n",
            "        [ 0.1308, -0.1454],\n",
            "        [-1.0417,  1.3138],\n",
            "        [ 0.4410, -0.6916],\n",
            "        [-1.0208,  1.0255],\n",
            "        [ 0.7024, -0.7238],\n",
            "        [ 0.9532, -1.0103],\n",
            "        [ 0.7005, -1.0046],\n",
            "        [-1.3486,  1.5394],\n",
            "        [ 1.0608, -1.0440]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6188, -0.5789],\n",
            "        [ 0.6478, -1.0718],\n",
            "        [ 0.8022, -0.8847],\n",
            "        [-1.2126,  1.0363],\n",
            "        [ 1.0670, -1.0024],\n",
            "        [ 0.9827, -1.0693],\n",
            "        [ 0.6962, -0.7403],\n",
            "        [-1.1369,  1.3868],\n",
            "        [-0.1934,  0.6638],\n",
            "        [-1.2227,  1.3712],\n",
            "        [ 0.8635, -0.4942],\n",
            "        [ 0.9158, -1.2122],\n",
            "        [-1.3533,  1.5468],\n",
            "        [-0.4137,  0.5577],\n",
            "        [ 0.7711, -0.9761],\n",
            "        [ 0.8103, -1.1229],\n",
            "        [ 0.2335, -0.1760],\n",
            "        [ 0.9505, -1.0451],\n",
            "        [-0.8242,  1.0920],\n",
            "        [ 0.8825, -0.9890],\n",
            "        [ 0.9141, -0.9528],\n",
            "        [-1.2686,  1.2087],\n",
            "        [ 0.3240, -0.3301],\n",
            "        [ 0.8818, -0.8423],\n",
            "        [ 0.6626, -1.3089],\n",
            "        [ 0.3524, -0.6267],\n",
            "        [-1.1697,  1.2740],\n",
            "        [-1.1213,  1.5711],\n",
            "        [-0.8568,  0.9674],\n",
            "        [-0.5138,  0.4208],\n",
            "        [-0.9094,  0.9865],\n",
            "        [ 0.8327, -0.9860]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4625, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7892,  1.0476],\n",
            "        [ 0.5831, -0.3622],\n",
            "        [ 0.7916, -0.8906],\n",
            "        [ 0.7469, -0.9103],\n",
            "        [ 0.3954, -0.4799],\n",
            "        [ 0.4312, -0.4959],\n",
            "        [ 0.5975, -0.7683],\n",
            "        [ 0.8838, -1.0325],\n",
            "        [-0.9002,  1.0757],\n",
            "        [-0.9909,  1.0724],\n",
            "        [ 0.2964, -0.4572],\n",
            "        [-0.4647,  0.5513],\n",
            "        [ 0.3025, -0.1789],\n",
            "        [ 0.3758, -0.2103],\n",
            "        [ 0.7793, -0.8596],\n",
            "        [-1.0119,  1.1971],\n",
            "        [ 0.6496, -0.9619],\n",
            "        [ 0.7437, -0.9590],\n",
            "        [-1.1040,  1.3269],\n",
            "        [ 0.8963, -1.1529],\n",
            "        [ 0.4603, -0.5932],\n",
            "        [ 0.3453, -0.3103],\n",
            "        [ 0.2158, -0.3559],\n",
            "        [ 0.8105, -1.1955],\n",
            "        [ 0.2030, -0.3398],\n",
            "        [ 0.7623, -1.0233],\n",
            "        [-0.5381,  0.5663],\n",
            "        [-1.1299,  1.2841],\n",
            "        [ 0.6810, -0.9455],\n",
            "        [ 0.4157, -0.6586],\n",
            "        [ 0.4932, -0.4362],\n",
            "        [ 0.7990, -1.0244]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4717, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8339, -1.0422],\n",
            "        [ 0.6650, -0.9051],\n",
            "        [-0.9232,  1.4059],\n",
            "        [ 0.7749, -0.9342],\n",
            "        [ 0.7156, -1.2587],\n",
            "        [-0.3539,  0.7366],\n",
            "        [ 0.5777, -0.7155],\n",
            "        [ 0.6258, -0.9005],\n",
            "        [ 0.5483, -0.7116],\n",
            "        [ 0.6978, -0.7333],\n",
            "        [-0.9136,  1.2898],\n",
            "        [ 0.8850, -0.5029],\n",
            "        [-0.4263,  0.4616],\n",
            "        [ 0.6428, -0.9630],\n",
            "        [ 0.5797, -0.9582],\n",
            "        [ 0.4044,  0.0024],\n",
            "        [ 0.7292, -0.6584],\n",
            "        [ 0.7891, -1.0249],\n",
            "        [ 0.1595, -0.3150],\n",
            "        [ 0.6659, -1.1532],\n",
            "        [ 0.9860, -1.1074],\n",
            "        [-0.2573,  0.3785],\n",
            "        [ 0.2833, -0.1171],\n",
            "        [-1.3682,  1.2802],\n",
            "        [-1.1002,  1.3702],\n",
            "        [ 0.9275, -1.1874],\n",
            "        [-0.1161,  0.0489],\n",
            "        [ 0.8633, -0.9051],\n",
            "        [ 0.8258, -0.8452],\n",
            "        [ 0.7338, -0.7582],\n",
            "        [ 0.3675, -0.6124],\n",
            "        [ 0.7768, -0.8793]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8566, -0.9391],\n",
            "        [-1.3854,  1.3138],\n",
            "        [ 0.6298, -1.0154],\n",
            "        [-1.0665,  1.4053],\n",
            "        [-0.2575, -0.1485],\n",
            "        [ 0.5946, -1.0665],\n",
            "        [ 0.6974, -0.9520],\n",
            "        [-0.9372,  1.0022],\n",
            "        [-0.7634,  1.2908],\n",
            "        [ 0.6655, -0.8548],\n",
            "        [ 0.3323, -0.5156],\n",
            "        [ 0.5807, -0.9044],\n",
            "        [-1.2620,  1.5601],\n",
            "        [-1.3343,  1.6017],\n",
            "        [ 1.1768, -0.9773],\n",
            "        [ 0.4013, -0.8168],\n",
            "        [ 0.7138, -1.1825],\n",
            "        [ 0.3002, -0.1936],\n",
            "        [ 0.0789, -0.0552],\n",
            "        [ 0.6654, -1.0938],\n",
            "        [ 0.8272, -1.1479],\n",
            "        [ 0.0169,  0.2004],\n",
            "        [-1.2825,  1.3649],\n",
            "        [-1.3917,  1.4790],\n",
            "        [-1.3261,  1.3121],\n",
            "        [ 0.7269, -0.8793],\n",
            "        [-0.9590,  1.3213],\n",
            "        [ 0.9860, -0.9115],\n",
            "        [ 0.7330, -0.8545],\n",
            "        [-0.9490,  1.0606],\n",
            "        [ 0.1723, -0.1218],\n",
            "        [-1.1538,  0.7481]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3210,  0.3411],\n",
            "        [ 0.7434, -1.0822],\n",
            "        [ 0.9957, -1.3232],\n",
            "        [ 0.7787, -1.0300],\n",
            "        [ 0.6262, -0.7199],\n",
            "        [ 0.5599, -0.6677],\n",
            "        [ 0.6857, -0.6944],\n",
            "        [ 0.7687, -1.2036],\n",
            "        [-1.0713,  1.0321],\n",
            "        [ 0.7668, -1.1747],\n",
            "        [ 0.4289, -0.3273],\n",
            "        [ 0.9672, -0.8075],\n",
            "        [ 1.1129, -1.2960],\n",
            "        [ 0.6820, -0.7474],\n",
            "        [ 1.0127, -0.9303],\n",
            "        [ 0.1153, -0.1326],\n",
            "        [ 0.1180, -0.2852],\n",
            "        [ 0.8538, -0.9174],\n",
            "        [-0.5782,  0.5096],\n",
            "        [ 1.1963, -1.2368],\n",
            "        [ 0.5015, -0.0298],\n",
            "        [ 0.7208, -0.8166],\n",
            "        [ 0.7753, -1.1394],\n",
            "        [-1.1882,  1.1434],\n",
            "        [-0.2544, -0.1192],\n",
            "        [-0.7633,  0.8152],\n",
            "        [-1.1948,  1.0728],\n",
            "        [ 1.0017, -0.8424],\n",
            "        [ 0.9515, -1.1027],\n",
            "        [-1.0974,  1.4985],\n",
            "        [-0.1669,  0.2596],\n",
            "        [-1.2261,  1.3325]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8525, -1.2277],\n",
            "        [-0.8552,  1.0875],\n",
            "        [ 0.6438, -0.9257],\n",
            "        [-1.0877,  1.1129],\n",
            "        [ 0.5291, -0.5280],\n",
            "        [ 0.5435, -0.7448],\n",
            "        [ 0.8975, -1.0304],\n",
            "        [ 1.0937, -0.9115],\n",
            "        [ 0.9501, -0.8306],\n",
            "        [ 0.9754, -1.0578],\n",
            "        [ 0.4652, -0.5376],\n",
            "        [ 0.6985, -0.8603],\n",
            "        [-0.7126,  1.1578],\n",
            "        [ 0.6236, -0.5712],\n",
            "        [-1.1387,  1.3861],\n",
            "        [-0.7638,  0.8512],\n",
            "        [ 0.5240, -0.7485],\n",
            "        [ 0.8170, -1.2804],\n",
            "        [ 0.9529, -0.8447],\n",
            "        [ 0.9006, -0.7388],\n",
            "        [-0.2708,  0.4432],\n",
            "        [-0.1846, -0.0840],\n",
            "        [-0.4424,  0.3369],\n",
            "        [-0.5590,  0.4915],\n",
            "        [ 0.8974, -1.0694],\n",
            "        [-0.9381,  1.1561],\n",
            "        [ 1.0328, -0.8995],\n",
            "        [ 0.0895,  0.1267],\n",
            "        [ 0.7514, -0.9007],\n",
            "        [-0.5790,  0.8555],\n",
            "        [-1.0588,  1.1498],\n",
            "        [ 0.3055, -0.1990]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2585, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1017,  1.3284],\n",
            "        [-1.2212,  1.3148],\n",
            "        [ 0.3640, -0.4140],\n",
            "        [ 0.9712, -1.1031],\n",
            "        [ 0.9364, -1.0416],\n",
            "        [ 0.2459, -0.4777],\n",
            "        [-1.1713,  1.4236],\n",
            "        [ 0.7855, -0.9608],\n",
            "        [ 0.5961, -0.7349],\n",
            "        [-1.0801,  1.1447],\n",
            "        [-0.9825,  1.1559],\n",
            "        [-1.1712,  1.3097],\n",
            "        [ 0.8487, -1.2357],\n",
            "        [ 0.8081, -0.5628],\n",
            "        [ 0.6092, -0.8212],\n",
            "        [ 0.0110,  0.2662],\n",
            "        [ 0.2371,  0.0111],\n",
            "        [ 0.1796, -0.2053],\n",
            "        [-0.2439,  0.4839],\n",
            "        [-1.1982,  1.2357],\n",
            "        [ 0.7441, -0.9135],\n",
            "        [-0.7258,  0.6960],\n",
            "        [-0.5292,  0.8384],\n",
            "        [-0.1316,  0.3979],\n",
            "        [ 0.8736, -1.0726],\n",
            "        [-1.2752,  1.5657],\n",
            "        [ 0.9142, -0.9882],\n",
            "        [-1.1577,  1.0990],\n",
            "        [-1.1527,  1.0737],\n",
            "        [ 0.6353, -0.8082],\n",
            "        [ 0.5477, -0.6647],\n",
            "        [-0.7958,  0.9501]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0535, -1.2775],\n",
            "        [ 0.7183, -1.0268],\n",
            "        [-1.0717,  0.9869],\n",
            "        [ 0.6783, -0.7259],\n",
            "        [ 1.0016, -0.9814],\n",
            "        [ 0.8490, -1.2125],\n",
            "        [ 0.0131,  0.3284],\n",
            "        [-1.1646,  1.4620],\n",
            "        [ 0.3077, -0.2690],\n",
            "        [-1.5407,  1.2620],\n",
            "        [ 0.6032, -0.9523],\n",
            "        [-1.4892,  1.4300],\n",
            "        [-0.9922,  1.2356],\n",
            "        [-0.5038,  0.5017],\n",
            "        [ 0.1266,  0.1439],\n",
            "        [-0.9370,  1.0125],\n",
            "        [-1.4249,  1.3416],\n",
            "        [-0.3469,  0.4063],\n",
            "        [ 0.4519, -0.2998],\n",
            "        [-0.9328,  1.2131],\n",
            "        [ 0.9259, -1.0028],\n",
            "        [ 1.0463, -1.0369],\n",
            "        [-1.0632,  1.2453],\n",
            "        [ 0.7434, -0.8767],\n",
            "        [ 0.6917, -0.8454],\n",
            "        [-0.3078,  0.7711],\n",
            "        [-0.7308,  1.0914],\n",
            "        [-1.1050,  1.3235],\n",
            "        [ 0.8203, -1.0014],\n",
            "        [-1.1752,  1.6014],\n",
            "        [ 0.5853, -1.0371],\n",
            "        [-1.2019,  1.3217]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2848, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1061,  0.0660],\n",
            "        [ 0.7793, -0.5734],\n",
            "        [ 0.4341, -0.6232],\n",
            "        [-0.8150,  1.1607],\n",
            "        [ 0.8572, -1.0643],\n",
            "        [ 0.6200, -0.8283],\n",
            "        [ 0.7502, -0.9757],\n",
            "        [ 0.3763, -0.2437],\n",
            "        [ 0.5344, -0.9892],\n",
            "        [ 0.8196, -0.9901],\n",
            "        [ 0.8847, -0.9633],\n",
            "        [-0.0937,  0.0901],\n",
            "        [-0.7666,  1.0582],\n",
            "        [-1.1020,  1.2024],\n",
            "        [-1.1927,  1.4730],\n",
            "        [-1.0554,  1.0964],\n",
            "        [ 0.4517, -0.4482],\n",
            "        [-0.0370,  0.0425],\n",
            "        [ 0.1223, -0.2946],\n",
            "        [-1.3546,  1.4322],\n",
            "        [ 0.4206, -0.7301],\n",
            "        [ 0.2934, -0.4022],\n",
            "        [ 0.8866, -1.0353],\n",
            "        [ 0.5093, -0.8631],\n",
            "        [ 0.6448, -0.7723],\n",
            "        [ 1.0207, -0.9509],\n",
            "        [ 1.1492, -1.0490],\n",
            "        [-1.3025,  1.6616],\n",
            "        [ 0.8483, -0.9081],\n",
            "        [-0.9357,  1.1348],\n",
            "        [-1.5081,  1.4553],\n",
            "        [-1.3091,  1.4476]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4596, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5609, -0.7466],\n",
            "        [ 0.6890, -0.9352],\n",
            "        [-1.2177,  1.4342],\n",
            "        [ 1.0115, -0.8573],\n",
            "        [-0.2090,  0.7345],\n",
            "        [ 0.6290, -1.1445],\n",
            "        [-0.2054, -0.1158],\n",
            "        [ 0.8620, -1.2549],\n",
            "        [ 0.8619, -1.0112],\n",
            "        [-1.0302,  1.1674],\n",
            "        [ 0.8069, -0.9936],\n",
            "        [-1.3347,  1.3266],\n",
            "        [ 0.9275, -0.9000],\n",
            "        [-0.8817,  1.0276],\n",
            "        [ 0.1950, -0.5455],\n",
            "        [-0.4765,  0.6008],\n",
            "        [ 0.2044,  0.1376],\n",
            "        [ 0.8145, -1.0574],\n",
            "        [ 0.7470, -0.9188],\n",
            "        [ 0.7567, -0.8469],\n",
            "        [ 0.8772, -1.0950],\n",
            "        [-1.0300,  1.2339],\n",
            "        [-1.3122,  1.4622],\n",
            "        [-0.0121, -0.2736],\n",
            "        [-1.0765,  1.2985],\n",
            "        [ 0.7536, -1.1476],\n",
            "        [-0.8990,  0.8935],\n",
            "        [-1.1324,  1.3848],\n",
            "        [ 0.5483, -0.3717],\n",
            "        [ 0.3726, -0.5806],\n",
            "        [ 0.9309, -1.0674],\n",
            "        [ 0.7945, -0.8198]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6569, -0.9115],\n",
            "        [ 0.5827, -0.9854],\n",
            "        [ 0.7529, -0.9887],\n",
            "        [-1.1666,  1.3934],\n",
            "        [-0.4564,  0.6922],\n",
            "        [-0.5411,  0.4485],\n",
            "        [ 0.2018, -0.3130],\n",
            "        [ 0.7816, -0.8480],\n",
            "        [-1.2700,  1.2412],\n",
            "        [-0.5046,  0.5916],\n",
            "        [-0.3319,  0.4645],\n",
            "        [ 0.4996, -0.4219],\n",
            "        [ 0.4997, -0.9803],\n",
            "        [ 0.8701, -1.0949],\n",
            "        [ 0.0253, -0.1240],\n",
            "        [ 1.0268, -1.1368],\n",
            "        [ 0.9476, -0.8738],\n",
            "        [-0.7501,  1.1638],\n",
            "        [ 0.4088,  0.0214],\n",
            "        [ 0.3925, -0.4053],\n",
            "        [-0.1043,  0.2112],\n",
            "        [-0.0964,  0.4332],\n",
            "        [ 0.9169, -0.8842],\n",
            "        [ 0.1414, -0.4444],\n",
            "        [ 0.7531, -0.7800],\n",
            "        [-0.8701,  0.9031],\n",
            "        [ 0.2820, -0.7169],\n",
            "        [ 0.6395, -1.1667],\n",
            "        [ 0.7037, -1.0336],\n",
            "        [ 0.6441, -1.0783],\n",
            "        [ 0.8823, -1.0557],\n",
            "        [ 0.7191, -1.2820]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8637,  1.3083],\n",
            "        [ 0.9610, -0.9912],\n",
            "        [ 0.8617, -1.0188],\n",
            "        [ 0.8301, -0.7700],\n",
            "        [ 0.2284, -0.0978],\n",
            "        [-0.4920,  0.7144],\n",
            "        [ 0.2374, -0.0614],\n",
            "        [ 0.5982, -0.8437],\n",
            "        [-1.2824,  1.3405],\n",
            "        [ 0.5769, -0.9727],\n",
            "        [ 0.6484, -0.6921],\n",
            "        [ 0.8317, -0.9642],\n",
            "        [ 0.9362, -1.1458],\n",
            "        [ 0.7796, -0.7999],\n",
            "        [-0.8595,  1.1581],\n",
            "        [ 0.5801, -0.8639],\n",
            "        [ 0.5437, -0.9209],\n",
            "        [ 0.8996, -1.2957],\n",
            "        [ 0.7478, -0.9995],\n",
            "        [ 0.9627, -1.2309],\n",
            "        [-1.0760,  1.2344],\n",
            "        [ 0.6903, -0.7982],\n",
            "        [-1.1512,  1.6099],\n",
            "        [ 1.0482, -1.1132],\n",
            "        [ 0.5980, -0.7122],\n",
            "        [ 0.6307, -0.9811],\n",
            "        [-1.0243,  1.0794],\n",
            "        [ 1.0944, -0.8466],\n",
            "        [ 0.9435, -0.7704],\n",
            "        [ 0.7171, -1.0128],\n",
            "        [ 0.1899, -0.4146],\n",
            "        [ 0.8576, -0.8454]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1975,  1.3192],\n",
            "        [-1.2551,  1.1131],\n",
            "        [ 0.9059, -1.0580],\n",
            "        [ 0.7543, -1.1613],\n",
            "        [ 0.9767, -1.0812],\n",
            "        [ 0.6731, -1.0840],\n",
            "        [ 0.7025, -0.7917],\n",
            "        [-1.0467,  1.5155],\n",
            "        [ 0.6199, -0.6732],\n",
            "        [ 1.1847, -1.0728],\n",
            "        [-1.1563,  1.1837],\n",
            "        [ 0.8951, -1.2503],\n",
            "        [-0.9507,  1.1009],\n",
            "        [ 0.2048, -0.5058],\n",
            "        [-0.2016,  0.3835],\n",
            "        [ 0.7029, -0.8799],\n",
            "        [-0.4356,  0.7371],\n",
            "        [ 0.8855, -1.0713],\n",
            "        [ 0.6208, -0.9115],\n",
            "        [ 0.7093, -1.1366],\n",
            "        [-1.1406,  1.4841],\n",
            "        [ 1.0929, -1.0958],\n",
            "        [ 0.7454, -0.4886],\n",
            "        [-1.2840,  1.5910],\n",
            "        [-1.1422,  1.4870],\n",
            "        [ 0.7708, -0.9873],\n",
            "        [-0.7539,  0.6515],\n",
            "        [-1.1210,  1.4652],\n",
            "        [ 1.0947, -1.1547],\n",
            "        [-0.9233,  0.6695],\n",
            "        [-0.7317,  1.0043],\n",
            "        [-0.4664,  0.3136]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1815, -1.0865],\n",
            "        [-0.4521,  0.5876],\n",
            "        [ 0.8943, -0.9630],\n",
            "        [ 0.5395, -0.8666],\n",
            "        [ 0.7909, -0.9674],\n",
            "        [-0.0709,  0.4632],\n",
            "        [-1.2103,  1.3306],\n",
            "        [ 0.7834, -1.2055],\n",
            "        [-0.6132,  0.5079],\n",
            "        [ 0.7096, -1.0694],\n",
            "        [ 0.4195, -0.8650],\n",
            "        [-0.6982,  0.7932],\n",
            "        [ 0.8106, -1.0077],\n",
            "        [-0.4399,  0.6315],\n",
            "        [ 0.7976, -0.7433],\n",
            "        [-0.6238,  1.0890],\n",
            "        [ 0.9652, -1.0185],\n",
            "        [ 0.5330, -0.8570],\n",
            "        [-1.0728,  1.1463],\n",
            "        [ 0.8686, -0.9251],\n",
            "        [-1.0994,  1.1527],\n",
            "        [ 1.1077, -0.9165],\n",
            "        [ 0.8301, -1.2550],\n",
            "        [ 1.0979, -0.7387],\n",
            "        [ 0.7341, -1.1171],\n",
            "        [-0.9946,  1.1515],\n",
            "        [-0.4910,  0.7857],\n",
            "        [ 0.8760, -1.0079],\n",
            "        [-1.4035,  1.4818],\n",
            "        [-0.9731,  1.2131],\n",
            "        [-0.9370,  0.9991],\n",
            "        [-1.4603,  1.7218]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3751, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9779, -1.2157],\n",
            "        [ 0.7547, -0.7504],\n",
            "        [-1.4521,  1.4799],\n",
            "        [ 0.5729, -0.7418],\n",
            "        [ 0.5968, -1.0230],\n",
            "        [-0.9119,  1.2389],\n",
            "        [ 0.9374, -0.9831],\n",
            "        [ 0.7727, -1.0065],\n",
            "        [ 0.6121, -0.9818],\n",
            "        [ 0.8202, -1.0472],\n",
            "        [-0.9921,  0.9880],\n",
            "        [ 0.5852, -0.6953],\n",
            "        [-0.2779,  0.3488],\n",
            "        [-0.1977,  0.2158],\n",
            "        [ 0.8088, -0.7697],\n",
            "        [ 0.5683, -0.7470],\n",
            "        [-1.4395,  1.5820],\n",
            "        [-1.2642,  1.4076],\n",
            "        [-0.7401,  0.9156],\n",
            "        [ 0.4410, -0.2980],\n",
            "        [ 0.8981, -0.7755],\n",
            "        [-1.0172,  1.3211],\n",
            "        [ 0.7804, -1.0039],\n",
            "        [-1.1442,  1.5277],\n",
            "        [-1.4162,  1.1939],\n",
            "        [ 0.5068, -0.7973],\n",
            "        [ 0.8920, -0.9467],\n",
            "        [-0.6224,  0.6531],\n",
            "        [ 0.9351, -1.1232],\n",
            "        [-0.1965,  0.2518],\n",
            "        [ 0.9108, -0.7170],\n",
            "        [ 0.9319, -0.9791]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8136, -0.8819],\n",
            "        [ 1.0950, -1.1649],\n",
            "        [ 0.6585, -0.4307],\n",
            "        [ 0.8411, -0.9881],\n",
            "        [ 0.1981, -0.3872],\n",
            "        [ 0.7025, -0.7823],\n",
            "        [ 0.4008, -0.8593],\n",
            "        [ 0.8785, -1.1421],\n",
            "        [ 0.5501, -0.5602],\n",
            "        [ 0.6945, -1.0554],\n",
            "        [ 0.0025,  0.2382],\n",
            "        [ 0.8369, -1.0943],\n",
            "        [ 0.7208, -1.0168],\n",
            "        [ 0.7412, -1.0688],\n",
            "        [ 0.9622, -1.0503],\n",
            "        [-0.1927,  0.4995],\n",
            "        [ 0.7578, -1.1807],\n",
            "        [-0.7375,  1.1285],\n",
            "        [-0.2798,  0.2111],\n",
            "        [ 0.7653, -0.9053],\n",
            "        [ 0.6937, -1.1125],\n",
            "        [-1.3081,  1.3021],\n",
            "        [-1.2477,  1.3743],\n",
            "        [-0.0262,  0.0735],\n",
            "        [ 0.4744, -0.4824],\n",
            "        [ 0.1194,  0.2067],\n",
            "        [ 0.6889, -0.8028],\n",
            "        [ 0.7535, -0.9398],\n",
            "        [ 0.7738, -1.0219],\n",
            "        [ 0.8581, -1.0523],\n",
            "        [-1.2273,  1.2564],\n",
            "        [ 1.1341, -0.9155]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8891,  0.7452],\n",
            "        [ 0.9569, -1.0743],\n",
            "        [ 0.7114, -0.9608],\n",
            "        [-0.2201, -0.0264],\n",
            "        [-1.3222,  1.3133],\n",
            "        [ 0.7109, -0.8628],\n",
            "        [ 0.7330, -0.9169],\n",
            "        [-0.9539,  1.3019],\n",
            "        [-1.2862,  1.3501],\n",
            "        [ 0.9251, -0.7485],\n",
            "        [-0.8807,  0.9056],\n",
            "        [ 0.7781, -0.5725],\n",
            "        [ 0.8856, -0.8191],\n",
            "        [ 0.1847,  0.0100],\n",
            "        [ 0.7137, -1.0313],\n",
            "        [ 0.7634, -0.7969],\n",
            "        [ 0.5492, -0.9277],\n",
            "        [ 0.3650, -0.8487],\n",
            "        [-0.2021,  0.1221],\n",
            "        [ 0.4411, -0.5732],\n",
            "        [-1.3182,  1.2525],\n",
            "        [ 0.9224, -1.0802],\n",
            "        [ 0.8615, -0.9806],\n",
            "        [ 0.6748, -1.0770],\n",
            "        [-0.9505,  1.1176],\n",
            "        [ 0.0856, -0.0570],\n",
            "        [ 1.1458, -1.1591],\n",
            "        [-0.4067,  0.3971],\n",
            "        [-0.0493,  0.0726],\n",
            "        [ 0.1577, -0.6065],\n",
            "        [-0.5165,  0.6552],\n",
            "        [ 0.5765, -1.0920]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9557, -1.3021],\n",
            "        [ 0.4491, -0.7919],\n",
            "        [ 0.7834, -0.7267],\n",
            "        [ 0.5786, -0.3577],\n",
            "        [ 0.7772, -1.0171],\n",
            "        [ 0.7761, -1.0460],\n",
            "        [ 0.5760, -1.0371],\n",
            "        [ 0.5840, -0.8651],\n",
            "        [ 0.2692, -0.1427],\n",
            "        [ 0.2929, -0.5296],\n",
            "        [-0.9326,  0.9677],\n",
            "        [ 1.0198, -1.2099],\n",
            "        [-1.1132,  1.0613],\n",
            "        [ 0.8835, -1.0023],\n",
            "        [-0.3446,  0.4071],\n",
            "        [ 1.1612, -0.7319],\n",
            "        [-0.5194,  1.0122],\n",
            "        [ 0.9716, -0.9686],\n",
            "        [-0.2437,  0.2092],\n",
            "        [ 0.8340, -1.1886],\n",
            "        [-0.0458,  0.1478],\n",
            "        [ 0.7504, -1.0321],\n",
            "        [-1.2811,  1.3788],\n",
            "        [-0.1306,  0.3931],\n",
            "        [ 0.9858, -1.0038],\n",
            "        [ 0.8344, -1.0771],\n",
            "        [-0.4816,  0.1439],\n",
            "        [-1.3505,  1.3506],\n",
            "        [-1.2481,  1.0241],\n",
            "        [-1.1196,  1.2604],\n",
            "        [ 0.2734, -0.2315],\n",
            "        [ 0.7118, -0.6539]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0490,  1.1579],\n",
            "        [-1.3394,  1.2625],\n",
            "        [ 0.5928, -0.9303],\n",
            "        [ 0.6022, -0.7980],\n",
            "        [ 0.3061, -0.2783],\n",
            "        [ 0.5551, -0.7182],\n",
            "        [ 0.8779, -1.1005],\n",
            "        [ 0.1754,  0.1032],\n",
            "        [ 0.8043, -1.0231],\n",
            "        [ 0.9097, -1.1909],\n",
            "        [ 0.7777, -0.8122],\n",
            "        [-0.6509,  0.7927],\n",
            "        [ 0.7247, -1.0485],\n",
            "        [ 1.0248, -1.0166],\n",
            "        [ 1.0283, -1.4030],\n",
            "        [ 0.0696, -0.2187],\n",
            "        [-0.5703,  0.6060],\n",
            "        [ 0.6138, -1.0608],\n",
            "        [ 0.7596, -0.9825],\n",
            "        [-0.7026,  0.6624],\n",
            "        [ 0.6038, -0.8961],\n",
            "        [-1.4515,  1.5253],\n",
            "        [-1.2666,  1.0286],\n",
            "        [ 0.5969, -0.6961],\n",
            "        [ 0.6725, -0.9817],\n",
            "        [-0.7353,  1.0496],\n",
            "        [ 0.7918, -1.0851],\n",
            "        [-1.2388,  1.3983],\n",
            "        [ 0.2295, -0.3742],\n",
            "        [-1.3932,  1.3256],\n",
            "        [-0.8436,  0.8683],\n",
            "        [-0.8291,  0.9140]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7028, -1.0622],\n",
            "        [-1.1554,  1.3740],\n",
            "        [-1.3299,  1.4497],\n",
            "        [ 0.6128, -0.6355],\n",
            "        [ 0.8731, -1.0815],\n",
            "        [-1.3033,  1.2215],\n",
            "        [ 1.0010, -1.2910],\n",
            "        [ 0.5165, -0.7064],\n",
            "        [-1.1715,  1.4483],\n",
            "        [ 0.5346, -0.7703],\n",
            "        [ 0.8125, -0.9483],\n",
            "        [ 0.5983, -1.0192],\n",
            "        [ 0.7019, -1.2357],\n",
            "        [-0.2231,  0.2498],\n",
            "        [ 0.6864, -0.7319],\n",
            "        [ 0.1540,  0.0231],\n",
            "        [ 0.9056, -1.2349],\n",
            "        [ 0.2466, -0.0752],\n",
            "        [-0.9005,  0.9009],\n",
            "        [-1.4989,  1.3948],\n",
            "        [-1.0261,  1.4765],\n",
            "        [ 0.5880, -0.8938],\n",
            "        [-1.1877,  1.3104],\n",
            "        [-1.1132,  1.5255],\n",
            "        [-0.2079, -0.2385],\n",
            "        [ 0.7243, -0.9931],\n",
            "        [ 0.6559, -0.8097],\n",
            "        [ 0.7338, -0.9112],\n",
            "        [ 0.9652, -1.1269],\n",
            "        [ 0.8680, -1.0545],\n",
            "        [-1.1674,  1.1954],\n",
            "        [ 0.7498, -0.9604]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4865, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7331, -1.1269],\n",
            "        [ 0.8304, -1.0444],\n",
            "        [ 0.9179, -1.0821],\n",
            "        [ 0.3917, -0.4962],\n",
            "        [ 0.8552, -1.0099],\n",
            "        [-0.9815,  0.8007],\n",
            "        [ 0.6023, -1.1520],\n",
            "        [ 0.7425, -1.2682],\n",
            "        [ 0.8304, -1.1220],\n",
            "        [-0.8565,  1.0793],\n",
            "        [ 0.6939, -0.8221],\n",
            "        [ 0.7338, -0.9837],\n",
            "        [ 0.0265, -0.3128],\n",
            "        [ 0.9175, -1.0859],\n",
            "        [-1.1114,  1.1481],\n",
            "        [-1.2001,  1.1908],\n",
            "        [ 0.9170, -0.9617],\n",
            "        [-0.4322,  0.6737],\n",
            "        [ 0.8194, -1.0316],\n",
            "        [-1.2833,  1.4546],\n",
            "        [ 0.5993, -0.9333],\n",
            "        [ 0.2250, -0.2613],\n",
            "        [ 0.7613, -0.8485],\n",
            "        [-1.3338,  1.4745],\n",
            "        [ 0.8903, -0.9765],\n",
            "        [ 0.8052, -0.6195],\n",
            "        [ 0.7137, -0.8201],\n",
            "        [-0.4369,  0.7467],\n",
            "        [-1.0113,  1.3178],\n",
            "        [-0.3787,  0.5191],\n",
            "        [ 1.0049, -1.1522],\n",
            "        [ 1.0973, -1.1524]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8550, -0.9607],\n",
            "        [-1.3379,  1.4400],\n",
            "        [ 1.1274, -1.0761],\n",
            "        [-0.9739,  1.2538],\n",
            "        [-1.3264,  1.6479],\n",
            "        [-0.2011,  0.0874],\n",
            "        [ 0.5089, -0.8364],\n",
            "        [ 0.8052, -1.0259],\n",
            "        [-0.0956, -0.0694],\n",
            "        [-1.1179,  0.9769],\n",
            "        [ 0.7345, -1.2173],\n",
            "        [-1.5108,  1.3815],\n",
            "        [ 0.8609, -1.1558],\n",
            "        [ 0.7830, -1.0492],\n",
            "        [-0.6227,  0.6852],\n",
            "        [ 0.1863, -0.1271],\n",
            "        [ 0.6995, -0.9964],\n",
            "        [-0.8927,  1.1717],\n",
            "        [-1.3568,  1.3294],\n",
            "        [-1.5203,  1.5651],\n",
            "        [ 0.7345, -1.1522],\n",
            "        [ 0.9102, -1.1001],\n",
            "        [-0.5830,  0.6536],\n",
            "        [-0.4785,  0.4631],\n",
            "        [-1.5063,  1.3497],\n",
            "        [ 0.6174, -0.8707],\n",
            "        [ 0.7978, -1.1957],\n",
            "        [ 0.7439, -0.9271],\n",
            "        [ 0.7142, -1.0514],\n",
            "        [-1.2000,  1.4863],\n",
            "        [ 0.0719, -0.5289],\n",
            "        [ 0.8595, -1.2914]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9523,  1.3652],\n",
            "        [ 0.7216, -0.7708],\n",
            "        [-0.3797,  0.4044],\n",
            "        [ 1.0086, -1.2259],\n",
            "        [ 0.9289, -0.9863],\n",
            "        [-0.5785,  0.6187],\n",
            "        [ 0.7572, -0.9708],\n",
            "        [ 0.6807, -1.0239],\n",
            "        [ 0.2682, -0.4319],\n",
            "        [-0.3801,  0.3998],\n",
            "        [ 0.7936, -0.8487],\n",
            "        [ 0.2679, -0.3509],\n",
            "        [ 0.8669, -1.1550],\n",
            "        [-0.1219, -0.1499],\n",
            "        [ 0.5590, -0.8338],\n",
            "        [ 0.6761, -0.9223],\n",
            "        [ 1.0045, -1.0084],\n",
            "        [ 0.8773, -0.9283],\n",
            "        [ 0.7872, -1.0361],\n",
            "        [ 0.8697, -1.0774],\n",
            "        [ 0.9545, -1.0058],\n",
            "        [ 0.6854, -0.8786],\n",
            "        [ 0.9595, -1.1244],\n",
            "        [ 0.7129, -1.1956],\n",
            "        [-1.3125,  1.3145],\n",
            "        [ 0.5076, -0.5220],\n",
            "        [ 0.4779, -0.8333],\n",
            "        [ 1.0858, -1.3977],\n",
            "        [ 0.7367, -0.9787],\n",
            "        [ 0.5894, -0.6059],\n",
            "        [ 0.9352, -1.2581],\n",
            "        [ 0.8205, -1.1353]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4590, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6015, -0.8451],\n",
            "        [ 0.8602, -1.0377],\n",
            "        [ 0.6558, -0.8461],\n",
            "        [ 0.5143, -0.7860],\n",
            "        [-1.1371,  1.1341],\n",
            "        [ 0.7904, -1.2131],\n",
            "        [-0.8511,  0.7931],\n",
            "        [ 1.0227, -0.8945],\n",
            "        [-1.2193,  1.1625],\n",
            "        [ 0.7821, -1.1045],\n",
            "        [ 0.7297, -0.9058],\n",
            "        [-1.1677,  1.3294],\n",
            "        [ 0.7145, -0.7655],\n",
            "        [ 0.4071, -0.6136],\n",
            "        [-1.3031,  1.4843],\n",
            "        [ 0.9650, -1.0148],\n",
            "        [-1.2583,  1.2134],\n",
            "        [-1.3674,  1.3184],\n",
            "        [ 0.6175, -1.1432],\n",
            "        [ 0.7516, -1.0088],\n",
            "        [-1.2329,  1.2536],\n",
            "        [ 0.6574, -0.7669],\n",
            "        [ 0.4629, -1.0304],\n",
            "        [-1.1573,  1.1364],\n",
            "        [-1.2021,  1.0901],\n",
            "        [ 1.0312, -1.2013],\n",
            "        [ 0.8293, -1.3031],\n",
            "        [ 0.6154, -0.9846],\n",
            "        [ 0.9521, -1.0903],\n",
            "        [-1.4009,  1.3662],\n",
            "        [ 0.3640, -0.8625],\n",
            "        [ 0.1671, -0.0940]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4187,  0.5147],\n",
            "        [ 0.9893, -0.9629],\n",
            "        [-0.0132,  0.2825],\n",
            "        [ 0.9109, -0.9258],\n",
            "        [ 0.5635, -0.7368],\n",
            "        [ 0.8279, -0.8256],\n",
            "        [ 0.7701, -1.3699],\n",
            "        [-1.1018,  1.1946],\n",
            "        [ 0.5135, -0.7730],\n",
            "        [ 0.9159, -1.0695],\n",
            "        [-1.2670,  1.5515],\n",
            "        [ 0.3861, -0.4236],\n",
            "        [-0.2399, -0.1525],\n",
            "        [ 0.8461, -1.0249],\n",
            "        [ 1.0289, -1.1959],\n",
            "        [ 0.3259, -0.5833],\n",
            "        [ 1.0151, -1.1172],\n",
            "        [-1.2651,  1.4032],\n",
            "        [ 0.8255, -0.7964],\n",
            "        [ 0.1135, -0.4087],\n",
            "        [-1.2391,  1.5282],\n",
            "        [-0.7937,  0.8869],\n",
            "        [-1.1737,  1.3163],\n",
            "        [-1.3464,  1.0171],\n",
            "        [ 0.7782, -0.9545],\n",
            "        [-1.2709,  1.5636],\n",
            "        [ 0.3850, -0.7585],\n",
            "        [ 0.4886, -0.5831],\n",
            "        [ 0.8425, -0.9969],\n",
            "        [ 0.4260, -1.0743],\n",
            "        [-1.3487,  1.1587],\n",
            "        [-0.0862,  0.0667]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3976,  0.7180],\n",
            "        [-0.2047,  0.0157],\n",
            "        [ 0.7343, -1.2441],\n",
            "        [-1.2695,  1.4900],\n",
            "        [-1.4538,  1.3314],\n",
            "        [-1.4152,  1.3709],\n",
            "        [-1.1149,  1.4578],\n",
            "        [ 0.8162, -0.9123],\n",
            "        [ 0.7522, -1.0990],\n",
            "        [ 0.5817, -0.8638],\n",
            "        [ 0.3039, -0.3592],\n",
            "        [ 0.8535, -1.0770],\n",
            "        [ 0.7129, -1.3259],\n",
            "        [-1.0475,  1.5356],\n",
            "        [-0.4102,  0.5725],\n",
            "        [-0.4586,  0.7297],\n",
            "        [-1.2078,  1.3550],\n",
            "        [ 0.7930, -1.1741],\n",
            "        [ 0.0583, -0.1266],\n",
            "        [ 0.9065, -0.9714],\n",
            "        [ 0.6432, -0.8765],\n",
            "        [-0.6982,  0.8543],\n",
            "        [-0.0324, -0.2047],\n",
            "        [ 0.5864, -0.6279],\n",
            "        [ 1.1108, -0.9231],\n",
            "        [-1.3917,  1.5729],\n",
            "        [ 0.4371, -0.6173],\n",
            "        [ 0.3152, -0.3972],\n",
            "        [ 0.5809, -0.9743],\n",
            "        [-1.1141,  1.2488],\n",
            "        [-0.3003, -0.1297],\n",
            "        [ 0.8430, -0.9743]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5782, -1.2052],\n",
            "        [-1.3373,  1.1353],\n",
            "        [ 0.1653, -0.6343],\n",
            "        [-0.8443,  0.7752],\n",
            "        [-0.6027,  0.6878],\n",
            "        [ 0.9163, -1.0376],\n",
            "        [ 0.4318, -0.8822],\n",
            "        [ 0.0023, -0.0747],\n",
            "        [ 0.4646, -1.0670],\n",
            "        [ 1.0529, -1.0664],\n",
            "        [-0.7504,  0.9815],\n",
            "        [ 0.3057, -0.4319],\n",
            "        [ 0.7643, -1.1770],\n",
            "        [ 0.9053, -0.8863],\n",
            "        [-1.1499,  1.4108],\n",
            "        [ 0.8775, -1.0336],\n",
            "        [-0.9918,  1.0488],\n",
            "        [-1.4259,  1.4099],\n",
            "        [ 0.3871, -0.6536],\n",
            "        [-1.0753,  1.2948],\n",
            "        [-1.1253,  1.2659],\n",
            "        [-0.6948,  1.0096],\n",
            "        [ 1.0789, -1.1875],\n",
            "        [-1.1339,  1.1150],\n",
            "        [-0.7760,  0.4892],\n",
            "        [ 0.9635, -1.2668],\n",
            "        [-1.0470,  1.2158],\n",
            "        [-1.3660,  1.3591],\n",
            "        [ 0.6711, -0.9395],\n",
            "        [ 0.8468, -0.8320],\n",
            "        [ 0.2896, -0.3482],\n",
            "        [ 1.0885, -0.9281]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5730, -0.7769],\n",
            "        [ 0.0554, -0.4703],\n",
            "        [ 0.9529, -0.9327],\n",
            "        [ 0.8927, -1.0447],\n",
            "        [ 0.6504, -0.9709],\n",
            "        [ 0.7560, -0.9740],\n",
            "        [-0.0061, -0.2529],\n",
            "        [-0.8358,  0.9064],\n",
            "        [ 0.6902, -1.0285],\n",
            "        [-0.0375, -0.1235],\n",
            "        [ 0.6878, -1.2602],\n",
            "        [ 0.3450, -0.9485],\n",
            "        [-1.0769,  1.3690],\n",
            "        [ 0.9409, -1.0403],\n",
            "        [-1.1721,  0.8631],\n",
            "        [-0.4257,  0.2806],\n",
            "        [-0.7355,  0.7436],\n",
            "        [ 1.1631, -1.2250],\n",
            "        [ 0.7785, -1.1580],\n",
            "        [-1.3299,  1.3710],\n",
            "        [-0.3002,  0.2241],\n",
            "        [-1.2609,  1.4925],\n",
            "        [-1.3883,  1.5882],\n",
            "        [ 0.9603, -1.2567],\n",
            "        [ 0.8946, -1.1326],\n",
            "        [ 0.6828, -1.0007],\n",
            "        [-1.4446,  1.3840],\n",
            "        [-1.2314,  0.9871],\n",
            "        [ 0.7253, -1.0970],\n",
            "        [ 0.9017, -1.1716],\n",
            "        [ 0.9121, -1.3139],\n",
            "        [ 0.7190, -0.5144]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2046e+00,  1.3850e+00],\n",
            "        [-1.4300e+00,  1.4571e+00],\n",
            "        [-1.2786e+00,  1.1569e+00],\n",
            "        [ 7.2101e-01, -1.3592e+00],\n",
            "        [-1.1982e+00,  1.3860e+00],\n",
            "        [-1.5396e+00,  1.3328e+00],\n",
            "        [ 4.8334e-02, -2.9402e-01],\n",
            "        [ 7.5979e-01, -1.0592e+00],\n",
            "        [ 7.4621e-01, -8.6381e-01],\n",
            "        [ 8.2846e-04, -3.1093e-01],\n",
            "        [ 8.0344e-01, -8.3711e-01],\n",
            "        [-9.1090e-01,  9.6090e-01],\n",
            "        [ 9.8360e-01, -1.1082e+00],\n",
            "        [ 7.5978e-01, -7.3686e-01],\n",
            "        [-1.4840e+00,  1.3865e+00],\n",
            "        [ 3.5268e-01, -5.3567e-01],\n",
            "        [ 7.7915e-01, -7.3420e-01],\n",
            "        [ 8.2536e-01, -1.0295e+00],\n",
            "        [-7.5484e-01,  6.3891e-01],\n",
            "        [ 3.2686e-01, -5.7450e-01],\n",
            "        [ 7.4608e-01, -1.0710e+00],\n",
            "        [ 4.7719e-01, -8.0493e-01],\n",
            "        [ 5.2714e-01, -6.5520e-01],\n",
            "        [-1.3603e+00,  1.2276e+00],\n",
            "        [-1.3371e+00,  1.2799e+00],\n",
            "        [-1.2209e+00,  1.6571e+00],\n",
            "        [-1.2025e+00,  1.6020e+00],\n",
            "        [ 6.6965e-01, -1.0997e+00],\n",
            "        [-1.4356e+00,  1.1170e+00],\n",
            "        [ 4.7364e-01, -7.8194e-01],\n",
            "        [-8.4684e-01,  1.0425e+00],\n",
            "        [ 5.7574e-01, -1.2248e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5286, -0.7613],\n",
            "        [-1.0255,  1.4304],\n",
            "        [ 0.5621, -0.8535],\n",
            "        [ 0.5924, -1.0106],\n",
            "        [ 0.5744, -1.0088],\n",
            "        [-1.3646,  1.4653],\n",
            "        [ 0.7622, -1.0338],\n",
            "        [ 0.7097, -1.0090],\n",
            "        [-1.5923,  1.2754],\n",
            "        [-1.2708,  1.3391],\n",
            "        [-1.0377,  1.3009],\n",
            "        [-0.2071, -0.0021],\n",
            "        [ 0.6614, -0.8879],\n",
            "        [-1.4725,  1.4469],\n",
            "        [-0.2873,  0.4406],\n",
            "        [ 0.0228,  0.3134],\n",
            "        [-0.3877,  0.3549],\n",
            "        [ 0.9157, -1.0853],\n",
            "        [-1.4040,  1.1699],\n",
            "        [ 0.9032, -1.1674],\n",
            "        [ 0.9414, -1.0864],\n",
            "        [-0.2212, -0.1872],\n",
            "        [ 0.8220, -1.0955],\n",
            "        [ 0.8514, -1.1198],\n",
            "        [ 0.8258, -1.2395],\n",
            "        [-0.3806,  0.0332],\n",
            "        [ 0.9732, -0.7935],\n",
            "        [-1.1999,  1.4862],\n",
            "        [-1.4043,  1.4971],\n",
            "        [ 0.6822, -1.0250],\n",
            "        [ 0.4208, -0.7287],\n",
            "        [ 0.8750, -0.8874]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4209, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5682, -0.6079],\n",
            "        [-1.1947,  0.9485],\n",
            "        [-0.9896,  0.7317],\n",
            "        [ 0.8293, -1.1526],\n",
            "        [-1.0175,  1.3176],\n",
            "        [-0.1924, -0.0769],\n",
            "        [-0.7300,  0.7019],\n",
            "        [-1.0922,  1.1180],\n",
            "        [-1.4246,  1.3712],\n",
            "        [-1.3194,  1.1526],\n",
            "        [-0.0174,  0.3334],\n",
            "        [ 0.7728, -0.8257],\n",
            "        [-1.1708,  1.4725],\n",
            "        [-0.0844, -0.4562],\n",
            "        [ 0.4556, -0.8338],\n",
            "        [ 0.7611, -0.8956],\n",
            "        [ 0.5053, -0.6513],\n",
            "        [-1.1670,  1.0296],\n",
            "        [ 0.0105, -0.3919],\n",
            "        [-1.2075,  1.3927],\n",
            "        [ 0.4310, -0.1739],\n",
            "        [ 1.0371, -1.1203],\n",
            "        [ 0.7205, -0.7730],\n",
            "        [ 0.8926, -1.0131],\n",
            "        [-1.2326,  1.2588],\n",
            "        [-1.2119,  1.3847],\n",
            "        [ 0.3579, -0.7961],\n",
            "        [ 0.3849, -0.3403],\n",
            "        [ 0.7215, -0.9893],\n",
            "        [ 0.6761, -0.9939],\n",
            "        [ 0.2344, -0.6565],\n",
            "        [-1.0730,  1.2426]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3399,  0.2638],\n",
            "        [-1.2367,  1.0001],\n",
            "        [ 0.7827, -1.2506],\n",
            "        [ 0.6943, -0.8868],\n",
            "        [ 0.6861, -1.0177],\n",
            "        [ 0.6967, -1.1325],\n",
            "        [ 0.8069, -0.8465],\n",
            "        [-1.3493,  1.6102],\n",
            "        [-1.1732,  1.5199],\n",
            "        [ 0.2341, -0.7207],\n",
            "        [ 0.1723, -0.2714],\n",
            "        [ 0.3946, -0.5872],\n",
            "        [ 0.9386, -0.6472],\n",
            "        [-0.8989,  0.8030],\n",
            "        [ 0.6980, -0.8721],\n",
            "        [-1.1701,  1.4144],\n",
            "        [-1.3882,  1.1735],\n",
            "        [-1.3535,  1.6871],\n",
            "        [ 0.9188, -0.9534],\n",
            "        [ 0.3076, -0.5217],\n",
            "        [ 0.3950, -0.7672],\n",
            "        [-1.2700,  1.2952],\n",
            "        [ 0.4572, -0.6287],\n",
            "        [-0.5760,  0.8225],\n",
            "        [ 0.0173, -0.3209],\n",
            "        [-0.9478,  1.1201],\n",
            "        [-0.0238, -0.3928],\n",
            "        [ 0.3361, -0.7758],\n",
            "        [-1.5020,  1.1812],\n",
            "        [ 0.5847, -0.8100],\n",
            "        [-1.2800,  1.3874],\n",
            "        [-0.7055,  0.5000]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8759,  1.0861],\n",
            "        [-0.6920,  0.7441],\n",
            "        [ 1.1858, -1.2309],\n",
            "        [ 0.1291, -0.4280],\n",
            "        [ 0.8952, -1.1130],\n",
            "        [ 0.1110, -0.4657],\n",
            "        [ 0.5168, -1.0380],\n",
            "        [-0.1978,  0.3664],\n",
            "        [-1.5692,  1.3938],\n",
            "        [ 0.6453, -1.0227],\n",
            "        [ 0.0305,  0.1118],\n",
            "        [ 0.4504, -1.2317],\n",
            "        [ 0.6345, -0.7044],\n",
            "        [ 0.7989, -0.9193],\n",
            "        [ 0.7516, -1.0754],\n",
            "        [ 0.6542, -0.7777],\n",
            "        [ 0.4192, -0.7169],\n",
            "        [ 0.4679, -0.5005],\n",
            "        [-0.2238, -0.0687],\n",
            "        [ 0.1951, -0.8373],\n",
            "        [-1.2454,  1.3784],\n",
            "        [ 0.5558, -1.0233],\n",
            "        [-1.3853,  1.4440],\n",
            "        [ 0.6621, -1.2395],\n",
            "        [ 0.9486, -0.9203],\n",
            "        [-1.5500,  1.5158],\n",
            "        [-0.0955,  0.0977],\n",
            "        [-0.7403,  0.9317],\n",
            "        [-0.1363,  0.0400],\n",
            "        [ 0.0286, -0.4382],\n",
            "        [-1.3214,  1.3148],\n",
            "        [-1.4243,  1.1446]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8912,  1.4161],\n",
            "        [-1.2916,  1.5394],\n",
            "        [ 0.8235, -1.0776],\n",
            "        [-1.3326,  1.3418],\n",
            "        [ 0.4034, -0.7744],\n",
            "        [ 0.3150, -0.5063],\n",
            "        [ 0.3882, -0.5252],\n",
            "        [-0.8499,  0.8657],\n",
            "        [ 0.8629, -0.9286],\n",
            "        [ 0.4496, -0.9831],\n",
            "        [ 0.8989, -1.1198],\n",
            "        [ 0.0479,  0.2483],\n",
            "        [ 0.7647, -1.1098],\n",
            "        [-1.3173,  1.3621],\n",
            "        [ 0.9541, -1.1151],\n",
            "        [ 0.6692, -0.9251],\n",
            "        [ 0.7748, -0.9111],\n",
            "        [ 0.9387, -1.2156],\n",
            "        [-0.6416,  0.6189],\n",
            "        [-1.2708,  1.3159],\n",
            "        [ 0.6315, -0.8874],\n",
            "        [ 0.5097, -0.9021],\n",
            "        [-0.3399, -0.0268],\n",
            "        [ 0.6765, -0.9477],\n",
            "        [ 0.9913, -1.1485],\n",
            "        [ 0.6259, -0.8950],\n",
            "        [-1.1937,  1.2375],\n",
            "        [-1.5011,  1.5797],\n",
            "        [-1.2865,  1.1294],\n",
            "        [ 0.3259, -0.5055],\n",
            "        [ 0.9650, -0.8836],\n",
            "        [-1.4313,  1.5503]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0548, -0.3253],\n",
            "        [-1.1352,  1.2380],\n",
            "        [ 0.3743, -0.2105],\n",
            "        [-1.3701,  1.4641],\n",
            "        [ 0.4127, -0.8121],\n",
            "        [ 0.8224, -0.6342],\n",
            "        [-1.4033,  1.2808],\n",
            "        [ 0.8611, -1.0351],\n",
            "        [ 0.7703, -1.0805],\n",
            "        [ 0.2978, -0.5020],\n",
            "        [-0.7756,  0.7648],\n",
            "        [-1.3234,  1.4146],\n",
            "        [ 0.5515, -0.8918],\n",
            "        [-0.7273,  0.8627],\n",
            "        [ 0.6755, -1.0147],\n",
            "        [-0.0118, -0.5884],\n",
            "        [-0.5345,  0.6056],\n",
            "        [ 0.5979, -1.0389],\n",
            "        [ 0.6685, -0.7836],\n",
            "        [-1.4075,  1.4584],\n",
            "        [-1.1126,  1.3889],\n",
            "        [ 0.7507, -1.1113],\n",
            "        [-0.8406,  0.9812],\n",
            "        [ 0.7840, -0.9504],\n",
            "        [ 0.9212, -0.9793],\n",
            "        [-0.4337, -0.1049],\n",
            "        [ 0.8171, -1.0195],\n",
            "        [-1.1209,  0.9710],\n",
            "        [-1.2744,  1.2482],\n",
            "        [-1.3571,  1.4206],\n",
            "        [-0.4894,  0.5067],\n",
            "        [ 0.7634, -1.0294]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch    50  of    191.    Elapsed: 0:01:09.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4681, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9972,  1.1375],\n",
            "        [ 1.0268, -1.0756],\n",
            "        [-1.2630,  1.4391],\n",
            "        [ 0.3082, -0.7759],\n",
            "        [ 0.7765, -0.9504],\n",
            "        [ 0.4775, -0.9858],\n",
            "        [ 0.5817, -1.2092],\n",
            "        [-0.4929,  0.5811],\n",
            "        [ 0.3478, -0.9171],\n",
            "        [-1.3873,  1.3474],\n",
            "        [ 0.6718, -0.6517],\n",
            "        [ 0.7228, -0.9112],\n",
            "        [-1.1939,  1.2266],\n",
            "        [-1.4731,  1.5786],\n",
            "        [ 1.0202, -1.2124],\n",
            "        [ 0.9504, -1.1935],\n",
            "        [-1.2932,  1.4533],\n",
            "        [ 0.5959, -1.2870],\n",
            "        [-0.3626,  0.0633],\n",
            "        [-1.2738,  1.4676],\n",
            "        [ 0.8682, -1.0814],\n",
            "        [ 0.9387, -1.2906],\n",
            "        [ 0.5303, -1.0707],\n",
            "        [ 0.8640, -1.0031],\n",
            "        [-0.2220, -0.1866],\n",
            "        [ 1.0378, -1.1558],\n",
            "        [ 0.6543, -1.1295],\n",
            "        [ 0.5061, -0.4370],\n",
            "        [ 0.6232, -0.7945],\n",
            "        [-1.2479,  1.5845],\n",
            "        [-1.3736,  1.3624],\n",
            "        [-1.2552,  1.3644]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3771,  1.3675],\n",
            "        [-1.4330,  1.5544],\n",
            "        [ 0.9060, -0.9792],\n",
            "        [-1.1274,  1.3495],\n",
            "        [-0.2090, -0.0673],\n",
            "        [ 0.9157, -1.0412],\n",
            "        [ 0.8206, -0.8371],\n",
            "        [-0.0147,  0.1521],\n",
            "        [ 0.9070, -1.1592],\n",
            "        [ 0.4634, -0.9837],\n",
            "        [-1.3883,  1.5624],\n",
            "        [ 0.5696, -1.0855],\n",
            "        [ 0.5750, -1.1595],\n",
            "        [ 0.6214, -1.1308],\n",
            "        [ 0.6230, -0.7261],\n",
            "        [-1.4055,  1.1324],\n",
            "        [ 0.6738, -1.2072],\n",
            "        [ 0.5413, -0.7331],\n",
            "        [ 0.5370, -1.0465],\n",
            "        [ 0.7739, -0.9608],\n",
            "        [ 0.5927, -0.9735],\n",
            "        [ 0.6354, -0.9482],\n",
            "        [ 0.6641, -0.6654],\n",
            "        [ 0.0580, -0.3786],\n",
            "        [-0.8363,  0.8629],\n",
            "        [ 0.8473, -1.0763],\n",
            "        [-0.4267,  0.2505],\n",
            "        [-1.0964,  1.0219],\n",
            "        [-1.2645,  1.4302],\n",
            "        [-0.2501,  0.4255],\n",
            "        [ 1.0479, -1.0991],\n",
            "        [ 0.5523, -0.8607]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8890, -1.1099],\n",
            "        [ 0.8564, -0.9515],\n",
            "        [-1.1191,  1.0953],\n",
            "        [ 0.1555, -0.8458],\n",
            "        [-0.3108,  0.2999],\n",
            "        [-1.2171,  1.3630],\n",
            "        [-1.4725,  1.1313],\n",
            "        [ 0.7130, -0.9949],\n",
            "        [ 0.8655, -1.2265],\n",
            "        [ 0.7200, -0.8909],\n",
            "        [ 0.4735, -0.6892],\n",
            "        [ 0.7374, -1.0604],\n",
            "        [-1.2597,  1.5030],\n",
            "        [ 0.7137, -0.8687],\n",
            "        [ 0.6414, -1.0135],\n",
            "        [-0.8954,  1.0184],\n",
            "        [ 0.6021, -0.8108],\n",
            "        [ 0.8787, -1.2364],\n",
            "        [-1.1700,  1.3460],\n",
            "        [-0.4240,  0.4190],\n",
            "        [-0.1054,  0.0896],\n",
            "        [-1.2047,  1.3382],\n",
            "        [ 0.8329, -0.8543],\n",
            "        [-1.3482,  1.4749],\n",
            "        [ 0.1302, -0.5082],\n",
            "        [ 0.2912, -0.1175],\n",
            "        [ 0.3350, -0.9726],\n",
            "        [-0.8790,  0.8986],\n",
            "        [-0.3780,  1.0409],\n",
            "        [ 0.7878, -1.0153],\n",
            "        [-1.4745,  1.3915],\n",
            "        [-1.4297,  1.3632]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4115, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3793,  1.3210],\n",
            "        [ 0.4026, -0.5484],\n",
            "        [-0.1825, -0.4699],\n",
            "        [-1.3157,  1.3892],\n",
            "        [ 0.5016, -0.6542],\n",
            "        [ 0.5341, -0.7934],\n",
            "        [ 0.6776, -0.7742],\n",
            "        [-0.8471,  0.9974],\n",
            "        [ 0.1076, -0.4529],\n",
            "        [ 0.6796, -0.6305],\n",
            "        [-1.0653,  1.1706],\n",
            "        [ 0.6601, -1.2019],\n",
            "        [-1.3457,  1.3085],\n",
            "        [-1.3141,  1.3354],\n",
            "        [ 0.9528, -0.9124],\n",
            "        [-1.4219,  1.2704],\n",
            "        [ 0.6365, -0.8115],\n",
            "        [ 0.2053, -0.4909],\n",
            "        [-0.7713,  0.7968],\n",
            "        [-1.3177,  1.2482],\n",
            "        [-0.8016,  0.8447],\n",
            "        [ 0.8312, -1.4228],\n",
            "        [-1.5243,  1.5817],\n",
            "        [ 0.8804, -0.9714],\n",
            "        [ 0.7100, -1.2668],\n",
            "        [-1.3855,  1.4145],\n",
            "        [ 0.7569, -1.0611],\n",
            "        [ 0.7525, -0.9052],\n",
            "        [-1.5898,  1.2732],\n",
            "        [ 0.1722, -0.1988],\n",
            "        [ 0.8638, -0.9699],\n",
            "        [-1.2156,  1.5148]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7994,  0.8625],\n",
            "        [ 0.8442, -1.2453],\n",
            "        [ 0.7641, -1.0689],\n",
            "        [-0.2620, -0.3030],\n",
            "        [ 0.6288, -0.8234],\n",
            "        [ 1.1763, -0.9827],\n",
            "        [ 0.4902, -0.8383],\n",
            "        [ 0.7189, -1.0771],\n",
            "        [ 0.6390, -0.8900],\n",
            "        [-1.0402,  0.5233],\n",
            "        [ 0.7338, -0.0794],\n",
            "        [-0.7247,  1.1072],\n",
            "        [ 0.8387, -1.0751],\n",
            "        [ 0.4526, -0.7610],\n",
            "        [ 0.8894, -1.1250],\n",
            "        [ 0.7958, -1.1817],\n",
            "        [-1.3007,  1.2778],\n",
            "        [-0.7917,  0.8664],\n",
            "        [-1.4410,  1.6308],\n",
            "        [ 0.6979, -1.2061],\n",
            "        [ 0.2973, -0.4382],\n",
            "        [-0.3482,  0.1690],\n",
            "        [-1.3294,  1.4994],\n",
            "        [ 0.5806, -1.1825],\n",
            "        [-1.0142,  1.2962],\n",
            "        [ 0.2260, -0.5325],\n",
            "        [-1.2917,  1.2754],\n",
            "        [ 0.6944, -1.1618],\n",
            "        [-0.8765,  1.0880],\n",
            "        [-1.1426,  1.1768],\n",
            "        [-0.1279, -0.5035],\n",
            "        [ 0.6346, -0.7449]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2884,  1.7598],\n",
            "        [ 0.6092, -0.9686],\n",
            "        [-0.8759,  0.8841],\n",
            "        [ 0.6452, -0.7378],\n",
            "        [ 0.7282, -1.0178],\n",
            "        [-1.3190,  1.3300],\n",
            "        [-1.3831,  1.5160],\n",
            "        [ 0.5072, -0.8064],\n",
            "        [ 0.7109, -0.7598],\n",
            "        [ 0.8886, -1.0064],\n",
            "        [ 0.9490, -0.8918],\n",
            "        [ 1.0866, -1.2440],\n",
            "        [ 0.9207, -1.0598],\n",
            "        [-0.8988,  0.9831],\n",
            "        [-1.3712,  1.5815],\n",
            "        [ 0.3884, -0.9037],\n",
            "        [-1.1111,  1.3832],\n",
            "        [ 1.0741, -1.0816],\n",
            "        [ 0.4620, -0.6397],\n",
            "        [ 0.9272, -1.4729],\n",
            "        [ 0.5223, -0.8556],\n",
            "        [-0.8939,  0.8031],\n",
            "        [-1.2790,  1.1648],\n",
            "        [ 0.6702, -0.3502],\n",
            "        [-1.3527,  1.5759],\n",
            "        [-0.1529,  0.1351],\n",
            "        [ 0.9549, -1.1032],\n",
            "        [-1.2758,  1.0124],\n",
            "        [ 0.6657, -0.9769],\n",
            "        [-1.1278,  1.1095],\n",
            "        [-1.0519,  1.0899],\n",
            "        [ 0.8444, -1.2109]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3262, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0889,  1.4948],\n",
            "        [ 0.7789, -1.0844],\n",
            "        [-0.4111,  0.1093],\n",
            "        [-1.3084,  1.5255],\n",
            "        [-1.1820,  1.4860],\n",
            "        [ 0.8638, -1.1969],\n",
            "        [ 0.6913, -1.0423],\n",
            "        [-1.3712,  1.2034],\n",
            "        [-0.6302,  0.7406],\n",
            "        [-0.9311,  0.7706],\n",
            "        [ 0.7834, -0.9499],\n",
            "        [ 0.3219, -0.3349],\n",
            "        [-0.5190,  0.4266],\n",
            "        [ 0.7134, -1.1425],\n",
            "        [-0.9607,  0.9944],\n",
            "        [ 0.9713, -0.9695],\n",
            "        [-1.1540,  1.0583],\n",
            "        [-0.6957,  0.9189],\n",
            "        [ 0.3808, -0.7357],\n",
            "        [-1.1975,  1.1726],\n",
            "        [-1.2015,  1.3157],\n",
            "        [-1.1613,  1.1233],\n",
            "        [ 0.7826, -1.2330],\n",
            "        [ 0.6243, -0.8662],\n",
            "        [ 0.9493, -1.0431],\n",
            "        [ 0.6476, -1.2500],\n",
            "        [-0.4067,  0.2986],\n",
            "        [ 0.4089, -0.6167],\n",
            "        [-1.1441,  1.4949],\n",
            "        [-0.7423,  0.7755],\n",
            "        [-0.6977,  0.9531],\n",
            "        [ 0.4305, -0.9557]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7031, -0.6177],\n",
            "        [ 0.2161, -0.2561],\n",
            "        [-1.1224,  0.8183],\n",
            "        [-1.1286,  0.9346],\n",
            "        [ 0.6678, -1.0887],\n",
            "        [-1.0524,  1.2224],\n",
            "        [-1.0838,  1.5564],\n",
            "        [-1.4228,  1.4793],\n",
            "        [ 0.8262, -1.2020],\n",
            "        [ 0.8017, -0.9439],\n",
            "        [ 0.3198, -0.0883],\n",
            "        [ 0.3309, -0.5355],\n",
            "        [-0.7153,  1.1764],\n",
            "        [ 0.3498, -0.5848],\n",
            "        [ 0.8582, -1.1328],\n",
            "        [-1.1130,  1.1503],\n",
            "        [-0.8069,  1.0341],\n",
            "        [ 0.5969, -1.0675],\n",
            "        [ 1.0386, -0.9317],\n",
            "        [ 0.9125, -1.0350],\n",
            "        [-0.9238,  0.7640],\n",
            "        [ 0.5897, -0.9829],\n",
            "        [ 0.1193, -0.0930],\n",
            "        [ 0.7694, -1.0542],\n",
            "        [ 0.6726, -0.8882],\n",
            "        [ 0.7173, -1.2879],\n",
            "        [ 0.9927, -1.1132],\n",
            "        [ 0.4037, -1.0858],\n",
            "        [ 0.7339, -1.0191],\n",
            "        [ 0.5050, -0.5343],\n",
            "        [ 0.9247, -1.0882],\n",
            "        [ 0.7929, -1.1183]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0490,  0.0430],\n",
            "        [ 0.6062, -0.6575],\n",
            "        [-0.8376,  1.1223],\n",
            "        [-1.1211,  1.4215],\n",
            "        [-0.0220,  0.1344],\n",
            "        [ 1.0291, -0.8798],\n",
            "        [ 0.4505, -0.7488],\n",
            "        [ 0.8300, -0.8245],\n",
            "        [ 0.6101, -0.8042],\n",
            "        [ 0.7482, -1.2409],\n",
            "        [ 0.0164, -0.4543],\n",
            "        [ 0.9152, -1.2112],\n",
            "        [ 1.1028, -1.1838],\n",
            "        [ 0.9292, -1.3558],\n",
            "        [-0.7525,  0.8742],\n",
            "        [ 0.4637, -0.7297],\n",
            "        [ 0.9366, -1.1673],\n",
            "        [ 0.4750, -1.0288],\n",
            "        [-0.3542,  0.0937],\n",
            "        [ 0.8838, -1.0579],\n",
            "        [ 0.6991, -0.8953],\n",
            "        [ 0.4965, -0.8736],\n",
            "        [-1.3378,  1.5479],\n",
            "        [-0.2306, -0.0469],\n",
            "        [ 0.5506, -0.9808],\n",
            "        [-0.9954,  0.8536],\n",
            "        [ 0.8105, -1.0993],\n",
            "        [ 0.6924, -0.8448],\n",
            "        [-1.4933,  1.5684],\n",
            "        [ 0.7016, -0.9814],\n",
            "        [-0.3166,  0.3469],\n",
            "        [ 0.8634, -0.8719]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0380,  1.0885],\n",
            "        [-0.3819,  0.5055],\n",
            "        [ 0.0592, -0.4956],\n",
            "        [ 0.3828, -0.4303],\n",
            "        [-0.7657,  0.6091],\n",
            "        [ 0.2242, -0.5486],\n",
            "        [-1.1741,  1.1678],\n",
            "        [ 0.0044, -0.4090],\n",
            "        [ 0.8678, -1.2391],\n",
            "        [-1.2173,  1.4907],\n",
            "        [ 0.7790, -1.2675],\n",
            "        [ 0.2221, -0.5139],\n",
            "        [-0.9628,  0.8879],\n",
            "        [ 0.5749, -0.9916],\n",
            "        [-0.8959,  1.0455],\n",
            "        [-0.9866,  0.6500],\n",
            "        [ 0.1927, -0.5938],\n",
            "        [ 0.6277, -0.6459],\n",
            "        [-1.2309,  1.4025],\n",
            "        [ 0.9714, -1.0108],\n",
            "        [-0.5015,  0.3539],\n",
            "        [ 0.2385, -0.5133],\n",
            "        [-1.4385,  1.2675],\n",
            "        [-1.5377,  1.3786],\n",
            "        [-1.1064,  1.1125],\n",
            "        [-0.8449,  0.8012],\n",
            "        [ 0.9264, -1.0397],\n",
            "        [-1.1120,  1.1407],\n",
            "        [-0.6825,  0.9515],\n",
            "        [-1.3393,  1.3920],\n",
            "        [ 0.0325, -0.0904],\n",
            "        [ 0.6656, -0.8712]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0866,  1.4156],\n",
            "        [ 0.8624, -0.8722],\n",
            "        [ 0.7586, -0.8709],\n",
            "        [ 0.5935, -1.0221],\n",
            "        [-1.3286,  1.2236],\n",
            "        [ 0.0831, -0.0802],\n",
            "        [ 0.4515, -0.4687],\n",
            "        [-1.2574,  1.3095],\n",
            "        [-1.1199,  1.2867],\n",
            "        [-1.1759,  1.3578],\n",
            "        [-1.2910,  1.4100],\n",
            "        [ 0.3979, -0.9412],\n",
            "        [ 0.7273, -0.9907],\n",
            "        [-0.0437, -0.4578],\n",
            "        [ 0.7782, -1.1174],\n",
            "        [-1.4751,  1.4392],\n",
            "        [-1.3041,  1.3369],\n",
            "        [-1.1660,  1.0004],\n",
            "        [ 0.9207, -1.3058],\n",
            "        [ 0.9088, -1.1022],\n",
            "        [-0.8463,  0.6849],\n",
            "        [ 0.9072, -1.1203],\n",
            "        [-0.8225,  1.4367],\n",
            "        [-0.9012,  0.9454],\n",
            "        [-1.3547,  1.4743],\n",
            "        [ 0.7454, -0.8382],\n",
            "        [-0.1224,  0.0768],\n",
            "        [ 0.9881, -1.0149],\n",
            "        [ 0.9688, -0.9408],\n",
            "        [ 0.9391, -0.9381],\n",
            "        [-1.3730,  1.3779],\n",
            "        [-1.3600,  1.5512]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2207, -0.5338],\n",
            "        [ 0.7591, -1.1379],\n",
            "        [ 0.0703, -0.1278],\n",
            "        [ 0.7101, -0.9887],\n",
            "        [ 0.7406, -0.9035],\n",
            "        [ 0.6247, -0.7986],\n",
            "        [ 0.9685, -1.2121],\n",
            "        [ 0.4027, -0.6128],\n",
            "        [-0.9877,  1.0507],\n",
            "        [ 0.7073, -1.0238],\n",
            "        [ 0.6824, -0.9178],\n",
            "        [ 0.4149, -1.2413],\n",
            "        [ 1.0422, -1.0962],\n",
            "        [-1.1853,  1.1329],\n",
            "        [ 0.0213, -0.1990],\n",
            "        [-0.5874,  0.2667],\n",
            "        [ 0.5709, -0.7398],\n",
            "        [ 0.0209, -0.6958],\n",
            "        [ 0.4570, -0.7183],\n",
            "        [ 0.4973, -0.7094],\n",
            "        [ 0.7599, -1.1192],\n",
            "        [-0.9962,  0.9042],\n",
            "        [ 0.8561, -1.1296],\n",
            "        [ 0.5631, -0.7762],\n",
            "        [ 0.7626, -1.2457],\n",
            "        [ 0.5516, -0.7339],\n",
            "        [ 1.0281, -1.1518],\n",
            "        [-0.4224,  0.5180],\n",
            "        [ 0.4868, -0.9751],\n",
            "        [-1.4667,  1.4699],\n",
            "        [ 0.3347, -0.7377],\n",
            "        [ 0.8097, -1.1614]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3344, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0784,  1.1082],\n",
            "        [ 1.3023, -1.5611],\n",
            "        [ 1.1219, -1.2455],\n",
            "        [ 0.9495, -1.0387],\n",
            "        [ 0.5200, -1.1623],\n",
            "        [ 0.5306, -0.9245],\n",
            "        [ 0.5422, -1.0794],\n",
            "        [ 0.5715, -0.9140],\n",
            "        [-0.9511,  1.1905],\n",
            "        [-1.2726,  1.2303],\n",
            "        [-1.0838,  1.4887],\n",
            "        [-1.2215,  1.3466],\n",
            "        [-1.1068,  1.4171],\n",
            "        [-1.1818,  1.1635],\n",
            "        [-1.1822,  1.3574],\n",
            "        [ 0.8992, -1.0163],\n",
            "        [ 0.5813, -0.6038],\n",
            "        [ 0.6623, -1.1677],\n",
            "        [ 0.9320, -1.2111],\n",
            "        [ 0.8115, -1.0969],\n",
            "        [ 0.2423, -0.6753],\n",
            "        [ 0.3582, -0.8404],\n",
            "        [ 0.5725, -0.6615],\n",
            "        [ 0.8655, -0.8987],\n",
            "        [-0.7022,  1.0143],\n",
            "        [ 0.5786, -0.8087],\n",
            "        [-1.2805,  1.4373],\n",
            "        [-0.5614,  0.6127],\n",
            "        [ 1.1266, -1.0486],\n",
            "        [ 0.6895, -0.6671],\n",
            "        [ 0.7492, -1.1910],\n",
            "        [ 0.8847, -1.0150]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8843, -1.0436],\n",
            "        [ 0.7808, -0.9014],\n",
            "        [ 0.7470, -0.7733],\n",
            "        [ 0.3691, -0.8389],\n",
            "        [-1.1209,  1.5056],\n",
            "        [-0.8810,  0.5105],\n",
            "        [ 0.8234, -1.1308],\n",
            "        [ 0.6742, -1.0665],\n",
            "        [ 0.9990, -0.9403],\n",
            "        [ 0.8429, -0.8494],\n",
            "        [ 0.9775, -1.0176],\n",
            "        [ 0.7876, -0.9048],\n",
            "        [ 0.6945, -0.9794],\n",
            "        [-1.0946,  1.1435],\n",
            "        [ 0.7505, -0.7575],\n",
            "        [ 0.7718, -1.2968],\n",
            "        [-1.2240,  1.3613],\n",
            "        [ 0.9081, -0.9887],\n",
            "        [ 0.3586, -0.1874],\n",
            "        [ 1.0523, -1.3146],\n",
            "        [ 0.7407, -0.9756],\n",
            "        [ 0.6587, -0.8351],\n",
            "        [-1.2556,  1.1747],\n",
            "        [-1.1673,  1.3703],\n",
            "        [ 0.1926, -0.5219],\n",
            "        [ 0.7757, -1.0470],\n",
            "        [-1.0714,  1.1135],\n",
            "        [ 0.4723, -0.5741],\n",
            "        [ 0.5163, -1.0950],\n",
            "        [-0.5307,  0.4028],\n",
            "        [ 0.6666, -1.1399],\n",
            "        [ 0.8745, -1.2638]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1970, -1.0856],\n",
            "        [ 0.8432, -1.2397],\n",
            "        [-0.7766,  0.7439],\n",
            "        [-1.2349,  1.2480],\n",
            "        [-1.3890,  1.4253],\n",
            "        [-0.9079,  1.1310],\n",
            "        [ 0.3701, -0.6776],\n",
            "        [ 0.5952, -0.2634],\n",
            "        [-1.1446,  1.3905],\n",
            "        [-0.9651,  0.6626],\n",
            "        [-1.2554,  1.2494],\n",
            "        [-1.3053,  1.3855],\n",
            "        [ 0.7086, -0.9432],\n",
            "        [ 0.3717, -0.8553],\n",
            "        [-0.2638,  0.0858],\n",
            "        [ 0.7967, -0.7940],\n",
            "        [ 1.0380, -0.9792],\n",
            "        [-1.1841,  1.5193],\n",
            "        [ 0.6628, -0.5155],\n",
            "        [-1.3869,  1.3562],\n",
            "        [-1.3909,  1.5106],\n",
            "        [-1.0862,  1.0343],\n",
            "        [ 0.5864, -0.8434],\n",
            "        [ 0.8229, -1.0300],\n",
            "        [ 0.2620, -0.5218],\n",
            "        [-1.4354,  1.5236],\n",
            "        [ 0.8649, -1.4670],\n",
            "        [-1.2973,  1.2726],\n",
            "        [-0.6356,  0.7638],\n",
            "        [ 0.8700, -1.1742],\n",
            "        [ 0.1775, -0.4207],\n",
            "        [ 1.0492, -1.3475]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3864, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9861,  0.9135],\n",
            "        [ 0.8517, -0.9342],\n",
            "        [ 0.6303, -1.1274],\n",
            "        [ 0.1368, -0.5142],\n",
            "        [-0.4008,  0.2392],\n",
            "        [-1.3017,  1.6370],\n",
            "        [ 0.6317, -0.9460],\n",
            "        [ 0.3363, -0.6391],\n",
            "        [ 1.1195, -1.1445],\n",
            "        [ 0.7997, -1.0755],\n",
            "        [ 0.3687, -0.1946],\n",
            "        [-1.1245,  1.4479],\n",
            "        [-0.8057,  0.9888],\n",
            "        [ 0.6388, -1.0479],\n",
            "        [ 1.0460, -1.0002],\n",
            "        [-1.0191,  1.5609],\n",
            "        [-1.3127,  1.4778],\n",
            "        [-1.0357,  1.1765],\n",
            "        [ 0.7133, -0.8161],\n",
            "        [ 0.8337, -0.8626],\n",
            "        [ 0.9451, -1.0348],\n",
            "        [ 0.6666, -0.9085],\n",
            "        [ 0.4623, -0.7859],\n",
            "        [-1.2690,  1.5378],\n",
            "        [-1.3321,  1.5180],\n",
            "        [-0.4713,  0.4252],\n",
            "        [-1.0856,  1.3640],\n",
            "        [ 1.4652, -0.9076],\n",
            "        [-0.7966,  0.6025],\n",
            "        [-1.1832,  1.3815],\n",
            "        [ 0.8074, -0.9741],\n",
            "        [-0.0580, -0.4341]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2982, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8502,  1.2484],\n",
            "        [ 0.7592, -0.6649],\n",
            "        [-0.8458,  0.9464],\n",
            "        [ 0.6770, -0.7546],\n",
            "        [ 0.6286, -0.8543],\n",
            "        [ 0.7052, -0.5925],\n",
            "        [-1.1088,  1.4800],\n",
            "        [ 0.9254, -1.2942],\n",
            "        [ 0.7504, -1.0852],\n",
            "        [ 1.0311, -1.1341],\n",
            "        [ 0.6475, -0.5360],\n",
            "        [-0.9835,  0.6547],\n",
            "        [ 0.3832, -0.4687],\n",
            "        [-1.1092,  1.2356],\n",
            "        [ 0.7451, -1.0311],\n",
            "        [ 0.8121, -1.1500],\n",
            "        [-1.5252,  1.3734],\n",
            "        [ 0.7812, -1.1782],\n",
            "        [ 0.5777, -0.8331],\n",
            "        [ 0.9324, -0.9234],\n",
            "        [-1.0404,  0.8362],\n",
            "        [-0.7937,  0.9121],\n",
            "        [-0.1517,  0.4002],\n",
            "        [ 0.8215, -0.7792],\n",
            "        [-1.2772,  1.5392],\n",
            "        [ 0.0959, -0.0259],\n",
            "        [ 0.4028, -0.8278],\n",
            "        [ 1.0489, -0.9435],\n",
            "        [ 0.7986, -0.9823],\n",
            "        [ 0.9510, -1.0333],\n",
            "        [-1.1641,  1.0967],\n",
            "        [-1.1228,  1.3019]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8269, -0.9532],\n",
            "        [ 0.4989, -0.1945],\n",
            "        [-0.1656, -0.0138],\n",
            "        [ 0.8338, -1.0993],\n",
            "        [-0.9413,  1.1957],\n",
            "        [ 0.6697, -1.1129],\n",
            "        [-1.1078,  1.3300],\n",
            "        [-0.0888, -0.0023],\n",
            "        [-1.2403,  1.4338],\n",
            "        [-1.4225,  1.4244],\n",
            "        [-1.4735,  1.4451],\n",
            "        [-1.3623,  1.3732],\n",
            "        [-0.0856,  0.1356],\n",
            "        [ 1.1878, -1.1336],\n",
            "        [ 0.8710, -0.4957],\n",
            "        [ 0.9789, -1.0025],\n",
            "        [-1.0063,  1.2655],\n",
            "        [ 0.4573, -0.6525],\n",
            "        [ 0.7643, -0.9056],\n",
            "        [-0.7658,  0.2985],\n",
            "        [-0.7187,  1.0272],\n",
            "        [-0.2046, -0.0135],\n",
            "        [ 0.7289, -0.9465],\n",
            "        [ 0.6842, -1.2786],\n",
            "        [ 0.8216, -1.0226],\n",
            "        [-1.3134,  1.6520],\n",
            "        [-0.6244,  0.9248],\n",
            "        [ 0.8174, -1.4218],\n",
            "        [ 0.9487, -1.0592],\n",
            "        [-0.7668,  1.0270],\n",
            "        [-1.3491,  1.5071],\n",
            "        [-1.1602,  1.2523]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1017,  1.6264],\n",
            "        [ 1.2163, -1.3432],\n",
            "        [ 0.9155, -0.9164],\n",
            "        [ 0.4209, -0.4639],\n",
            "        [ 0.7804, -0.9813],\n",
            "        [ 0.7549, -1.3107],\n",
            "        [-0.3538,  0.0709],\n",
            "        [-1.1182,  1.4163],\n",
            "        [ 0.4537, -0.8749],\n",
            "        [-1.1907,  1.3581],\n",
            "        [-1.1675,  0.8414],\n",
            "        [ 0.7872, -0.8820],\n",
            "        [ 0.7460, -0.9685],\n",
            "        [-1.4077,  1.2208],\n",
            "        [ 0.7935, -0.8712],\n",
            "        [-0.5644,  0.5209],\n",
            "        [ 0.8506, -1.0014],\n",
            "        [-0.6668,  0.6010],\n",
            "        [ 0.9977, -1.1324],\n",
            "        [ 0.9423, -1.0538],\n",
            "        [-0.9306,  1.1798],\n",
            "        [-1.2045,  1.4310],\n",
            "        [-0.9885,  1.3866],\n",
            "        [-1.5278,  1.2732],\n",
            "        [-0.2820,  0.3326],\n",
            "        [ 0.9726, -0.8743],\n",
            "        [ 0.2373, -0.3304],\n",
            "        [ 1.0258, -1.0109],\n",
            "        [ 0.9041, -0.8546],\n",
            "        [ 1.0059, -1.2391],\n",
            "        [-0.6856,  0.9067],\n",
            "        [-0.3225,  0.7090]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1372, -0.5822],\n",
            "        [ 0.5168, -0.7949],\n",
            "        [ 0.8346, -1.1134],\n",
            "        [ 0.5339, -0.7356],\n",
            "        [ 0.6703, -0.8878],\n",
            "        [ 0.9654, -1.0259],\n",
            "        [-1.1661,  1.0781],\n",
            "        [ 0.3219, -0.2161],\n",
            "        [-0.9120,  0.9341],\n",
            "        [-0.2115,  0.1892],\n",
            "        [-1.0915,  1.4669],\n",
            "        [ 0.1579, -0.5214],\n",
            "        [ 0.4661, -0.3825],\n",
            "        [ 0.7108, -1.2244],\n",
            "        [ 1.1411, -1.1857],\n",
            "        [ 0.4711, -1.2699],\n",
            "        [ 0.9096, -1.0827],\n",
            "        [-0.9548,  0.8509],\n",
            "        [-1.4097,  1.1820],\n",
            "        [-1.2263,  1.5900],\n",
            "        [ 0.9091, -1.0264],\n",
            "        [ 0.9179, -1.1689],\n",
            "        [ 0.6686, -0.9419],\n",
            "        [ 0.7844, -0.9531],\n",
            "        [ 0.6349, -0.7420],\n",
            "        [ 0.8967, -1.2118],\n",
            "        [-0.8103,  0.9851],\n",
            "        [-0.8579,  1.0382],\n",
            "        [ 0.6967, -0.7675],\n",
            "        [-1.0099,  1.2787],\n",
            "        [ 0.2041, -0.2998],\n",
            "        [ 0.7066, -0.7576]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3520, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2417,  1.2636],\n",
            "        [-0.9699,  1.1569],\n",
            "        [ 0.8633, -0.9914],\n",
            "        [ 1.0621, -1.1257],\n",
            "        [-1.2806,  1.4320],\n",
            "        [ 0.9647, -0.9107],\n",
            "        [-0.5378,  0.2148],\n",
            "        [-1.3182,  1.5469],\n",
            "        [-1.0554,  1.1042],\n",
            "        [-1.2679,  1.4793],\n",
            "        [ 1.1505, -1.3675],\n",
            "        [ 0.5105, -0.9231],\n",
            "        [ 0.1706, -0.1162],\n",
            "        [-1.2227,  1.4669],\n",
            "        [-0.0067, -0.4732],\n",
            "        [ 0.7591, -0.9786],\n",
            "        [ 0.6545, -0.6825],\n",
            "        [ 0.1891, -0.7756],\n",
            "        [ 0.9288, -1.1284],\n",
            "        [-0.2257,  0.2159],\n",
            "        [-1.1681,  1.6622],\n",
            "        [ 0.6619, -0.9170],\n",
            "        [-0.5901,  0.4349],\n",
            "        [ 0.5605, -0.6399],\n",
            "        [ 0.9062, -0.8041],\n",
            "        [ 0.6840, -0.7680],\n",
            "        [-0.8898,  1.2817],\n",
            "        [ 0.2098, -0.3358],\n",
            "        [-1.0049,  0.8859],\n",
            "        [ 0.0784, -0.3200],\n",
            "        [ 0.5303, -0.9528],\n",
            "        [ 0.7903, -1.1261]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3670, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5477, -0.7640],\n",
            "        [ 1.0469, -1.2977],\n",
            "        [ 0.6694, -0.7899],\n",
            "        [-0.8216,  0.7413],\n",
            "        [ 0.7280, -1.0662],\n",
            "        [-1.0584,  0.7699],\n",
            "        [-0.1692,  0.0282],\n",
            "        [ 0.6116, -0.8758],\n",
            "        [-0.0315,  0.0277],\n",
            "        [ 1.1376, -1.0140],\n",
            "        [-0.4415,  0.3167],\n",
            "        [-0.9249,  0.6854],\n",
            "        [ 0.9917, -0.9239],\n",
            "        [ 0.6567, -0.6657],\n",
            "        [ 0.8502, -0.9125],\n",
            "        [ 0.0415, -0.5169],\n",
            "        [ 0.7064, -0.9469],\n",
            "        [ 0.8108, -1.2477],\n",
            "        [ 0.5450, -1.0967],\n",
            "        [-0.6009,  0.4707],\n",
            "        [-1.3412,  1.5618],\n",
            "        [ 0.9694, -1.1244],\n",
            "        [-0.7163,  1.0029],\n",
            "        [ 1.2294, -1.0297],\n",
            "        [ 0.8988, -0.9169],\n",
            "        [ 0.6820, -0.9150],\n",
            "        [ 1.1377, -1.2269],\n",
            "        [ 0.6147, -0.7399],\n",
            "        [-1.3570,  1.3546],\n",
            "        [ 0.8134, -0.8868],\n",
            "        [ 0.8907, -1.2750],\n",
            "        [ 0.4866, -1.0657]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5442, -0.6498],\n",
            "        [ 0.8990, -1.3663],\n",
            "        [ 0.8225, -0.8104],\n",
            "        [-0.2157,  0.1338],\n",
            "        [ 0.7957, -1.0192],\n",
            "        [ 0.0918, -0.0100],\n",
            "        [ 0.8371, -1.2841],\n",
            "        [-1.2466,  1.3944],\n",
            "        [ 0.8809, -0.9684],\n",
            "        [-1.0854,  1.1245],\n",
            "        [-1.3243,  1.5659],\n",
            "        [-0.5042,  0.5246],\n",
            "        [ 0.6131, -0.9204],\n",
            "        [ 0.6792, -0.8159],\n",
            "        [ 0.7722, -1.0518],\n",
            "        [-0.4012,  0.2030],\n",
            "        [-1.2199,  1.2764],\n",
            "        [ 0.2366, -0.5139],\n",
            "        [-1.2585,  1.4578],\n",
            "        [-0.2951,  0.0619],\n",
            "        [ 0.9592, -1.2358],\n",
            "        [-1.1419,  1.2919],\n",
            "        [ 1.0608, -1.1646],\n",
            "        [ 0.5494, -1.0253],\n",
            "        [ 0.9064, -0.9788],\n",
            "        [ 0.9617, -1.1661],\n",
            "        [ 0.7600, -1.0630],\n",
            "        [ 1.0676, -1.2657],\n",
            "        [ 0.6343, -0.9813],\n",
            "        [-1.2457,  1.2024],\n",
            "        [-1.2088,  1.1332],\n",
            "        [-0.7873,  0.8469]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4196, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1325,  0.3687],\n",
            "        [ 0.2886, -0.4944],\n",
            "        [ 0.5464, -0.8718],\n",
            "        [ 0.2576, -0.4215],\n",
            "        [ 0.8358, -0.8481],\n",
            "        [ 0.2770, -0.6264],\n",
            "        [-1.4044,  1.4690],\n",
            "        [ 0.9798, -1.5006],\n",
            "        [ 0.9724, -1.1383],\n",
            "        [ 1.0008, -0.8962],\n",
            "        [ 0.9662, -0.9095],\n",
            "        [ 0.9085, -1.1574],\n",
            "        [-1.3698,  1.1218],\n",
            "        [ 1.1999, -1.1168],\n",
            "        [-0.8082,  1.1651],\n",
            "        [ 0.6107, -0.5145],\n",
            "        [ 0.2006, -0.4134],\n",
            "        [ 0.9402, -0.9197],\n",
            "        [ 0.6772, -1.1106],\n",
            "        [ 0.8726, -1.0817],\n",
            "        [ 0.5383, -0.8906],\n",
            "        [ 0.6799, -0.6425],\n",
            "        [ 0.7931, -1.0793],\n",
            "        [-0.9735,  1.3128],\n",
            "        [-1.5753,  1.4240],\n",
            "        [-1.4351,  1.3915],\n",
            "        [ 1.0512, -1.0807],\n",
            "        [-1.5419,  1.1878],\n",
            "        [ 0.6223, -0.6516],\n",
            "        [ 0.3379, -0.7408],\n",
            "        [-1.2186,  1.2430],\n",
            "        [-0.8271,  0.9103]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4798, -1.0095],\n",
            "        [ 0.9676, -1.0838],\n",
            "        [-0.3298,  0.1310],\n",
            "        [ 1.1076, -1.0947],\n",
            "        [ 0.6937, -0.9733],\n",
            "        [ 0.8081, -1.1958],\n",
            "        [-1.3704,  1.5217],\n",
            "        [ 0.6583, -0.6732],\n",
            "        [ 0.5232, -1.0589],\n",
            "        [-1.2820,  1.2704],\n",
            "        [-1.3516,  1.2838],\n",
            "        [ 0.7345, -1.2856],\n",
            "        [ 0.6239, -0.5409],\n",
            "        [ 0.9090, -1.0326],\n",
            "        [ 1.0229, -1.1813],\n",
            "        [ 0.7333, -0.9255],\n",
            "        [ 0.8690, -1.0187],\n",
            "        [-0.5970,  0.8379],\n",
            "        [-1.1854,  1.5023],\n",
            "        [ 0.6211, -1.0575],\n",
            "        [ 0.2851, -0.3710],\n",
            "        [-1.2273,  0.9428],\n",
            "        [-1.1511,  1.3365],\n",
            "        [ 0.9068, -1.2545],\n",
            "        [ 0.8339, -1.1228],\n",
            "        [-0.1938,  0.1842],\n",
            "        [-0.3547,  0.5160],\n",
            "        [ 1.1612, -0.9838],\n",
            "        [-0.9572,  1.1394],\n",
            "        [-0.9248,  1.0166],\n",
            "        [ 1.0098, -1.1092],\n",
            "        [ 0.4833, -0.4516]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6412, -1.1147],\n",
            "        [ 0.5712, -0.7317],\n",
            "        [-1.3387,  1.4413],\n",
            "        [ 0.1038, -0.1300],\n",
            "        [-1.0608,  1.3438],\n",
            "        [-0.3079,  0.8931],\n",
            "        [ 0.8900, -1.0899],\n",
            "        [ 0.8362, -0.9754],\n",
            "        [-1.1053,  1.2350],\n",
            "        [-0.9663,  0.9581],\n",
            "        [-1.4520,  1.4298],\n",
            "        [ 0.9791, -1.4280],\n",
            "        [-1.0006,  0.9106],\n",
            "        [-1.2384,  1.5090],\n",
            "        [-0.1681,  0.0557],\n",
            "        [ 1.1336, -1.1502],\n",
            "        [-0.6603,  0.6858],\n",
            "        [-0.6352,  0.9424],\n",
            "        [ 1.0574, -1.1570],\n",
            "        [-1.3388,  1.1599],\n",
            "        [-0.1568,  0.2534],\n",
            "        [-0.2623, -0.2171],\n",
            "        [-0.4187,  0.2493],\n",
            "        [ 0.7163, -0.9472],\n",
            "        [-1.2731,  1.4651],\n",
            "        [-1.1968,  1.3742],\n",
            "        [ 0.0720,  0.0965],\n",
            "        [ 0.9676, -1.0154],\n",
            "        [ 1.0849, -1.0555],\n",
            "        [-0.2307,  0.4703],\n",
            "        [ 0.8178, -1.1719],\n",
            "        [-1.2961,  1.4649]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4699, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5987, -0.7888],\n",
            "        [-1.0909,  1.0668],\n",
            "        [ 1.0710, -1.2311],\n",
            "        [ 0.6263, -1.0959],\n",
            "        [-0.5612,  0.8510],\n",
            "        [-0.6017,  0.5160],\n",
            "        [ 0.5532, -0.7344],\n",
            "        [ 0.9239, -1.2257],\n",
            "        [-1.1712,  1.2312],\n",
            "        [-1.2066,  1.5699],\n",
            "        [ 0.7459, -1.1986],\n",
            "        [-1.2061,  1.0181],\n",
            "        [ 0.5635, -0.9044],\n",
            "        [ 0.3160, -0.5386],\n",
            "        [-1.5039,  1.3429],\n",
            "        [ 0.3972, -0.6556],\n",
            "        [ 0.8494, -0.8806],\n",
            "        [-1.0091,  1.3423],\n",
            "        [-1.2849,  1.4488],\n",
            "        [ 0.7716, -0.8586],\n",
            "        [ 0.0452, -0.5700],\n",
            "        [-0.8389,  1.2251],\n",
            "        [-1.3079,  1.4684],\n",
            "        [ 0.3454, -0.6713],\n",
            "        [ 0.1339, -0.5916],\n",
            "        [ 0.6131, -0.9567],\n",
            "        [ 0.7721, -1.1295],\n",
            "        [-1.3514,  1.0594],\n",
            "        [-0.8955,  0.9861],\n",
            "        [ 1.0128, -1.3491],\n",
            "        [ 1.1579, -1.4943],\n",
            "        [ 0.6830, -0.9112]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7328, -0.9333],\n",
            "        [ 0.3319, -1.0487],\n",
            "        [ 0.7296, -0.9709],\n",
            "        [-1.2474,  1.5352],\n",
            "        [-0.4031,  0.3372],\n",
            "        [-1.1209,  1.1359],\n",
            "        [ 0.8487, -0.8929],\n",
            "        [ 0.4769, -0.5644],\n",
            "        [-1.2432,  1.5625],\n",
            "        [-0.3471,  0.3178],\n",
            "        [ 0.8955, -1.0247],\n",
            "        [ 0.6651, -0.5406],\n",
            "        [-1.2852,  1.5205],\n",
            "        [ 0.9636, -1.0310],\n",
            "        [ 0.7972, -1.0053],\n",
            "        [ 0.7497, -1.1064],\n",
            "        [ 0.4628, -0.2846],\n",
            "        [-1.3701,  1.4928],\n",
            "        [ 0.7863, -1.1276],\n",
            "        [ 0.9436, -1.0535],\n",
            "        [-0.8739,  0.9089],\n",
            "        [ 0.1950, -0.9699],\n",
            "        [-1.3368,  1.4477],\n",
            "        [-0.3453,  0.4237],\n",
            "        [ 0.7332, -1.1742],\n",
            "        [ 1.0618, -1.0637],\n",
            "        [ 0.7825, -1.3451],\n",
            "        [ 0.6783, -1.0275],\n",
            "        [ 0.8846, -0.9123],\n",
            "        [ 0.8038, -1.1927],\n",
            "        [ 1.0006, -1.1427],\n",
            "        [ 0.9464, -0.9666]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2884,  1.2388],\n",
            "        [ 0.0398, -0.3852],\n",
            "        [ 1.0291, -1.1738],\n",
            "        [-1.2439,  1.4067],\n",
            "        [ 0.2509, -0.5712],\n",
            "        [ 0.7770, -1.1950],\n",
            "        [ 1.0624, -0.9737],\n",
            "        [-1.3231,  1.3105],\n",
            "        [ 0.7172, -1.0058],\n",
            "        [ 0.9442, -1.1050],\n",
            "        [-1.0425,  1.3752],\n",
            "        [ 1.0043, -1.1073],\n",
            "        [ 0.0807, -0.0945],\n",
            "        [-0.7470,  1.0681],\n",
            "        [ 0.7903, -1.0680],\n",
            "        [ 0.6696, -1.0761],\n",
            "        [ 1.2992, -1.3778],\n",
            "        [-1.2184,  1.2375],\n",
            "        [ 0.1823, -0.5488],\n",
            "        [ 0.9036, -1.1360],\n",
            "        [-1.0373,  1.0746],\n",
            "        [ 0.7973, -1.0340],\n",
            "        [ 0.7757, -1.2304],\n",
            "        [ 0.4681, -0.9124],\n",
            "        [-1.2957,  1.3414],\n",
            "        [ 0.5742, -0.4141],\n",
            "        [ 0.7577, -1.0041],\n",
            "        [ 1.0276, -0.9232],\n",
            "        [-1.4104,  1.3805],\n",
            "        [ 0.6852, -0.9914],\n",
            "        [-1.0198,  1.0537],\n",
            "        [-1.4774,  1.4719]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7944,  0.7395],\n",
            "        [ 0.9314, -1.0891],\n",
            "        [-1.1941,  1.4626],\n",
            "        [ 0.3585, -0.7950],\n",
            "        [ 0.8438, -1.2202],\n",
            "        [ 1.0141, -1.3727],\n",
            "        [ 0.5351, -1.0902],\n",
            "        [-1.0761,  1.3923],\n",
            "        [ 1.0370, -1.2751],\n",
            "        [-0.2787,  0.0063],\n",
            "        [-0.2721,  0.1951],\n",
            "        [-1.2030,  1.4713],\n",
            "        [ 0.5076, -0.8608],\n",
            "        [-1.0331,  1.3560],\n",
            "        [-0.2239, -0.1074],\n",
            "        [ 0.7516, -1.3560],\n",
            "        [ 0.3149, -0.7096],\n",
            "        [ 0.8449, -1.2729],\n",
            "        [ 0.8076, -1.1121],\n",
            "        [ 1.0359, -1.3095],\n",
            "        [ 0.2139, -0.4325],\n",
            "        [ 0.1816, -0.3118],\n",
            "        [ 0.5459, -0.9167],\n",
            "        [-1.2615,  1.5281],\n",
            "        [ 0.8804, -1.2417],\n",
            "        [ 1.0234, -1.2665],\n",
            "        [ 0.8158, -1.0611],\n",
            "        [ 1.0184, -1.1932],\n",
            "        [-1.0679,  1.2962],\n",
            "        [ 0.9001, -0.8816],\n",
            "        [ 0.0703, -0.2900],\n",
            "        [-1.4606,  1.5872]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9505, -0.8503],\n",
            "        [-0.1261,  0.1271],\n",
            "        [ 1.0579, -1.1072],\n",
            "        [ 0.6140, -0.9142],\n",
            "        [ 0.8974, -0.9202],\n",
            "        [ 0.2690, -0.7188],\n",
            "        [ 0.7551, -1.1190],\n",
            "        [ 0.6573, -0.5106],\n",
            "        [ 0.8121, -1.2582],\n",
            "        [-0.8525,  1.0091],\n",
            "        [ 0.5247, -0.8937],\n",
            "        [-0.8052,  0.8324],\n",
            "        [ 1.0539, -0.9611],\n",
            "        [-0.9903,  1.3225],\n",
            "        [ 0.6187, -1.2161],\n",
            "        [ 1.0422, -1.1112],\n",
            "        [ 0.7066, -1.2089],\n",
            "        [-0.7253,  0.9274],\n",
            "        [ 1.0787, -1.1816],\n",
            "        [ 0.8763, -1.0319],\n",
            "        [ 1.1347, -1.2649],\n",
            "        [-0.1070, -0.1022],\n",
            "        [ 0.3835, -0.9797],\n",
            "        [ 0.8711, -1.2296],\n",
            "        [ 0.7271, -1.0410],\n",
            "        [-1.2937,  1.5378],\n",
            "        [ 0.9972, -1.0124],\n",
            "        [-0.3325,  0.6097],\n",
            "        [ 0.6681, -1.1504],\n",
            "        [-0.9224,  1.2023],\n",
            "        [-1.2866,  1.4704],\n",
            "        [ 0.1609, -0.4983]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2207, -0.5844],\n",
            "        [ 0.3499, -0.8266],\n",
            "        [-1.2979,  1.3027],\n",
            "        [ 1.0030, -1.1216],\n",
            "        [-0.0299, -0.3254],\n",
            "        [-1.2184,  1.2642],\n",
            "        [-1.3288,  1.6540],\n",
            "        [ 0.8115, -1.1151],\n",
            "        [ 1.0038, -1.0041],\n",
            "        [ 1.0379, -1.4519],\n",
            "        [ 0.3572, -0.7614],\n",
            "        [ 0.5984, -1.2188],\n",
            "        [-1.3634,  1.5092],\n",
            "        [ 0.7566, -1.0664],\n",
            "        [ 0.8058, -0.9091],\n",
            "        [-0.6070,  0.5443],\n",
            "        [ 1.1402, -1.2434],\n",
            "        [ 0.9038, -1.0212],\n",
            "        [-1.5612,  1.4568],\n",
            "        [ 0.7470, -0.8472],\n",
            "        [ 0.4113, -0.6033],\n",
            "        [ 0.9718, -1.3331],\n",
            "        [ 0.6933, -1.0462],\n",
            "        [ 0.9413, -1.2610],\n",
            "        [ 0.4246, -0.7132],\n",
            "        [-1.5197,  1.4705],\n",
            "        [ 0.9521, -1.2407],\n",
            "        [ 0.2002, -0.5108],\n",
            "        [-1.3303,  1.3055],\n",
            "        [-0.7534,  1.1221],\n",
            "        [ 0.9600, -1.2496],\n",
            "        [ 1.0083, -0.8707]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5071, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2048, -0.0033],\n",
            "        [-1.1068,  1.5208],\n",
            "        [-1.2835,  1.4823],\n",
            "        [ 0.9368, -1.2676],\n",
            "        [-1.0254,  1.4869],\n",
            "        [ 0.3860, -0.9130],\n",
            "        [-0.2452,  0.4593],\n",
            "        [ 0.7808, -1.0992],\n",
            "        [-1.3400,  1.1814],\n",
            "        [-1.2489,  1.2551],\n",
            "        [-1.0569,  1.2607],\n",
            "        [-1.3485,  1.4423],\n",
            "        [-1.4202,  1.3928],\n",
            "        [ 0.9626, -1.2323],\n",
            "        [-1.1329,  1.1389],\n",
            "        [ 0.3267, -0.7612],\n",
            "        [-0.7546,  0.2598],\n",
            "        [-0.8077,  1.0000],\n",
            "        [ 0.7014, -1.0352],\n",
            "        [-1.0712,  1.2378],\n",
            "        [ 0.4077, -1.0838],\n",
            "        [ 0.3536, -0.5279],\n",
            "        [-0.6206,  0.6152],\n",
            "        [-1.3661,  1.5400],\n",
            "        [ 0.5256, -1.0723],\n",
            "        [-1.2221,  1.5120],\n",
            "        [ 0.4195, -0.3885],\n",
            "        [ 0.7670, -1.0361],\n",
            "        [ 1.0146, -1.1088],\n",
            "        [ 0.8581, -0.8057],\n",
            "        [ 0.8588, -1.3116],\n",
            "        [ 0.1014,  0.0688]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7028, -1.0641],\n",
            "        [ 0.6987, -0.8996],\n",
            "        [ 0.9028, -1.0087],\n",
            "        [ 0.6478, -0.8970],\n",
            "        [-1.3165,  1.0693],\n",
            "        [-0.9898,  1.0918],\n",
            "        [ 0.7195, -0.9756],\n",
            "        [ 0.6990, -1.0793],\n",
            "        [ 0.9111, -1.0725],\n",
            "        [ 0.0724, -0.6503],\n",
            "        [ 1.0239, -1.1857],\n",
            "        [ 0.7387, -0.9945],\n",
            "        [ 0.8127, -1.0731],\n",
            "        [ 0.7888, -1.0616],\n",
            "        [ 0.8799, -0.7724],\n",
            "        [ 0.6439, -0.5422],\n",
            "        [ 0.7555, -1.2105],\n",
            "        [-0.0521,  0.0889],\n",
            "        [ 0.2968, -0.8469],\n",
            "        [ 0.9147, -0.8083],\n",
            "        [-1.2742,  1.5068],\n",
            "        [-0.4710,  0.3986],\n",
            "        [ 0.8283, -1.1776],\n",
            "        [ 0.4849, -0.7002],\n",
            "        [ 0.7540, -0.8397],\n",
            "        [-1.3917,  1.5060],\n",
            "        [-0.0948,  0.0455],\n",
            "        [ 0.6893, -0.8264],\n",
            "        [ 0.2929, -0.4143],\n",
            "        [-1.2987,  1.1662],\n",
            "        [ 0.7007, -1.1216],\n",
            "        [-1.2171,  1.3052]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1745,  1.4880],\n",
            "        [ 0.4425, -0.7181],\n",
            "        [-0.8650,  0.6423],\n",
            "        [ 0.0612, -0.7519],\n",
            "        [ 0.8779, -0.9873],\n",
            "        [-0.5632,  0.8505],\n",
            "        [-0.8605,  1.0310],\n",
            "        [ 0.7100, -0.9828],\n",
            "        [ 1.0055, -1.0243],\n",
            "        [-1.2050,  1.4052],\n",
            "        [-0.8557,  1.0077],\n",
            "        [ 1.0600, -1.1684],\n",
            "        [-1.1183,  1.2778],\n",
            "        [-1.2738,  1.1114],\n",
            "        [ 1.0174, -1.4062],\n",
            "        [ 0.5844, -0.9382],\n",
            "        [ 1.1623, -1.4106],\n",
            "        [ 0.5852, -1.0655],\n",
            "        [ 0.6207, -0.5530],\n",
            "        [ 0.9284, -1.1996],\n",
            "        [-0.7723,  0.8355],\n",
            "        [ 0.5810, -1.1097],\n",
            "        [-0.6976,  0.4667],\n",
            "        [-1.2949,  0.9210],\n",
            "        [-1.1715,  1.2583],\n",
            "        [ 0.8279, -0.8046],\n",
            "        [ 0.8000, -1.2516],\n",
            "        [ 0.1240, -0.5924],\n",
            "        [-1.1382,  1.2607],\n",
            "        [-1.2272,  1.1541],\n",
            "        [ 0.7221, -1.2412],\n",
            "        [ 0.7368, -0.6489]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4594,  1.2963],\n",
            "        [ 0.5420, -1.0139],\n",
            "        [ 0.7520, -1.2011],\n",
            "        [-0.4441,  0.6763],\n",
            "        [ 0.7144, -0.6700],\n",
            "        [ 0.7196, -1.1199],\n",
            "        [ 0.8912, -1.0669],\n",
            "        [ 0.5384, -0.5212],\n",
            "        [-0.3590,  0.3622],\n",
            "        [ 0.2810, -0.1666],\n",
            "        [ 0.7029, -1.0142],\n",
            "        [-0.3178,  0.3204],\n",
            "        [ 0.9054, -1.1551],\n",
            "        [-1.0681,  1.1536],\n",
            "        [-0.7908,  0.8536],\n",
            "        [-0.7430,  0.7942],\n",
            "        [-0.8022,  0.9139],\n",
            "        [ 0.1353, -0.3168],\n",
            "        [-0.6802,  0.8571],\n",
            "        [ 0.9836, -0.9301],\n",
            "        [-1.2907,  1.0899],\n",
            "        [ 0.8849, -1.1424],\n",
            "        [ 0.5891, -1.0126],\n",
            "        [-0.6329,  0.4631],\n",
            "        [-1.3998,  1.3135],\n",
            "        [-1.3746,  1.5323],\n",
            "        [-1.3947,  1.0988],\n",
            "        [ 0.3533, -0.4906],\n",
            "        [ 0.4837, -0.8129],\n",
            "        [-0.0430, -0.0901],\n",
            "        [ 0.8477, -0.8254],\n",
            "        [-1.1339,  1.4883]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6635, -0.9683],\n",
            "        [-1.0471,  1.2747],\n",
            "        [ 0.8441, -0.8407],\n",
            "        [ 1.0387, -1.0478],\n",
            "        [-0.0073, -0.1279],\n",
            "        [-0.1099, -0.2321],\n",
            "        [-0.7429,  0.8394],\n",
            "        [ 0.6016, -0.7276],\n",
            "        [-1.3839,  1.2702],\n",
            "        [ 0.7717, -0.8931],\n",
            "        [-0.9714,  1.3983],\n",
            "        [ 0.8305, -1.0941],\n",
            "        [ 0.7848, -1.0608],\n",
            "        [ 1.0315, -1.2064],\n",
            "        [-1.4059,  1.1807],\n",
            "        [ 1.0855, -1.0863],\n",
            "        [-0.5496,  0.6110],\n",
            "        [ 0.7312, -1.1566],\n",
            "        [ 0.7443, -0.8192],\n",
            "        [ 0.8314, -1.1966],\n",
            "        [-0.0515,  0.1834],\n",
            "        [-0.1014,  0.0638],\n",
            "        [-1.2259,  1.2554],\n",
            "        [-0.7964,  0.6899],\n",
            "        [ 1.1119, -1.1770],\n",
            "        [ 0.1112, -0.4448],\n",
            "        [ 0.7864, -1.2299],\n",
            "        [-1.2966,  1.3821],\n",
            "        [-1.0497,  1.0189],\n",
            "        [ 0.8611, -1.3742],\n",
            "        [ 1.1270, -1.0828],\n",
            "        [-0.4635,  0.5484]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7739, -1.2668],\n",
            "        [ 0.8425, -1.2044],\n",
            "        [ 0.8628, -1.0757],\n",
            "        [ 0.8321, -1.1077],\n",
            "        [ 0.9580, -1.3839],\n",
            "        [ 0.2030, -0.2087],\n",
            "        [ 1.1282, -0.9942],\n",
            "        [ 0.8779, -1.0918],\n",
            "        [ 0.5958, -0.6863],\n",
            "        [-0.1543,  0.2643],\n",
            "        [-1.4910,  1.5155],\n",
            "        [ 0.2840, -0.7317],\n",
            "        [-0.8988,  0.9876],\n",
            "        [ 0.9249, -1.1331],\n",
            "        [-0.7466,  1.0514],\n",
            "        [ 0.7924, -1.0931],\n",
            "        [ 0.7553, -0.9827],\n",
            "        [ 0.6150, -0.9847],\n",
            "        [-1.2326,  1.2355],\n",
            "        [-1.0711,  1.1429],\n",
            "        [ 0.6663, -1.2274],\n",
            "        [ 0.8241, -0.9342],\n",
            "        [ 0.4892, -0.8608],\n",
            "        [ 1.0604, -1.2264],\n",
            "        [ 0.8363, -0.8988],\n",
            "        [-1.5119,  1.5296],\n",
            "        [-0.5026,  0.2670],\n",
            "        [ 0.8798, -1.2182],\n",
            "        [-1.0790,  1.2090],\n",
            "        [ 1.0904, -1.3150],\n",
            "        [ 0.3970, -0.5691],\n",
            "        [-1.3475,  1.6019]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9066, -1.2801],\n",
            "        [-1.0352,  0.9669],\n",
            "        [ 0.7933, -1.0751],\n",
            "        [-1.2048,  1.5182],\n",
            "        [ 0.9371, -1.0127],\n",
            "        [ 0.1531, -0.2450],\n",
            "        [ 0.8409, -1.2305],\n",
            "        [ 0.8385, -0.9578],\n",
            "        [-0.1179,  0.0235],\n",
            "        [ 0.1750, -0.4513],\n",
            "        [-0.9696,  1.0610],\n",
            "        [ 0.9440, -1.3008],\n",
            "        [ 0.6300, -0.9230],\n",
            "        [-0.6404,  0.3008],\n",
            "        [-1.1833,  1.1703],\n",
            "        [ 0.7790, -1.0353],\n",
            "        [ 0.8569, -1.0221],\n",
            "        [-0.6709,  0.9289],\n",
            "        [ 0.0041,  0.0677],\n",
            "        [ 0.1977, -0.5751],\n",
            "        [-0.0180, -0.4526],\n",
            "        [ 0.7355, -1.0308],\n",
            "        [-1.2891,  1.4627],\n",
            "        [ 0.7657, -1.0074],\n",
            "        [ 0.0924,  0.1353],\n",
            "        [ 0.0483, -0.5602],\n",
            "        [ 1.0937, -1.1427],\n",
            "        [ 0.5660, -1.0523],\n",
            "        [-0.8115,  0.6609],\n",
            "        [ 0.7445, -0.8761],\n",
            "        [ 0.9374, -1.1584],\n",
            "        [ 0.5883, -0.9871]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9545,  1.0125],\n",
            "        [ 0.8171, -0.4243],\n",
            "        [ 1.0781, -1.3352],\n",
            "        [ 0.4347, -0.9024],\n",
            "        [-0.8681,  0.6305],\n",
            "        [ 0.6565, -0.8571],\n",
            "        [ 1.0704, -0.9967],\n",
            "        [ 0.7084, -1.2898],\n",
            "        [ 0.8753, -1.0563],\n",
            "        [-1.1371,  1.1044],\n",
            "        [ 0.7397, -0.8172],\n",
            "        [-0.8716,  1.0284],\n",
            "        [-1.4804,  1.1463],\n",
            "        [-0.4914,  0.8606],\n",
            "        [ 0.7210, -1.0928],\n",
            "        [ 1.0191, -1.0473],\n",
            "        [ 0.9214, -0.8491],\n",
            "        [ 0.3786, -0.5103],\n",
            "        [-1.0669,  0.9456],\n",
            "        [ 0.7718, -1.0133],\n",
            "        [-1.2303,  1.4151],\n",
            "        [ 0.9156, -1.2268],\n",
            "        [ 0.8264, -1.2374],\n",
            "        [ 0.1724, -0.1803],\n",
            "        [ 1.0504, -1.2487],\n",
            "        [ 0.8080, -1.1149],\n",
            "        [ 1.1693, -1.3855],\n",
            "        [ 0.3531, -0.8295],\n",
            "        [-1.0120,  1.1050],\n",
            "        [-1.0121,  1.1157],\n",
            "        [ 0.6783, -0.6663],\n",
            "        [ 0.9837, -1.3642]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2694, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-8.9539e-01,  1.1993e+00],\n",
            "        [ 3.8850e-01, -6.2416e-01],\n",
            "        [ 1.1476e+00, -1.4952e+00],\n",
            "        [ 7.3774e-01, -9.1667e-01],\n",
            "        [ 9.0640e-01, -1.3058e+00],\n",
            "        [-1.0859e+00,  9.7850e-01],\n",
            "        [ 1.0626e+00, -1.0426e+00],\n",
            "        [ 6.8502e-01, -5.6356e-01],\n",
            "        [ 7.3186e-01, -7.0864e-01],\n",
            "        [-1.1332e+00,  1.3209e+00],\n",
            "        [ 5.9535e-01, -7.9542e-01],\n",
            "        [ 7.1230e-01, -1.1073e+00],\n",
            "        [ 7.4199e-01, -1.3754e+00],\n",
            "        [ 8.3979e-01, -1.1814e+00],\n",
            "        [ 9.3319e-01, -7.2691e-01],\n",
            "        [-8.7329e-01,  8.9248e-01],\n",
            "        [ 1.4683e-01,  9.7608e-04],\n",
            "        [ 9.1208e-01, -1.4319e+00],\n",
            "        [ 2.0400e-01, -3.1283e-01],\n",
            "        [ 7.5750e-01, -7.8135e-01],\n",
            "        [ 5.3694e-01, -8.2815e-01],\n",
            "        [ 9.1673e-01, -1.0625e+00],\n",
            "        [ 1.1523e+00, -1.0098e+00],\n",
            "        [-8.6209e-01,  9.5319e-01],\n",
            "        [ 1.8857e-01, -4.6263e-01],\n",
            "        [ 8.4066e-01, -1.1646e+00],\n",
            "        [ 8.7734e-01, -1.1682e+00],\n",
            "        [ 9.5198e-01, -1.0194e+00],\n",
            "        [-1.1270e+00,  1.6068e+00],\n",
            "        [-1.4196e+00,  1.3377e+00],\n",
            "        [-1.5533e+00,  1.3907e+00],\n",
            "        [-1.1988e+00,  1.1618e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9411, -1.0773],\n",
            "        [ 0.5107, -0.8708],\n",
            "        [-0.8788,  1.1785],\n",
            "        [ 0.8336, -1.0253],\n",
            "        [ 0.7436, -1.0923],\n",
            "        [ 0.5756, -1.0500],\n",
            "        [-0.3843,  0.2482],\n",
            "        [ 0.6390, -0.9787],\n",
            "        [ 0.5241, -1.0545],\n",
            "        [ 0.3749, -0.4118],\n",
            "        [ 0.8836, -0.9045],\n",
            "        [ 0.0137, -0.1620],\n",
            "        [ 0.8256, -0.9006],\n",
            "        [-0.5077,  0.6806],\n",
            "        [ 0.6467, -0.8429],\n",
            "        [ 0.6681, -1.1386],\n",
            "        [ 0.9071, -1.3224],\n",
            "        [ 0.8260, -1.1154],\n",
            "        [-0.4698,  0.0820],\n",
            "        [-0.3000,  0.1911],\n",
            "        [-1.3766,  1.3849],\n",
            "        [ 0.0654, -0.4185],\n",
            "        [-1.1028,  0.9844],\n",
            "        [ 1.0648, -0.9626],\n",
            "        [-1.4435,  1.2588],\n",
            "        [ 0.9171, -1.3494],\n",
            "        [ 0.6487, -1.0264],\n",
            "        [-0.2234,  0.1606],\n",
            "        [-0.3798, -0.0034],\n",
            "        [ 0.7125, -1.2106],\n",
            "        [ 0.9383, -0.8954],\n",
            "        [-0.0622, -0.2125]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4827, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3446, -0.7485],\n",
            "        [ 0.9906, -1.1854],\n",
            "        [-1.2358,  1.5701],\n",
            "        [ 0.6433, -0.7333],\n",
            "        [-0.5759,  0.5207],\n",
            "        [ 0.7216, -1.1767],\n",
            "        [-0.9510,  1.1746],\n",
            "        [ 0.8914, -0.8766],\n",
            "        [ 0.7208, -1.3294],\n",
            "        [-0.2139,  0.3541],\n",
            "        [ 0.6446, -0.7798],\n",
            "        [ 0.9010, -1.2348],\n",
            "        [ 0.3086, -0.1182],\n",
            "        [-0.8283,  0.8121],\n",
            "        [ 0.9051, -1.0223],\n",
            "        [ 0.1606, -0.5051],\n",
            "        [ 0.7271, -0.8513],\n",
            "        [ 0.5191, -0.7574],\n",
            "        [ 0.8210, -0.8316],\n",
            "        [ 0.8254, -1.2248],\n",
            "        [ 1.2405, -1.0906],\n",
            "        [-1.5442,  1.3848],\n",
            "        [ 0.3096, -0.5272],\n",
            "        [ 0.8350, -1.1512],\n",
            "        [-0.0958, -0.5014],\n",
            "        [ 0.9638, -1.2715],\n",
            "        [-1.4556,  1.6401],\n",
            "        [-1.4277,  1.4468],\n",
            "        [ 0.4215, -0.6456],\n",
            "        [-1.1646,  0.9345],\n",
            "        [ 0.8745, -0.9238],\n",
            "        [-0.5389,  0.4761]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1908,  0.0275],\n",
            "        [-1.1278,  1.2017],\n",
            "        [-1.4383,  1.5248],\n",
            "        [ 0.7101, -0.9834],\n",
            "        [ 1.2981, -1.3659],\n",
            "        [ 0.9761, -1.3003],\n",
            "        [ 0.9930, -1.0642],\n",
            "        [-0.5937,  0.3542],\n",
            "        [ 0.8618, -1.1740],\n",
            "        [ 0.8191, -0.9047],\n",
            "        [-1.4450,  1.4081],\n",
            "        [ 0.9857, -1.2025],\n",
            "        [ 0.8579, -0.9796],\n",
            "        [ 1.0069, -0.9879],\n",
            "        [ 0.9657, -1.0208],\n",
            "        [ 0.0463, -0.3820],\n",
            "        [ 0.7098, -1.0405],\n",
            "        [ 0.2064, -0.3510],\n",
            "        [-1.2568,  1.3950],\n",
            "        [ 0.7830, -0.9588],\n",
            "        [ 0.5593, -0.9733],\n",
            "        [ 0.8773, -0.9941],\n",
            "        [ 0.2793, -0.5191],\n",
            "        [-0.8511,  0.7179],\n",
            "        [ 0.5929, -0.9910],\n",
            "        [-1.0011,  1.4404],\n",
            "        [ 0.9318, -1.2255],\n",
            "        [ 0.8133, -0.9389],\n",
            "        [ 0.6537, -0.7763],\n",
            "        [ 0.7786, -0.7572],\n",
            "        [ 0.7743, -1.0014],\n",
            "        [ 0.8039, -1.1561]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8565, -0.9525],\n",
            "        [-0.9638,  0.6190],\n",
            "        [ 0.2674, -0.5666],\n",
            "        [ 0.5482, -1.0850],\n",
            "        [ 0.9607, -1.0102],\n",
            "        [ 0.5697, -1.0898],\n",
            "        [ 0.8955, -1.1990],\n",
            "        [-1.3447,  1.3318],\n",
            "        [ 0.7303, -1.1650],\n",
            "        [ 0.7599, -0.9510],\n",
            "        [ 1.1253, -1.4004],\n",
            "        [-1.0251,  1.3090],\n",
            "        [ 0.8062, -0.8098],\n",
            "        [-1.2189,  1.3778],\n",
            "        [-1.3536,  1.5065],\n",
            "        [ 0.8902, -0.9271],\n",
            "        [ 0.3702, -0.6134],\n",
            "        [ 1.1648, -0.9437],\n",
            "        [ 0.7878, -1.1641],\n",
            "        [ 0.6144, -1.0012],\n",
            "        [ 0.1243, -0.4129],\n",
            "        [ 0.2272, -0.7735],\n",
            "        [ 0.8926, -1.1625],\n",
            "        [ 0.9107, -1.1744],\n",
            "        [ 0.8526, -1.0753],\n",
            "        [ 0.5484, -0.9750],\n",
            "        [-1.2311,  1.2956],\n",
            "        [ 0.1246, -0.3191],\n",
            "        [ 1.0156, -1.2529],\n",
            "        [ 0.7796, -0.9465],\n",
            "        [ 0.6942, -0.3643],\n",
            "        [ 0.7049, -1.2180]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2061, -0.4808],\n",
            "        [ 0.3977, -1.0454],\n",
            "        [-1.2257,  1.4550],\n",
            "        [-0.4119,  0.0961],\n",
            "        [ 0.8390, -0.8179],\n",
            "        [-0.9237,  1.3959],\n",
            "        [ 0.8472, -1.1266],\n",
            "        [ 0.5100, -1.0159],\n",
            "        [ 0.4708, -0.3900],\n",
            "        [ 0.7849, -0.9450],\n",
            "        [ 0.8834, -1.1801],\n",
            "        [-1.1365,  1.4540],\n",
            "        [-1.3615,  1.4419],\n",
            "        [ 0.6604, -0.9844],\n",
            "        [ 0.0391, -0.2431],\n",
            "        [ 1.0072, -1.1021],\n",
            "        [ 0.8336, -0.8381],\n",
            "        [ 0.5921, -0.9733],\n",
            "        [ 0.8270, -0.9969],\n",
            "        [ 0.5503, -0.8559],\n",
            "        [ 0.4970, -0.8181],\n",
            "        [ 0.6449, -0.7036],\n",
            "        [ 0.3944, -0.7469],\n",
            "        [ 0.5443, -0.4543],\n",
            "        [ 0.7835, -0.7567],\n",
            "        [ 0.6312, -1.1148],\n",
            "        [ 0.1667, -0.2432],\n",
            "        [ 0.5812, -0.8247],\n",
            "        [ 0.9597, -0.8707],\n",
            "        [ 1.0448, -1.2665],\n",
            "        [-1.0603,  1.3558],\n",
            "        [-0.1002,  0.0109]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4747, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5580, -0.9710],\n",
            "        [-0.2359,  0.6659],\n",
            "        [ 0.7758, -0.7857],\n",
            "        [ 0.8988, -0.9984],\n",
            "        [ 1.0311, -1.1208],\n",
            "        [-0.8946,  1.1752],\n",
            "        [-1.4119,  1.4476],\n",
            "        [ 0.3978, -0.8030],\n",
            "        [ 1.0121, -0.9146],\n",
            "        [-1.1534,  1.3140],\n",
            "        [-1.3056,  1.4798],\n",
            "        [ 0.5251, -0.2877],\n",
            "        [-0.4970,  0.7664],\n",
            "        [-0.6540,  0.5704],\n",
            "        [-0.4208,  0.6315],\n",
            "        [ 0.0935, -0.0290],\n",
            "        [ 0.8624, -1.2217],\n",
            "        [ 0.7853, -1.3125],\n",
            "        [ 0.6034, -0.6816],\n",
            "        [-0.2639,  0.3749],\n",
            "        [ 0.9164, -1.2796],\n",
            "        [ 0.8432, -1.0424],\n",
            "        [-1.1884,  1.2365],\n",
            "        [-0.6445,  0.5578],\n",
            "        [-1.2489,  1.4982],\n",
            "        [ 0.9501, -1.0720],\n",
            "        [ 0.8496, -0.9159],\n",
            "        [ 0.5319, -0.9917],\n",
            "        [ 0.8766, -1.1352],\n",
            "        [ 1.0053, -0.8587],\n",
            "        [ 0.4550, -0.8078],\n",
            "        [-1.5408,  1.3599]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4523, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7105, -1.0786],\n",
            "        [ 0.6579, -1.0568],\n",
            "        [ 1.0971, -1.0548],\n",
            "        [ 0.7185, -1.1701],\n",
            "        [ 0.4699, -0.4246],\n",
            "        [ 0.0520, -0.0644],\n",
            "        [-1.3483,  1.3136],\n",
            "        [-0.8483,  0.9407],\n",
            "        [ 0.8728, -1.1347],\n",
            "        [-0.7165,  0.7758],\n",
            "        [ 0.6747, -1.1717],\n",
            "        [ 0.8294, -0.9704],\n",
            "        [ 0.5137, -0.7082],\n",
            "        [ 0.0466, -0.2687],\n",
            "        [-0.6613,  0.5278],\n",
            "        [ 0.2087, -0.3158],\n",
            "        [ 0.1635, -0.6762],\n",
            "        [-1.3742,  1.5250],\n",
            "        [ 0.1272, -0.3812],\n",
            "        [-0.3903,  0.3728],\n",
            "        [ 0.8666, -1.1130],\n",
            "        [-1.0976,  1.1180],\n",
            "        [ 0.5952, -0.7586],\n",
            "        [ 0.9493, -1.1751],\n",
            "        [ 1.0789, -0.9697],\n",
            "        [-0.4558,  0.6016],\n",
            "        [ 0.8726, -1.0882],\n",
            "        [ 0.7996, -1.0525],\n",
            "        [-0.3875,  0.3007],\n",
            "        [-1.3051,  1.6327],\n",
            "        [-1.3516,  1.4313],\n",
            "        [-1.3412,  1.5948]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4195,  1.5642],\n",
            "        [ 0.8358, -1.2397],\n",
            "        [ 0.2244, -0.5716],\n",
            "        [ 0.6794, -0.3608],\n",
            "        [ 0.4947, -0.3945],\n",
            "        [ 0.9164, -1.0988],\n",
            "        [ 0.8803, -1.1778],\n",
            "        [-0.5014,  0.6467],\n",
            "        [ 0.6361, -1.2374],\n",
            "        [ 0.7294, -1.0573],\n",
            "        [ 0.6749, -0.6165],\n",
            "        [-1.2528,  1.4384],\n",
            "        [-0.1637,  0.1139],\n",
            "        [ 0.9254, -0.8730],\n",
            "        [-1.0763,  1.3600],\n",
            "        [-1.0551,  1.1908],\n",
            "        [-0.2412,  0.0314],\n",
            "        [-0.9346,  1.0100],\n",
            "        [-1.0439,  1.0825],\n",
            "        [ 0.7810, -0.8994],\n",
            "        [ 0.1621, -0.1198],\n",
            "        [ 0.8677, -1.1609],\n",
            "        [ 0.9429, -1.3663],\n",
            "        [ 1.1046, -1.0685],\n",
            "        [ 1.0229, -1.1214],\n",
            "        [ 0.7349, -0.9605],\n",
            "        [ 0.8248, -1.1782],\n",
            "        [ 0.7742, -0.5731],\n",
            "        [-0.7014,  0.7610],\n",
            "        [ 0.4192, -0.9624],\n",
            "        [ 0.8465, -1.2139],\n",
            "        [ 0.9718, -0.8013]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9777, -0.8506],\n",
            "        [ 0.1062, -0.1022],\n",
            "        [-1.2314,  1.3399],\n",
            "        [-1.3760,  1.5681],\n",
            "        [ 0.1700, -0.3513],\n",
            "        [ 1.0057, -1.1260],\n",
            "        [-1.0197,  1.3325],\n",
            "        [ 0.8343, -0.8398],\n",
            "        [-1.1623,  1.3880],\n",
            "        [-1.3865,  1.4221],\n",
            "        [ 0.9878, -1.3791],\n",
            "        [ 0.3462, -0.8093],\n",
            "        [-1.1627,  1.1534],\n",
            "        [ 1.0552, -0.7927],\n",
            "        [-0.3089,  0.3744],\n",
            "        [ 0.8007, -0.9940],\n",
            "        [ 0.6734, -1.0567],\n",
            "        [-0.9738,  1.1722],\n",
            "        [ 0.9357, -1.0931],\n",
            "        [-0.9087,  1.0584],\n",
            "        [ 0.0033, -0.2257],\n",
            "        [ 0.8919, -1.0179],\n",
            "        [-1.1197,  1.1337],\n",
            "        [-1.2075,  1.1803],\n",
            "        [ 0.7784, -1.2245],\n",
            "        [-1.3406,  1.3127],\n",
            "        [ 0.5512, -0.8037],\n",
            "        [-0.8833,  1.0361],\n",
            "        [ 0.8301, -0.8319],\n",
            "        [-1.3400,  1.1582],\n",
            "        [-0.7357,  0.8734],\n",
            "        [ 1.1598, -1.0976]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch   100  of    191.    Elapsed: 0:02:18.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7946,  0.6991],\n",
            "        [-1.2989,  1.2681],\n",
            "        [ 1.0695, -0.9790],\n",
            "        [-0.8277,  0.7267],\n",
            "        [ 0.9202, -1.1145],\n",
            "        [ 0.9155, -1.0757],\n",
            "        [-0.9894,  1.4225],\n",
            "        [ 0.3532, -0.4221],\n",
            "        [-1.1861,  0.9344],\n",
            "        [ 0.4602, -0.4679],\n",
            "        [-1.2687,  1.6446],\n",
            "        [ 0.6036, -1.2093],\n",
            "        [ 0.6153, -0.8256],\n",
            "        [ 0.9307, -1.0879],\n",
            "        [ 0.7829, -1.0683],\n",
            "        [-0.0710, -0.0258],\n",
            "        [-1.4651,  1.3785],\n",
            "        [ 0.8117, -1.0708],\n",
            "        [-1.3055,  1.4845],\n",
            "        [ 0.7777, -0.7524],\n",
            "        [ 0.3498, -0.5998],\n",
            "        [ 0.6628, -1.0802],\n",
            "        [ 0.9677, -0.9404],\n",
            "        [ 0.8570, -0.4276],\n",
            "        [ 0.8199, -1.2582],\n",
            "        [ 0.7903, -1.0717],\n",
            "        [ 0.9134, -0.7045],\n",
            "        [-0.4221,  0.3082],\n",
            "        [ 0.9435, -1.0744],\n",
            "        [ 0.9001, -0.9342],\n",
            "        [-1.3348,  1.4525],\n",
            "        [-0.7409,  1.1827]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3444,  1.2471],\n",
            "        [-0.6882,  0.8187],\n",
            "        [-1.3103,  1.4460],\n",
            "        [-0.8373,  0.8960],\n",
            "        [ 0.5355, -0.7821],\n",
            "        [ 0.6320, -0.1830],\n",
            "        [ 1.0841, -1.3764],\n",
            "        [-0.6934,  0.6642],\n",
            "        [ 0.8184, -1.1785],\n",
            "        [-0.4400,  0.3773],\n",
            "        [ 0.1852, -0.6204],\n",
            "        [ 0.9951, -1.0706],\n",
            "        [ 0.5968, -0.6690],\n",
            "        [ 0.4588, -0.7326],\n",
            "        [ 0.5862, -0.5203],\n",
            "        [ 0.6218, -0.7002],\n",
            "        [ 0.7681, -0.9267],\n",
            "        [ 0.1345, -0.1528],\n",
            "        [-0.5443,  0.8440],\n",
            "        [ 0.4940, -0.6078],\n",
            "        [ 0.9516, -0.8737],\n",
            "        [ 1.0450, -0.8667],\n",
            "        [ 0.9009, -1.0192],\n",
            "        [ 0.9722, -0.9342],\n",
            "        [ 0.8863, -0.8887],\n",
            "        [-1.5085,  1.3209],\n",
            "        [-0.7576,  1.0182],\n",
            "        [ 0.8619, -1.2480],\n",
            "        [ 0.5649, -1.0400],\n",
            "        [-1.0042,  1.2843],\n",
            "        [-1.0354,  1.3620],\n",
            "        [ 0.7234, -1.1662]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0838, -1.2232],\n",
            "        [ 1.1347, -1.2561],\n",
            "        [-0.5276,  0.3587],\n",
            "        [ 0.8790, -1.0811],\n",
            "        [ 0.5599, -1.0406],\n",
            "        [ 0.9272, -0.9657],\n",
            "        [ 0.8719, -0.7276],\n",
            "        [ 0.3182, -0.8531],\n",
            "        [ 0.5931, -0.8141],\n",
            "        [-0.8513,  1.1129],\n",
            "        [-1.1905,  1.6278],\n",
            "        [ 0.7823, -1.0687],\n",
            "        [-1.2460,  1.6213],\n",
            "        [ 0.9080, -1.1075],\n",
            "        [ 0.8065, -0.9501],\n",
            "        [ 0.9480, -1.3037],\n",
            "        [ 0.6647, -0.6937],\n",
            "        [ 0.3588, -0.4666],\n",
            "        [ 0.4812, -0.6222],\n",
            "        [ 0.7888, -0.7840],\n",
            "        [ 0.7680, -1.1378],\n",
            "        [-0.0488, -0.1974],\n",
            "        [ 1.1893, -1.1725],\n",
            "        [ 0.9261, -1.1985],\n",
            "        [ 0.8601, -1.0760],\n",
            "        [ 0.8918, -0.9352],\n",
            "        [ 0.6699, -0.7984],\n",
            "        [ 0.7544, -0.9226],\n",
            "        [-1.5642,  1.2968],\n",
            "        [ 0.8143, -1.1466],\n",
            "        [-1.3183,  1.4142],\n",
            "        [ 0.7474, -0.9517]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7061,  1.1036],\n",
            "        [-1.2757,  1.6172],\n",
            "        [ 0.7862, -1.0534],\n",
            "        [ 1.0519, -1.2414],\n",
            "        [ 0.8609, -0.6741],\n",
            "        [-0.9744,  0.8952],\n",
            "        [ 0.6387, -0.8828],\n",
            "        [-0.6752,  0.8849],\n",
            "        [ 1.1205, -1.2695],\n",
            "        [ 0.6127, -0.8297],\n",
            "        [ 0.9466, -1.5042],\n",
            "        [ 0.6484, -0.6582],\n",
            "        [-0.5244,  0.6965],\n",
            "        [ 0.6475, -1.0725],\n",
            "        [ 0.8848, -1.2976],\n",
            "        [ 0.6302, -0.7597],\n",
            "        [-1.1489,  1.4097],\n",
            "        [ 0.9498, -1.1910],\n",
            "        [ 0.9179, -1.2175],\n",
            "        [ 0.1079, -0.0728],\n",
            "        [ 0.7253, -0.6859],\n",
            "        [-0.6260,  0.5804],\n",
            "        [-0.6435,  0.8187],\n",
            "        [-1.5223,  1.5469],\n",
            "        [ 1.0228, -0.9738],\n",
            "        [ 1.0542, -0.8399],\n",
            "        [ 1.0634, -1.0623],\n",
            "        [-1.2215,  1.5520],\n",
            "        [-0.6907,  0.5865],\n",
            "        [ 0.7179, -1.4113],\n",
            "        [-1.1926,  1.5781],\n",
            "        [ 0.4784, -1.0070]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3495, -0.6636],\n",
            "        [ 0.8672, -1.0525],\n",
            "        [-1.1809,  1.1890],\n",
            "        [ 1.0185, -1.1550],\n",
            "        [ 0.0662, -0.0467],\n",
            "        [-1.1164,  1.3116],\n",
            "        [ 0.8990, -1.0680],\n",
            "        [-1.0926,  1.0055],\n",
            "        [ 0.4098, -0.7191],\n",
            "        [ 0.4696, -0.9938],\n",
            "        [ 0.8703, -1.1610],\n",
            "        [-0.1844,  0.1092],\n",
            "        [-1.3921,  1.2376],\n",
            "        [ 0.8083, -1.1743],\n",
            "        [-0.6066,  0.6721],\n",
            "        [ 0.9221, -0.7886],\n",
            "        [ 0.8191, -1.0445],\n",
            "        [-1.5300,  1.5390],\n",
            "        [ 0.6265, -0.8633],\n",
            "        [-0.8533,  1.4024],\n",
            "        [-1.0785,  1.2057],\n",
            "        [ 0.7997, -0.9798],\n",
            "        [ 0.9476, -0.9878],\n",
            "        [ 0.7057, -1.2151],\n",
            "        [ 0.6747, -0.8002],\n",
            "        [ 0.8178, -1.1119],\n",
            "        [ 1.0428, -1.0485],\n",
            "        [ 0.9448, -0.6609],\n",
            "        [ 0.5033, -0.8331],\n",
            "        [ 0.6519, -0.9874],\n",
            "        [ 0.8064, -1.1063],\n",
            "        [-0.6788,  1.0715]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0918, -1.1970],\n",
            "        [ 0.9943, -0.9855],\n",
            "        [ 0.4207, -0.9240],\n",
            "        [-1.2385,  1.3468],\n",
            "        [ 0.8360, -1.3418],\n",
            "        [ 0.7351, -1.0645],\n",
            "        [-1.3878,  1.5458],\n",
            "        [-1.3640,  1.3674],\n",
            "        [ 0.9990, -1.1969],\n",
            "        [ 0.7226, -1.0136],\n",
            "        [ 0.8605, -1.1991],\n",
            "        [ 1.0029, -0.9020],\n",
            "        [-0.1596, -0.1151],\n",
            "        [ 1.0018, -1.1138],\n",
            "        [-0.7033,  0.9100],\n",
            "        [ 0.5144, -0.7536],\n",
            "        [-0.7766,  1.0541],\n",
            "        [ 0.9774, -0.7073],\n",
            "        [ 0.3952, -0.7885],\n",
            "        [-1.3015,  1.5741],\n",
            "        [ 0.3422, -0.6421],\n",
            "        [-1.2862,  1.2292],\n",
            "        [-0.6967,  0.7929],\n",
            "        [-1.1196,  1.2535],\n",
            "        [ 0.2784, -0.4429],\n",
            "        [ 0.9823, -1.2435],\n",
            "        [-1.5401,  1.4774],\n",
            "        [ 0.9084, -1.3176],\n",
            "        [-0.0457, -0.3893],\n",
            "        [ 0.8584, -1.1757],\n",
            "        [ 0.0222, -0.1427],\n",
            "        [ 0.9826, -1.3793]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4734, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1557, -1.1781],\n",
            "        [-1.3544,  1.5538],\n",
            "        [ 0.9124, -1.2188],\n",
            "        [-0.6911,  0.6221],\n",
            "        [ 0.9327, -1.3420],\n",
            "        [-0.7847,  0.8996],\n",
            "        [-0.3074,  0.3237],\n",
            "        [ 0.9715, -0.8853],\n",
            "        [ 0.7798, -0.7745],\n",
            "        [ 0.1986, -0.1037],\n",
            "        [ 0.7446, -1.1187],\n",
            "        [ 0.5165, -0.6363],\n",
            "        [ 0.1176, -0.6271],\n",
            "        [ 0.7063, -0.9106],\n",
            "        [ 0.8151, -0.9443],\n",
            "        [-0.8881,  1.1576],\n",
            "        [-1.3425,  1.4199],\n",
            "        [-1.4725,  1.6869],\n",
            "        [-1.0879,  1.5071],\n",
            "        [ 0.6005, -0.8220],\n",
            "        [ 0.6399, -0.6562],\n",
            "        [ 0.1105,  0.0158],\n",
            "        [-1.2491,  1.3843],\n",
            "        [-1.4615,  1.4411],\n",
            "        [ 0.8828, -1.1914],\n",
            "        [-0.8105,  0.7755],\n",
            "        [ 0.7044, -0.7817],\n",
            "        [-1.2212,  1.4669],\n",
            "        [ 0.2875, -0.4578],\n",
            "        [ 0.9095, -0.9645],\n",
            "        [-1.1334,  1.5408],\n",
            "        [-0.3847,  0.1132]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1058,  0.2622],\n",
            "        [ 0.1160, -0.5972],\n",
            "        [-1.2152,  1.2817],\n",
            "        [-0.5002,  0.1312],\n",
            "        [ 0.9062, -0.9053],\n",
            "        [ 0.9674, -0.6248],\n",
            "        [ 0.2643, -0.0188],\n",
            "        [ 0.7125, -0.9951],\n",
            "        [-0.4544,  0.5586],\n",
            "        [ 0.7314, -0.8690],\n",
            "        [ 0.9359, -0.8419],\n",
            "        [ 0.9456, -1.0544],\n",
            "        [ 0.7800, -0.6402],\n",
            "        [-1.4446,  1.4399],\n",
            "        [-0.3960,  0.4575],\n",
            "        [-1.2235,  1.4414],\n",
            "        [ 0.7646, -1.1389],\n",
            "        [-1.2065,  1.0279],\n",
            "        [ 0.1624, -0.3189],\n",
            "        [ 0.1722, -0.4523],\n",
            "        [ 0.7414, -1.1674],\n",
            "        [ 0.2632, -0.5883],\n",
            "        [-1.4463,  1.4326],\n",
            "        [-1.0725,  1.6555],\n",
            "        [ 0.8088, -1.2397],\n",
            "        [-1.2548,  1.5025],\n",
            "        [-1.2768,  1.5316],\n",
            "        [ 0.3581,  0.0537],\n",
            "        [ 0.5290, -1.1072],\n",
            "        [ 0.5904, -0.8880],\n",
            "        [ 0.9596, -0.6074],\n",
            "        [ 0.5982, -0.6037]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7027, -0.9534],\n",
            "        [ 0.9180, -1.0313],\n",
            "        [-1.4520,  1.6238],\n",
            "        [ 0.8619, -1.2408],\n",
            "        [ 1.0667, -1.1277],\n",
            "        [ 0.8645, -1.0870],\n",
            "        [-1.3319,  1.4547],\n",
            "        [ 0.7695, -0.9833],\n",
            "        [ 0.6888, -1.0764],\n",
            "        [-0.8958,  0.8226],\n",
            "        [ 0.9687, -1.2867],\n",
            "        [ 0.5283, -1.1274],\n",
            "        [ 0.2709, -0.4684],\n",
            "        [-1.4455,  1.3844],\n",
            "        [ 0.2963, -0.7582],\n",
            "        [ 0.7059, -0.8317],\n",
            "        [-0.0949,  0.0254],\n",
            "        [ 0.6562, -1.0450],\n",
            "        [ 0.3838, -0.7379],\n",
            "        [ 1.0797, -1.0325],\n",
            "        [ 0.6338, -0.5919],\n",
            "        [ 0.6152, -0.5018],\n",
            "        [-0.5808,  0.6269],\n",
            "        [-1.4070,  1.4076],\n",
            "        [-1.4393,  1.5292],\n",
            "        [-0.7454,  0.7102],\n",
            "        [-0.3853,  0.2409],\n",
            "        [-1.0335,  0.9593],\n",
            "        [-0.8990,  1.2184],\n",
            "        [ 0.4631, -0.8375],\n",
            "        [ 0.2189, -0.0977],\n",
            "        [-1.5065,  1.6290]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7310, -0.8928],\n",
            "        [ 0.9040, -1.1123],\n",
            "        [ 0.4855, -1.0836],\n",
            "        [-0.7906,  0.6858],\n",
            "        [ 0.5133, -0.7698],\n",
            "        [-0.2719,  0.2770],\n",
            "        [ 0.6899, -0.8465],\n",
            "        [ 0.3917, -0.3595],\n",
            "        [ 0.9162, -0.9732],\n",
            "        [ 1.0302, -0.9868],\n",
            "        [ 0.8307, -1.2619],\n",
            "        [ 0.9866, -0.9020],\n",
            "        [-1.2214,  1.7339],\n",
            "        [-0.7798,  1.0210],\n",
            "        [ 0.6918, -1.0240],\n",
            "        [ 0.6884, -0.9858],\n",
            "        [ 0.0272, -0.1036],\n",
            "        [ 0.8119, -0.9382],\n",
            "        [-1.8025,  1.4881],\n",
            "        [ 0.3612, -0.7031],\n",
            "        [ 0.7262, -1.0622],\n",
            "        [ 0.8846, -1.0190],\n",
            "        [ 0.8156, -0.9908],\n",
            "        [ 0.7579, -1.1729],\n",
            "        [ 0.9954, -1.0700],\n",
            "        [ 1.0456, -0.9836],\n",
            "        [-0.9360,  1.0014],\n",
            "        [-1.0780,  1.2375],\n",
            "        [ 0.3114, -0.2144],\n",
            "        [ 0.7895, -0.7326],\n",
            "        [-0.5664,  0.8775],\n",
            "        [ 0.8402, -1.0574]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4237, -0.1865],\n",
            "        [-0.3670,  0.0871],\n",
            "        [-0.2581,  0.3853],\n",
            "        [-0.9683,  1.2190],\n",
            "        [ 1.0252, -1.0147],\n",
            "        [ 1.0547, -0.9451],\n",
            "        [ 0.7549, -0.9261],\n",
            "        [ 0.9477, -0.9839],\n",
            "        [-1.1263,  1.4495],\n",
            "        [ 0.9796, -0.9477],\n",
            "        [ 0.9232, -1.3260],\n",
            "        [ 0.2661, -0.6578],\n",
            "        [ 0.9517, -1.2082],\n",
            "        [-1.0292,  1.4891],\n",
            "        [ 0.7316, -0.7475],\n",
            "        [ 0.9116, -0.8832],\n",
            "        [ 0.2282, -0.3191],\n",
            "        [-0.9881,  0.9879],\n",
            "        [-1.0094,  1.0288],\n",
            "        [ 0.7267, -1.0359],\n",
            "        [-1.3830,  1.4331],\n",
            "        [ 0.3600, -0.8206],\n",
            "        [-0.6442,  0.5895],\n",
            "        [ 0.1115, -0.6143],\n",
            "        [ 1.1009, -0.9989],\n",
            "        [-1.1490,  1.2731],\n",
            "        [ 0.9917, -0.6983],\n",
            "        [-1.1309,  1.5703],\n",
            "        [ 1.1423, -0.8843],\n",
            "        [-1.2063,  1.4107],\n",
            "        [ 1.1568, -1.1857],\n",
            "        [-1.2250,  1.7068]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4070, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7009, -1.2268],\n",
            "        [ 0.7940, -1.0657],\n",
            "        [ 0.3736, -0.4835],\n",
            "        [-0.2088,  0.1292],\n",
            "        [-0.6453,  0.5800],\n",
            "        [ 0.7811, -1.1165],\n",
            "        [ 0.6571, -0.8837],\n",
            "        [-0.9727,  1.0081],\n",
            "        [ 0.1921, -0.6233],\n",
            "        [-1.5162,  1.4420],\n",
            "        [-1.3125,  1.4607],\n",
            "        [ 0.2594, -0.2389],\n",
            "        [-1.4060,  1.5480],\n",
            "        [ 0.0810, -0.4228],\n",
            "        [-1.0359,  0.9820],\n",
            "        [ 0.8161, -1.0040],\n",
            "        [-0.5091,  0.6021],\n",
            "        [ 0.7000, -0.8697],\n",
            "        [-1.3601,  1.3304],\n",
            "        [ 0.6074, -0.6670],\n",
            "        [ 0.4332, -0.6235],\n",
            "        [-1.3177,  1.2918],\n",
            "        [-1.3579,  1.5596],\n",
            "        [ 1.3177, -1.3155],\n",
            "        [ 0.2982, -0.9823],\n",
            "        [-1.2909,  1.3757],\n",
            "        [-1.1461,  1.1756],\n",
            "        [ 0.8919, -1.0760],\n",
            "        [-1.2947,  1.5035],\n",
            "        [-1.0722,  1.2314],\n",
            "        [ 1.2142, -1.0246],\n",
            "        [ 1.2712, -0.9022]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0276, -0.9201],\n",
            "        [ 0.9728, -0.9947],\n",
            "        [ 0.9123, -1.1929],\n",
            "        [ 0.7792, -0.9492],\n",
            "        [ 0.1039, -0.6230],\n",
            "        [ 0.9388, -1.0889],\n",
            "        [-0.9437,  1.2216],\n",
            "        [ 0.5352, -0.5437],\n",
            "        [ 0.7038, -1.0810],\n",
            "        [-1.4734,  1.6377],\n",
            "        [-1.5605,  1.4912],\n",
            "        [ 0.8278, -0.9505],\n",
            "        [-1.2524,  1.4887],\n",
            "        [ 0.4497, -0.5765],\n",
            "        [ 0.8221, -1.0340],\n",
            "        [ 0.1473, -0.0607],\n",
            "        [ 0.9439, -1.1013],\n",
            "        [ 0.8250, -1.1063],\n",
            "        [-1.2029,  1.7301],\n",
            "        [-1.5883,  1.3951],\n",
            "        [ 0.7780, -1.1687],\n",
            "        [ 0.4670, -0.5693],\n",
            "        [ 1.0160, -1.1941],\n",
            "        [-1.2040,  1.1360],\n",
            "        [ 0.7663, -1.1428],\n",
            "        [ 1.1095, -1.1585],\n",
            "        [-1.1225,  1.1555],\n",
            "        [ 0.4549, -0.4972],\n",
            "        [-1.7976,  1.4926],\n",
            "        [ 0.5335, -1.0123],\n",
            "        [-1.4545,  1.3921],\n",
            "        [ 0.1315,  0.0501]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4278, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3451,  1.3561],\n",
            "        [ 0.7684, -1.1167],\n",
            "        [-1.5185,  1.6236],\n",
            "        [ 1.0598, -0.7851],\n",
            "        [-0.9926,  1.3302],\n",
            "        [ 0.5799, -0.9767],\n",
            "        [ 0.7681, -1.2026],\n",
            "        [-0.1206, -0.0094],\n",
            "        [ 1.0816, -1.1133],\n",
            "        [ 0.7329, -1.0647],\n",
            "        [ 0.8601, -1.1009],\n",
            "        [ 0.9475, -1.1344],\n",
            "        [ 0.7931, -1.0751],\n",
            "        [ 0.9166, -1.1393],\n",
            "        [ 1.0624, -1.0592],\n",
            "        [ 0.9852, -0.8252],\n",
            "        [ 0.8946, -1.1428],\n",
            "        [-1.2182,  1.4638],\n",
            "        [-0.6728,  0.7698],\n",
            "        [ 0.7137, -0.8806],\n",
            "        [-0.4714,  0.5664],\n",
            "        [-0.6441,  0.9993],\n",
            "        [-1.2329,  1.4286],\n",
            "        [-1.3703,  1.2951],\n",
            "        [-0.6117,  0.4304],\n",
            "        [-0.9658,  1.0768],\n",
            "        [ 0.4587, -0.6570],\n",
            "        [ 1.0793, -0.9723],\n",
            "        [ 0.6801, -1.0595],\n",
            "        [-1.3291,  1.3084],\n",
            "        [ 0.8489, -0.8848],\n",
            "        [-1.2579,  1.4622]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4072,  1.5687],\n",
            "        [-1.1539,  1.2597],\n",
            "        [ 0.5806, -0.8208],\n",
            "        [ 0.7293, -0.9424],\n",
            "        [-1.0711,  1.5127],\n",
            "        [-0.4220,  0.3885],\n",
            "        [ 0.3140, -0.7335],\n",
            "        [-0.9983,  1.0559],\n",
            "        [ 0.6035, -0.9428],\n",
            "        [ 0.8269, -1.0452],\n",
            "        [ 0.8082, -1.2462],\n",
            "        [-0.6191,  0.9237],\n",
            "        [ 0.9280, -1.1392],\n",
            "        [ 0.8301, -0.9237],\n",
            "        [-1.3017,  1.5321],\n",
            "        [-1.3600,  1.5809],\n",
            "        [-1.2606,  1.4598],\n",
            "        [-1.3761,  1.6348],\n",
            "        [ 0.9751, -1.0919],\n",
            "        [-1.2099,  1.4996],\n",
            "        [ 0.7773, -0.9001],\n",
            "        [ 0.6976, -0.7171],\n",
            "        [ 0.2289, -0.4311],\n",
            "        [-0.3420,  0.4381],\n",
            "        [-1.1583,  1.4502],\n",
            "        [ 0.8615, -1.1696],\n",
            "        [ 0.8805, -0.9137],\n",
            "        [ 0.9019, -0.9731],\n",
            "        [-1.2734,  1.0473],\n",
            "        [ 0.3411, -0.6027],\n",
            "        [-1.1432,  1.3062],\n",
            "        [ 0.4677, -0.7960]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0834, -1.4994],\n",
            "        [ 0.7146, -1.0712],\n",
            "        [-1.3442,  1.4072],\n",
            "        [ 0.2665, -0.6405],\n",
            "        [ 0.3960, -0.9039],\n",
            "        [ 0.7305, -1.0129],\n",
            "        [ 1.0286, -1.3315],\n",
            "        [-1.3235,  1.3389],\n",
            "        [ 0.2777, -0.4595],\n",
            "        [ 0.9612, -1.1251],\n",
            "        [ 0.6263, -1.0012],\n",
            "        [ 0.8211, -1.0914],\n",
            "        [ 0.3361, -0.4951],\n",
            "        [ 0.8343, -1.0192],\n",
            "        [-1.4755,  1.6325],\n",
            "        [ 0.5531, -0.7155],\n",
            "        [ 0.1588, -0.1679],\n",
            "        [ 0.7735, -1.0815],\n",
            "        [-0.7205,  0.9311],\n",
            "        [ 0.8479, -1.1058],\n",
            "        [-1.2842,  1.5318],\n",
            "        [-0.9372,  1.1229],\n",
            "        [ 0.6536, -1.0774],\n",
            "        [ 0.3493, -0.4119],\n",
            "        [ 0.8377, -1.2266],\n",
            "        [ 1.0821, -1.2898],\n",
            "        [ 0.6171, -0.8536],\n",
            "        [ 1.0956, -1.1182],\n",
            "        [ 0.7315, -0.9594],\n",
            "        [ 0.7116, -0.7035],\n",
            "        [ 0.7839, -0.7171],\n",
            "        [-1.3957,  1.5048]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7403, -1.0810],\n",
            "        [ 1.0029, -1.2668],\n",
            "        [ 1.2920, -1.1643],\n",
            "        [ 1.0930, -1.1107],\n",
            "        [ 0.6076, -1.1352],\n",
            "        [ 0.6199, -0.5024],\n",
            "        [ 1.0968, -1.0887],\n",
            "        [-1.2022,  1.4462],\n",
            "        [ 0.6901, -0.7956],\n",
            "        [-0.9823,  1.2454],\n",
            "        [ 1.1912, -1.1314],\n",
            "        [ 0.5988, -0.7204],\n",
            "        [-0.8665,  1.2363],\n",
            "        [ 0.8102, -0.8976],\n",
            "        [-1.0758,  1.0066],\n",
            "        [ 0.4488, -0.6904],\n",
            "        [-0.6460,  0.8847],\n",
            "        [ 0.6262, -1.0195],\n",
            "        [-1.3072,  1.3671],\n",
            "        [ 0.3849, -0.5712],\n",
            "        [-1.2198,  1.2765],\n",
            "        [ 0.1668, -0.3800],\n",
            "        [-0.1892,  0.1215],\n",
            "        [ 1.0902, -1.0400],\n",
            "        [ 0.7264, -1.0395],\n",
            "        [ 0.8467, -1.3702],\n",
            "        [ 0.0992, -0.4358],\n",
            "        [ 1.0031, -0.8554],\n",
            "        [ 0.9922, -1.1842],\n",
            "        [ 0.7186, -0.6030],\n",
            "        [ 0.2642, -0.5932],\n",
            "        [ 0.8201, -0.9727]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3551,  1.2987],\n",
            "        [ 0.7710, -1.0467],\n",
            "        [ 1.0379, -1.1150],\n",
            "        [-0.7506,  0.9879],\n",
            "        [ 0.7632, -0.3735],\n",
            "        [ 1.0565, -1.1374],\n",
            "        [-0.9809,  1.0037],\n",
            "        [ 0.9674, -1.1096],\n",
            "        [-1.4564,  1.4911],\n",
            "        [-1.0944,  1.2062],\n",
            "        [-1.2980,  1.3988],\n",
            "        [-0.8190,  0.9558],\n",
            "        [-1.3704,  1.4554],\n",
            "        [ 0.8458, -1.1949],\n",
            "        [-1.1136,  1.3512],\n",
            "        [-1.1385,  1.3390],\n",
            "        [ 1.1349, -1.0390],\n",
            "        [ 0.9012, -1.0505],\n",
            "        [-0.4498,  0.4327],\n",
            "        [-0.6391,  0.5269],\n",
            "        [-0.4963,  0.3976],\n",
            "        [-1.4024,  1.2580],\n",
            "        [ 0.8066, -0.9268],\n",
            "        [ 0.7969, -1.0792],\n",
            "        [ 0.8869, -0.7623],\n",
            "        [ 0.8901, -0.9658],\n",
            "        [-0.5117,  0.5331],\n",
            "        [ 0.6838, -1.1786],\n",
            "        [-0.4603,  0.5230],\n",
            "        [ 0.9845, -1.0693],\n",
            "        [-0.6083,  0.7128],\n",
            "        [ 0.9331, -1.0071]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4544, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0126, -1.5817],\n",
            "        [ 0.7959, -0.5722],\n",
            "        [ 0.0473,  0.0766],\n",
            "        [ 0.8504, -1.0121],\n",
            "        [-1.0543,  1.3690],\n",
            "        [-1.3098,  1.4726],\n",
            "        [ 0.5040, -1.0944],\n",
            "        [ 1.0012, -1.1637],\n",
            "        [-0.6786,  0.5437],\n",
            "        [ 0.2429, -0.8833],\n",
            "        [ 0.9428, -1.1264],\n",
            "        [ 0.3169, -0.3895],\n",
            "        [ 0.4439, -0.9819],\n",
            "        [ 0.7908, -1.1691],\n",
            "        [-0.1270, -0.0205],\n",
            "        [-0.2423,  0.2250],\n",
            "        [-0.2162,  0.1473],\n",
            "        [ 0.9100, -1.0936],\n",
            "        [-0.0075, -0.0745],\n",
            "        [-1.2161,  1.1169],\n",
            "        [ 0.5618, -0.7667],\n",
            "        [-1.0063,  0.9326],\n",
            "        [-0.4399,  0.5485],\n",
            "        [ 1.0016, -0.9732],\n",
            "        [-1.2901,  1.7296],\n",
            "        [ 0.9201, -1.1074],\n",
            "        [ 0.0570, -0.0174],\n",
            "        [-1.3392,  1.6902],\n",
            "        [-1.1467,  1.2742],\n",
            "        [ 0.7444, -0.9796],\n",
            "        [ 0.8020, -1.0611],\n",
            "        [-0.9103,  1.2439]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8475, -1.1426],\n",
            "        [ 0.0165, -0.2879],\n",
            "        [-1.5049,  1.3837],\n",
            "        [ 1.1188, -1.1182],\n",
            "        [ 0.6985, -0.8506],\n",
            "        [-0.4532,  0.4569],\n",
            "        [ 0.8893, -1.0938],\n",
            "        [-1.1720,  1.0921],\n",
            "        [ 0.4525, -0.4972],\n",
            "        [-1.2508,  1.1302],\n",
            "        [-0.1938,  0.0917],\n",
            "        [ 1.0012, -1.1905],\n",
            "        [ 0.9886, -1.3304],\n",
            "        [ 0.8756, -0.9682],\n",
            "        [-1.2557,  1.3225],\n",
            "        [ 0.9866, -1.4101],\n",
            "        [-0.7809,  1.0301],\n",
            "        [ 0.5421, -0.6429],\n",
            "        [-1.1440,  1.3411],\n",
            "        [-0.8411,  0.9650],\n",
            "        [ 0.0143, -0.1753],\n",
            "        [ 0.1531, -0.4036],\n",
            "        [-0.7325,  1.0153],\n",
            "        [ 0.9232, -1.0573],\n",
            "        [ 0.8585, -1.0330],\n",
            "        [-1.1658,  1.3517],\n",
            "        [ 1.0751, -1.1552],\n",
            "        [ 0.3207, -0.2380],\n",
            "        [ 0.4825, -0.7316],\n",
            "        [-1.3858,  1.4598],\n",
            "        [ 0.0703, -0.4522],\n",
            "        [-1.5731,  1.6501]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4384, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5510, -0.6941],\n",
            "        [-0.0038, -0.2087],\n",
            "        [ 0.5937, -0.8011],\n",
            "        [ 0.9521, -1.0931],\n",
            "        [ 0.7906, -1.1352],\n",
            "        [ 0.8598, -1.0193],\n",
            "        [ 0.9281, -0.9506],\n",
            "        [-1.3560,  1.3981],\n",
            "        [ 0.8612, -1.1003],\n",
            "        [ 0.2590, -0.3665],\n",
            "        [ 0.3776, -0.8489],\n",
            "        [ 0.9913, -1.1336],\n",
            "        [ 0.8718, -1.1222],\n",
            "        [-0.7917,  0.6567],\n",
            "        [ 1.0384, -1.0646],\n",
            "        [-1.3081,  1.3548],\n",
            "        [ 0.6081, -0.8322],\n",
            "        [-1.1017,  1.2551],\n",
            "        [ 0.9584, -0.9933],\n",
            "        [ 0.4767, -0.7385],\n",
            "        [ 0.9394, -0.9111],\n",
            "        [ 0.9155, -1.1950],\n",
            "        [-0.7898,  0.6869],\n",
            "        [-0.2934,  0.4010],\n",
            "        [-1.2307,  1.3570],\n",
            "        [ 1.1312, -1.0553],\n",
            "        [-0.2513,  0.1215],\n",
            "        [ 0.6062, -0.6001],\n",
            "        [ 0.5249, -0.5335],\n",
            "        [-1.2193,  1.3740],\n",
            "        [ 0.4561, -0.5790],\n",
            "        [-1.0548,  1.3440]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7703,  0.5123],\n",
            "        [ 1.1755, -1.4371],\n",
            "        [ 0.7778, -0.9066],\n",
            "        [ 1.2256, -1.0951],\n",
            "        [ 1.0424, -1.2483],\n",
            "        [ 0.7510, -1.2840],\n",
            "        [-1.2167,  1.0169],\n",
            "        [-0.0162, -0.1710],\n",
            "        [ 0.1646, -0.3431],\n",
            "        [ 0.8628, -0.9746],\n",
            "        [ 1.0437, -1.0541],\n",
            "        [ 0.4367, -1.0947],\n",
            "        [ 0.8858, -1.1842],\n",
            "        [ 0.4899, -0.6745],\n",
            "        [ 0.5323, -0.8657],\n",
            "        [ 0.4500, -0.8170],\n",
            "        [-1.4694,  1.3580],\n",
            "        [-0.5221,  0.3496],\n",
            "        [ 1.0715, -1.0661],\n",
            "        [ 0.1577, -0.5946],\n",
            "        [ 1.0851, -1.3292],\n",
            "        [-1.5174,  1.3195],\n",
            "        [-1.4304,  1.6137],\n",
            "        [-1.3787,  1.3815],\n",
            "        [ 0.4820, -0.9625],\n",
            "        [ 0.0994, -0.4229],\n",
            "        [ 0.5436, -0.5562],\n",
            "        [ 0.9691, -1.1457],\n",
            "        [ 0.7733, -0.8773],\n",
            "        [-0.9067,  0.8498],\n",
            "        [ 0.9250, -0.9930],\n",
            "        [-0.2222, -0.2296]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2192,  1.6432],\n",
            "        [ 0.5927, -0.9928],\n",
            "        [-0.5108,  0.3590],\n",
            "        [-1.5796,  1.4081],\n",
            "        [ 0.2983, -0.5883],\n",
            "        [ 1.0134, -1.2097],\n",
            "        [ 0.8406, -1.1188],\n",
            "        [-0.1847,  0.0368],\n",
            "        [ 0.2934, -0.2907],\n",
            "        [ 0.6457, -0.8912],\n",
            "        [-1.5013,  1.3756],\n",
            "        [-1.4265,  1.4540],\n",
            "        [ 0.7940, -1.3115],\n",
            "        [ 0.2813, -0.4297],\n",
            "        [ 0.7962, -1.1210],\n",
            "        [-1.1126,  1.0475],\n",
            "        [-1.2798,  1.4265],\n",
            "        [-1.4108,  1.3486],\n",
            "        [ 0.8987, -1.2224],\n",
            "        [-1.3586,  1.3429],\n",
            "        [ 0.9342, -1.1191],\n",
            "        [ 1.0748, -1.1458],\n",
            "        [ 0.9235, -1.0167],\n",
            "        [ 0.9183, -1.2064],\n",
            "        [ 0.0787, -0.2330],\n",
            "        [-1.3395,  1.5158],\n",
            "        [-1.5761,  1.3459],\n",
            "        [-1.5085,  1.5041],\n",
            "        [ 0.8801, -0.9081],\n",
            "        [ 0.9056, -1.0939],\n",
            "        [ 1.0266, -1.3500],\n",
            "        [ 1.1238, -0.8645]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8882, -1.2185],\n",
            "        [ 0.2653, -0.2956],\n",
            "        [ 0.8859, -0.8808],\n",
            "        [-1.0421,  1.2012],\n",
            "        [-1.5706,  1.4717],\n",
            "        [-1.2323,  1.1013],\n",
            "        [-1.1436,  1.3485],\n",
            "        [-0.4516,  0.4279],\n",
            "        [ 0.9017, -0.9426],\n",
            "        [ 1.2375, -1.1914],\n",
            "        [ 0.8635, -0.9178],\n",
            "        [ 0.7322, -0.7812],\n",
            "        [-1.3786,  1.4167],\n",
            "        [ 0.7766, -1.3875],\n",
            "        [ 0.6622, -0.9598],\n",
            "        [ 0.0471, -0.3215],\n",
            "        [-0.3673, -0.0069],\n",
            "        [-0.8784,  1.1649],\n",
            "        [ 0.6955, -0.6223],\n",
            "        [-1.3797,  1.5290],\n",
            "        [ 0.8497, -0.9882],\n",
            "        [ 0.8341, -1.0883],\n",
            "        [ 0.7544, -1.1627],\n",
            "        [ 0.4795, -0.6974],\n",
            "        [ 0.7631, -0.7929],\n",
            "        [ 0.8949, -0.8690],\n",
            "        [ 0.3294, -0.2782],\n",
            "        [-0.5807,  0.5314],\n",
            "        [-1.2070,  1.5857],\n",
            "        [-0.4307,  0.1141],\n",
            "        [ 0.0319,  0.1464],\n",
            "        [ 0.7752, -1.0497]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4147,  1.4695],\n",
            "        [-1.1544,  1.5339],\n",
            "        [ 1.0166, -1.1332],\n",
            "        [-1.6622,  1.5802],\n",
            "        [ 0.4438, -0.9549],\n",
            "        [ 0.8559, -1.2239],\n",
            "        [ 0.6087, -0.8010],\n",
            "        [ 0.7746, -1.2509],\n",
            "        [-1.2797,  1.2625],\n",
            "        [ 1.1663, -1.3246],\n",
            "        [ 0.5821, -1.1572],\n",
            "        [-1.4466,  1.6076],\n",
            "        [ 0.9357, -1.3129],\n",
            "        [ 0.8742, -0.9320],\n",
            "        [-1.5110,  1.2776],\n",
            "        [-1.3673,  1.6842],\n",
            "        [-0.1043, -0.3868],\n",
            "        [ 0.8929, -1.0797],\n",
            "        [-0.5198,  0.3566],\n",
            "        [ 0.5188, -0.7307],\n",
            "        [-0.9459,  1.2062],\n",
            "        [ 0.7826, -1.4302],\n",
            "        [ 0.8114, -1.2486],\n",
            "        [ 1.0699, -1.1114],\n",
            "        [ 0.9568, -1.3067],\n",
            "        [ 0.3476, -0.5777],\n",
            "        [ 0.5539, -0.9103],\n",
            "        [-0.0991,  0.4626],\n",
            "        [-1.2185,  1.2911],\n",
            "        [ 0.0939, -0.0554],\n",
            "        [-0.6803,  0.8545],\n",
            "        [-1.0466,  1.3208]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5768, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9437, -1.1009],\n",
            "        [-0.6408,  0.8850],\n",
            "        [ 0.9097, -1.0765],\n",
            "        [ 0.0142, -0.4501],\n",
            "        [ 1.1747, -1.0822],\n",
            "        [ 0.8017, -1.1334],\n",
            "        [ 0.9400, -1.0192],\n",
            "        [ 0.8267, -1.0419],\n",
            "        [-1.2563,  1.1643],\n",
            "        [ 0.7360, -0.9181],\n",
            "        [-0.9440,  1.0950],\n",
            "        [ 0.3280, -0.1868],\n",
            "        [ 0.2261, -0.7462],\n",
            "        [ 0.4156, -0.9706],\n",
            "        [ 0.9116, -1.0663],\n",
            "        [ 0.8273, -0.9944],\n",
            "        [ 0.8652, -0.9945],\n",
            "        [-0.9861,  0.9494],\n",
            "        [ 0.8820, -1.2835],\n",
            "        [ 0.5391, -0.8361],\n",
            "        [ 0.5446, -1.0691],\n",
            "        [ 1.0041, -1.1726],\n",
            "        [ 0.8816, -0.9647],\n",
            "        [-0.1899,  0.1161],\n",
            "        [-1.4522,  1.2456],\n",
            "        [ 0.8233, -0.8436],\n",
            "        [ 0.2229, -0.3245],\n",
            "        [ 0.8435, -0.9758],\n",
            "        [ 0.9869, -1.2147],\n",
            "        [ 0.6051, -0.6502],\n",
            "        [ 0.7652, -0.7010],\n",
            "        [ 1.2712, -1.1038]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9029, -0.9476],\n",
            "        [-1.1428,  0.6750],\n",
            "        [-0.0417,  0.1422],\n",
            "        [-1.0589,  1.1315],\n",
            "        [ 0.7582, -1.0491],\n",
            "        [-1.4551,  1.3674],\n",
            "        [ 0.5773, -1.0173],\n",
            "        [-0.4950,  0.6094],\n",
            "        [-1.1133,  1.2638],\n",
            "        [-0.9407,  0.5943],\n",
            "        [ 0.6250, -0.9332],\n",
            "        [ 1.0028, -1.2128],\n",
            "        [-1.2822,  1.4109],\n",
            "        [ 0.8408, -0.9607],\n",
            "        [-1.2726,  1.2438],\n",
            "        [-1.3936,  1.2524],\n",
            "        [-1.0398,  1.2434],\n",
            "        [ 0.3529, -0.7272],\n",
            "        [ 0.7431, -0.9559],\n",
            "        [ 0.8450, -1.2493],\n",
            "        [-1.2849,  1.5200],\n",
            "        [-0.4552,  0.3767],\n",
            "        [ 0.3509, -0.9867],\n",
            "        [ 1.1664, -0.9574],\n",
            "        [ 0.7895, -0.8916],\n",
            "        [ 0.8880, -1.3675],\n",
            "        [-0.9741,  1.0651],\n",
            "        [ 0.9872, -1.0022],\n",
            "        [ 1.0154, -1.1625],\n",
            "        [ 1.0119, -1.3819],\n",
            "        [-0.5420,  0.6813],\n",
            "        [ 0.8394, -1.1286]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5265, -1.0706],\n",
            "        [ 0.9455, -0.8833],\n",
            "        [ 0.8352, -1.1896],\n",
            "        [ 0.8464, -1.1886],\n",
            "        [-0.8210,  0.8003],\n",
            "        [ 1.0798, -1.0186],\n",
            "        [ 0.4777, -1.1423],\n",
            "        [-1.0922,  1.3215],\n",
            "        [ 1.0634, -1.2500],\n",
            "        [-1.1652,  1.2358],\n",
            "        [ 0.8092, -0.7559],\n",
            "        [-0.8738,  1.1581],\n",
            "        [ 0.8998, -1.2189],\n",
            "        [ 0.8009, -0.6187],\n",
            "        [ 0.7231, -0.9050],\n",
            "        [ 0.7327, -0.9010],\n",
            "        [ 0.3630, -0.6325],\n",
            "        [ 0.7902, -1.1692],\n",
            "        [-1.2675,  1.1184],\n",
            "        [ 0.9374, -1.0223],\n",
            "        [-0.4395,  0.3304],\n",
            "        [ 0.4386, -0.5799],\n",
            "        [-0.9865,  0.8051],\n",
            "        [-0.0625,  0.1511],\n",
            "        [-1.4963,  1.5053],\n",
            "        [ 0.3509, -0.6972],\n",
            "        [ 0.7020, -0.8997],\n",
            "        [-0.3824,  0.4659],\n",
            "        [-1.6141,  1.4699],\n",
            "        [ 0.8834, -1.2622],\n",
            "        [ 0.3788, -0.9712],\n",
            "        [ 0.6566, -1.1157]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4103, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9940,  1.2798],\n",
            "        [ 0.6535, -0.6355],\n",
            "        [-0.4744,  0.6986],\n",
            "        [-1.3859,  1.2197],\n",
            "        [ 0.8749, -1.2249],\n",
            "        [-0.3212,  0.1244],\n",
            "        [ 0.9190, -0.8458],\n",
            "        [ 0.1967, -0.0432],\n",
            "        [ 1.1102, -1.0473],\n",
            "        [ 0.7915, -0.9029],\n",
            "        [-0.0037,  0.0382],\n",
            "        [ 0.0783,  0.1194],\n",
            "        [ 0.9043, -1.1399],\n",
            "        [ 1.1477, -1.0873],\n",
            "        [ 1.0237, -1.2246],\n",
            "        [-1.4967,  1.6517],\n",
            "        [-1.1774,  1.1548],\n",
            "        [ 1.0291, -1.1285],\n",
            "        [ 0.8378, -1.0658],\n",
            "        [ 0.8737, -1.0123],\n",
            "        [-1.2118,  1.5938],\n",
            "        [-0.6906,  0.9502],\n",
            "        [ 0.6595, -0.6882],\n",
            "        [ 0.7305, -0.6769],\n",
            "        [ 0.8830, -0.9018],\n",
            "        [-1.0067,  1.2842],\n",
            "        [ 0.8100, -0.7906],\n",
            "        [ 0.6599, -1.0361],\n",
            "        [ 0.9623, -0.9251],\n",
            "        [-1.4564,  1.5635],\n",
            "        [-0.9986,  0.9479],\n",
            "        [-0.7623,  0.9696]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1185,  1.1801],\n",
            "        [ 0.3134, -0.5105],\n",
            "        [-0.8311,  1.0070],\n",
            "        [-0.2364,  0.6796],\n",
            "        [-0.7186,  0.9513],\n",
            "        [-0.7599,  0.9067],\n",
            "        [-1.2919,  1.4195],\n",
            "        [-0.3685,  0.3975],\n",
            "        [-1.2085,  1.4778],\n",
            "        [ 0.8768, -1.0542],\n",
            "        [-1.5393,  1.5233],\n",
            "        [ 0.6489, -0.9237],\n",
            "        [-1.5109,  1.4740],\n",
            "        [-1.0388,  0.9902],\n",
            "        [ 1.0584, -1.0092],\n",
            "        [-0.4291,  0.5657],\n",
            "        [ 0.9094, -0.8051],\n",
            "        [-1.4797,  1.5144],\n",
            "        [ 0.8312, -0.8866],\n",
            "        [ 1.0762, -0.9848],\n",
            "        [ 0.7893, -1.1267],\n",
            "        [ 0.5089, -0.7791],\n",
            "        [-1.1462,  1.0492],\n",
            "        [-1.5353,  1.4526],\n",
            "        [-1.0873,  1.3846],\n",
            "        [ 0.8815, -1.0120],\n",
            "        [-0.2305,  0.3841],\n",
            "        [ 1.0627, -1.2648],\n",
            "        [ 0.5353, -0.8522],\n",
            "        [ 0.9778, -1.0202],\n",
            "        [-1.2180,  1.3392],\n",
            "        [ 0.8942, -1.2860]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4242, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5124, -0.8351],\n",
            "        [ 0.9783, -1.1836],\n",
            "        [ 0.7131, -1.1023],\n",
            "        [ 0.4741, -0.3402],\n",
            "        [ 0.9025, -0.8920],\n",
            "        [ 0.8431, -0.9692],\n",
            "        [ 0.6457, -0.7663],\n",
            "        [-1.2684,  1.4620],\n",
            "        [ 0.3680, -0.6487],\n",
            "        [ 1.1421, -1.2443],\n",
            "        [ 0.0362, -0.4054],\n",
            "        [ 0.2454, -0.2749],\n",
            "        [-1.3414,  1.4805],\n",
            "        [ 0.8279, -0.7237],\n",
            "        [-0.0563, -0.1790],\n",
            "        [ 0.9458, -0.9627],\n",
            "        [ 0.6076, -0.6960],\n",
            "        [-1.2301,  1.3824],\n",
            "        [-1.0908,  1.4488],\n",
            "        [ 0.8882, -1.0607],\n",
            "        [ 0.3015, -0.5459],\n",
            "        [ 0.5641, -0.6945],\n",
            "        [ 0.2591, -0.4649],\n",
            "        [ 0.9355, -1.1446],\n",
            "        [ 0.9172, -1.0924],\n",
            "        [-0.7537,  0.6081],\n",
            "        [ 0.9132, -1.1911],\n",
            "        [-0.9327,  0.9987],\n",
            "        [ 0.2702, -0.5616],\n",
            "        [ 0.6822, -0.5029],\n",
            "        [-1.1461,  1.4536],\n",
            "        [ 0.6737, -0.9775]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8954, -1.0329],\n",
            "        [-0.8298,  0.7523],\n",
            "        [ 0.7593, -0.9152],\n",
            "        [ 0.9586, -1.2766],\n",
            "        [-0.8547,  0.7633],\n",
            "        [ 0.6127, -0.8373],\n",
            "        [ 0.1964, -0.2096],\n",
            "        [-1.1119,  1.1070],\n",
            "        [ 0.9776, -1.0516],\n",
            "        [ 0.8477, -1.0205],\n",
            "        [ 0.8954, -1.0829],\n",
            "        [ 0.7006, -1.0051],\n",
            "        [ 1.2536, -1.0672],\n",
            "        [ 0.8102, -0.7273],\n",
            "        [ 1.1599, -0.9415],\n",
            "        [ 0.7732, -0.9059],\n",
            "        [-1.0497,  1.1046],\n",
            "        [ 0.6333, -0.8406],\n",
            "        [ 1.0273, -1.0925],\n",
            "        [ 0.2033, -0.6093],\n",
            "        [ 0.7964, -0.9923],\n",
            "        [-1.4462,  1.5907],\n",
            "        [-1.4278,  1.4552],\n",
            "        [ 0.9762, -0.8423],\n",
            "        [ 0.8208, -1.0520],\n",
            "        [ 0.5567, -0.9706],\n",
            "        [-0.5238,  0.5338],\n",
            "        [ 0.5240, -0.9999],\n",
            "        [-1.3134,  1.5868],\n",
            "        [-1.5354,  1.5530],\n",
            "        [-1.4313,  1.6185],\n",
            "        [ 1.0602, -0.9611]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4393, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9662,  0.8147],\n",
            "        [ 0.8208, -1.4588],\n",
            "        [ 0.2972, -0.7162],\n",
            "        [ 1.0144, -1.2276],\n",
            "        [-0.4128,  0.2991],\n",
            "        [ 0.9889, -1.0705],\n",
            "        [-1.4146,  1.5411],\n",
            "        [ 0.6460, -0.5959],\n",
            "        [-1.5387,  1.5890],\n",
            "        [ 1.2780, -1.2220],\n",
            "        [-0.1656,  0.0682],\n",
            "        [ 0.7654, -1.0155],\n",
            "        [ 0.6084, -1.1862],\n",
            "        [ 0.8757, -0.9820],\n",
            "        [ 0.7917, -0.8671],\n",
            "        [ 0.8214, -1.0433],\n",
            "        [-0.8649,  0.9869],\n",
            "        [ 0.8526, -1.0305],\n",
            "        [ 0.7631, -0.8756],\n",
            "        [ 0.5904, -0.9004],\n",
            "        [ 1.1989, -0.9419],\n",
            "        [ 0.7888, -1.0680],\n",
            "        [ 0.9770, -1.3136],\n",
            "        [-0.1718, -0.0425],\n",
            "        [ 0.1070, -0.6248],\n",
            "        [ 0.9951, -1.3973],\n",
            "        [ 1.0492, -0.9014],\n",
            "        [ 0.9389, -1.1885],\n",
            "        [ 0.3655, -0.8055],\n",
            "        [ 0.2367, -0.3094],\n",
            "        [ 0.3912, -0.6987],\n",
            "        [-0.1573,  0.1452]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5980, -0.8355],\n",
            "        [-1.2451,  1.3008],\n",
            "        [ 0.5105, -0.8230],\n",
            "        [ 1.0204, -1.1157],\n",
            "        [ 1.0390, -1.2320],\n",
            "        [ 0.6519, -0.8879],\n",
            "        [ 0.9196, -1.3168],\n",
            "        [-1.4843,  1.6144],\n",
            "        [-1.4127,  1.3752],\n",
            "        [ 0.9828, -1.3071],\n",
            "        [ 0.8852, -1.0670],\n",
            "        [-1.4479,  1.6127],\n",
            "        [ 0.5488, -0.8735],\n",
            "        [ 0.8222, -0.9923],\n",
            "        [-0.2999,  0.6370],\n",
            "        [ 0.8367, -1.1249],\n",
            "        [ 0.8525, -1.1092],\n",
            "        [-0.5516,  0.6104],\n",
            "        [-0.9920,  0.8954],\n",
            "        [ 0.9437, -1.0196],\n",
            "        [ 0.8877, -1.1249],\n",
            "        [-1.4596,  1.4043],\n",
            "        [-1.2116,  1.4172],\n",
            "        [ 0.4924, -0.7844],\n",
            "        [ 0.8721, -1.0180],\n",
            "        [-1.3923,  1.4832],\n",
            "        [ 0.9093, -0.9544],\n",
            "        [ 0.6296, -1.2920],\n",
            "        [ 0.3206, -0.6788],\n",
            "        [ 0.9032, -1.1906],\n",
            "        [ 0.7843, -0.8316],\n",
            "        [-1.5599,  1.2837]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8699, -1.2202],\n",
            "        [ 0.7862, -0.8319],\n",
            "        [ 1.0752, -1.2276],\n",
            "        [-0.7203,  0.7312],\n",
            "        [-0.4148,  0.3249],\n",
            "        [-0.7145,  0.7369],\n",
            "        [ 1.0682, -1.1748],\n",
            "        [ 0.6945, -1.0937],\n",
            "        [ 0.8503, -1.1091],\n",
            "        [-1.1786,  1.2919],\n",
            "        [-1.4307,  1.3742],\n",
            "        [ 0.8434, -1.0227],\n",
            "        [ 0.8494, -1.3125],\n",
            "        [ 1.1944, -1.2697],\n",
            "        [-1.5071,  1.5901],\n",
            "        [-0.5435,  0.5966],\n",
            "        [ 0.8968, -1.2007],\n",
            "        [ 0.3304, -0.6251],\n",
            "        [ 1.0309, -1.1951],\n",
            "        [ 0.9079, -1.2039],\n",
            "        [-1.4337,  1.6088],\n",
            "        [-0.2099, -0.0946],\n",
            "        [ 0.9122, -0.8591],\n",
            "        [-1.4046,  1.1522],\n",
            "        [-1.4731,  1.4662],\n",
            "        [ 0.2410, -0.3412],\n",
            "        [ 0.6840, -1.2576],\n",
            "        [-1.0594,  1.0359],\n",
            "        [-1.1763,  1.4221],\n",
            "        [ 0.7283, -1.1470],\n",
            "        [-1.6066,  1.2562],\n",
            "        [ 0.6381, -1.0962]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9363, -0.7212],\n",
            "        [ 0.3615, -0.5106],\n",
            "        [ 0.0647, -0.3086],\n",
            "        [ 0.6874, -0.8528],\n",
            "        [-0.3878,  0.2720],\n",
            "        [ 0.8604, -0.9131],\n",
            "        [ 0.5149, -0.5868],\n",
            "        [-0.9637,  1.0904],\n",
            "        [ 0.4623, -0.9226],\n",
            "        [-1.3484,  1.3490],\n",
            "        [-1.1725,  1.0374],\n",
            "        [ 0.6823, -1.1781],\n",
            "        [-0.7950,  0.9241],\n",
            "        [-1.4615,  1.4411],\n",
            "        [-1.2079,  1.3375],\n",
            "        [ 1.1862, -1.2361],\n",
            "        [ 0.1870, -0.5212],\n",
            "        [-1.3761,  1.3235],\n",
            "        [-1.6165,  1.3667],\n",
            "        [ 0.8544, -1.0231],\n",
            "        [-1.0810,  1.3790],\n",
            "        [ 0.7705, -0.9345],\n",
            "        [ 0.1453, -0.2722],\n",
            "        [-1.1280,  1.6315],\n",
            "        [-1.5185,  1.3280],\n",
            "        [-0.1377,  0.3061],\n",
            "        [-1.4747,  1.5191],\n",
            "        [ 1.0076, -0.8952],\n",
            "        [ 0.6774, -1.2879],\n",
            "        [ 0.0093, -0.0647],\n",
            "        [-1.5001,  1.4983],\n",
            "        [ 0.4643, -0.5697]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3622,  1.3572],\n",
            "        [ 1.1167, -1.2055],\n",
            "        [ 0.8786, -1.3729],\n",
            "        [ 0.8767, -1.1990],\n",
            "        [-1.2251,  1.5455],\n",
            "        [ 0.7371, -1.0317],\n",
            "        [ 1.0514, -1.0587],\n",
            "        [-1.0403,  0.8159],\n",
            "        [ 0.0834, -0.3706],\n",
            "        [ 0.5806, -0.9922],\n",
            "        [ 1.0034, -1.0817],\n",
            "        [ 0.8798, -0.8087],\n",
            "        [-1.2205,  1.3639],\n",
            "        [-1.5896,  1.4749],\n",
            "        [ 1.0596, -1.1712],\n",
            "        [ 0.8141, -1.2790],\n",
            "        [ 1.0291, -1.2159],\n",
            "        [-0.0598,  0.2771],\n",
            "        [ 0.5407, -1.1048],\n",
            "        [-0.8732,  1.1928],\n",
            "        [ 0.6422, -0.6916],\n",
            "        [-0.3009,  0.3625],\n",
            "        [ 0.7450, -1.1488],\n",
            "        [ 0.4848, -0.8349],\n",
            "        [-1.4646,  1.5969],\n",
            "        [-0.1820, -0.1429],\n",
            "        [ 0.5409, -0.9740],\n",
            "        [ 0.6117, -0.5833],\n",
            "        [-1.2999,  1.2833],\n",
            "        [-1.3647,  1.3670],\n",
            "        [-1.1711,  1.2338],\n",
            "        [-1.0214,  1.3843]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8084, -0.9047],\n",
            "        [ 0.3343, -0.5564],\n",
            "        [-0.7124,  0.7535],\n",
            "        [-0.5069,  0.3990],\n",
            "        [-0.8233,  0.8011],\n",
            "        [-1.2346,  1.3096],\n",
            "        [-1.5608,  1.3852],\n",
            "        [ 0.7254, -0.9366],\n",
            "        [-0.0189,  0.0543],\n",
            "        [-1.6744,  1.5886],\n",
            "        [-0.5124,  0.3772],\n",
            "        [ 0.6466, -1.2213],\n",
            "        [-1.3366,  1.6367],\n",
            "        [ 0.6463, -1.2517],\n",
            "        [ 0.8822, -0.8834],\n",
            "        [ 0.9775, -1.0935],\n",
            "        [-0.5352,  0.5787],\n",
            "        [ 0.8928, -1.1597],\n",
            "        [ 0.6783, -0.8866],\n",
            "        [-1.3541,  1.6036],\n",
            "        [-1.2863,  1.2106],\n",
            "        [-1.1021,  1.2176],\n",
            "        [-0.0055,  0.3595],\n",
            "        [ 0.6713, -1.0405],\n",
            "        [-1.3572,  1.3636],\n",
            "        [-1.1804,  1.1836],\n",
            "        [ 1.0263, -1.0232],\n",
            "        [ 0.7015, -1.0737],\n",
            "        [ 0.7808, -0.8987],\n",
            "        [-0.8001,  1.1067],\n",
            "        [ 0.8146, -1.0671],\n",
            "        [-0.7756,  0.4905]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5492,  0.7583],\n",
            "        [ 0.9029, -1.1641],\n",
            "        [ 0.8920, -1.0256],\n",
            "        [ 0.6169, -0.4135],\n",
            "        [ 0.9657, -1.1129],\n",
            "        [ 0.9156, -1.1058],\n",
            "        [ 1.0182, -0.9542],\n",
            "        [-1.2139,  1.5317],\n",
            "        [-1.4593,  1.6086],\n",
            "        [ 1.0090, -1.0991],\n",
            "        [ 0.3192, -0.5458],\n",
            "        [-0.2854, -0.0125],\n",
            "        [ 0.6829, -0.9950],\n",
            "        [-1.3227,  1.3652],\n",
            "        [-1.7863,  1.5389],\n",
            "        [ 0.0339,  0.1066],\n",
            "        [ 0.7185, -1.0058],\n",
            "        [ 0.4597, -0.8630],\n",
            "        [-0.0372, -0.1508],\n",
            "        [ 1.0449, -1.0159],\n",
            "        [-1.1525,  1.6715],\n",
            "        [ 0.0633, -0.2274],\n",
            "        [ 1.0167, -1.2837],\n",
            "        [-0.5820,  0.5242],\n",
            "        [ 0.9257, -1.2027],\n",
            "        [ 0.8082, -1.0254],\n",
            "        [-0.0766, -0.2922],\n",
            "        [-0.4958,  0.3446],\n",
            "        [ 0.9613, -1.1607],\n",
            "        [-1.2832,  1.5366],\n",
            "        [-1.5853,  1.4976],\n",
            "        [ 0.9428, -1.3437]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5206, -0.9771],\n",
            "        [ 1.0202, -1.2645],\n",
            "        [-1.4176,  1.4631],\n",
            "        [-0.1399,  0.4582],\n",
            "        [ 1.0842, -0.9266],\n",
            "        [ 0.7307, -1.1566],\n",
            "        [ 0.6876, -1.0164],\n",
            "        [ 1.0027, -1.0941],\n",
            "        [ 0.5885, -0.8778],\n",
            "        [ 0.7422, -1.0841],\n",
            "        [ 0.7469, -0.9242],\n",
            "        [ 0.8778, -1.2578],\n",
            "        [-1.4937,  1.4989],\n",
            "        [-0.0019, -0.1340],\n",
            "        [ 1.1179, -1.2131],\n",
            "        [ 0.9441, -0.7975],\n",
            "        [ 0.9154, -0.9767],\n",
            "        [-1.3616,  1.3927],\n",
            "        [ 0.5949, -0.9903],\n",
            "        [ 0.3715, -0.6852],\n",
            "        [-0.8246,  1.1131],\n",
            "        [ 0.9721, -1.1338],\n",
            "        [-1.2071,  1.3756],\n",
            "        [ 0.2598, -0.7158],\n",
            "        [-0.9778,  0.9061],\n",
            "        [-0.9549,  1.3395],\n",
            "        [ 1.1337, -1.1922],\n",
            "        [ 0.7784, -1.2445],\n",
            "        [-1.2391,  1.2576],\n",
            "        [ 0.8897, -1.1859],\n",
            "        [ 0.2840, -0.5878],\n",
            "        [-0.3711,  0.2566]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9585, -0.8645],\n",
            "        [ 0.9158, -1.1488],\n",
            "        [ 0.8697, -1.0837],\n",
            "        [ 0.8819, -1.1848],\n",
            "        [ 0.2794, -0.7271],\n",
            "        [-0.4283,  0.4882],\n",
            "        [-0.5489,  0.5639],\n",
            "        [ 0.9162, -1.2194],\n",
            "        [ 0.2585, -0.7642],\n",
            "        [ 0.5209, -0.3873],\n",
            "        [ 0.8279, -0.9199],\n",
            "        [ 0.8052, -0.9612],\n",
            "        [ 0.9718, -0.7970],\n",
            "        [-0.9082,  1.1387],\n",
            "        [-0.8990,  0.8702],\n",
            "        [ 0.8411, -0.9211],\n",
            "        [-1.1447,  1.6142],\n",
            "        [-1.2536,  1.6693],\n",
            "        [ 0.8669, -0.9545],\n",
            "        [ 0.9675, -1.1796],\n",
            "        [ 0.2988, -0.5349],\n",
            "        [ 1.2259, -1.2132],\n",
            "        [ 0.7639, -1.2043],\n",
            "        [-1.4341,  1.5742],\n",
            "        [ 1.1116, -1.2037],\n",
            "        [-0.9601,  1.2316],\n",
            "        [ 0.9185, -0.9865],\n",
            "        [ 0.6787, -1.1205],\n",
            "        [ 0.0899, -0.4638],\n",
            "        [-1.3668,  1.1685],\n",
            "        [-1.0491,  1.3350],\n",
            "        [-1.4264,  1.3613]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5950, -1.0682],\n",
            "        [-0.9108,  1.2688],\n",
            "        [ 0.7198, -1.2036],\n",
            "        [ 0.7867, -0.9228],\n",
            "        [ 0.8699, -0.9487],\n",
            "        [ 0.3806, -0.3466],\n",
            "        [ 0.0689,  0.1562],\n",
            "        [-1.3835,  1.4395],\n",
            "        [ 0.7051, -0.9700],\n",
            "        [-1.4589,  1.5256],\n",
            "        [-0.6362,  0.5218],\n",
            "        [ 0.9251, -1.1308],\n",
            "        [ 0.9647, -1.0468],\n",
            "        [ 1.0128, -1.1831],\n",
            "        [ 0.6010, -0.9727],\n",
            "        [ 0.9737, -1.1667],\n",
            "        [-1.3045,  1.5576],\n",
            "        [ 0.7840, -0.8375],\n",
            "        [ 0.5128, -0.5821],\n",
            "        [ 0.7980, -0.7628],\n",
            "        [-1.5053,  1.4495],\n",
            "        [ 0.8253, -0.9791],\n",
            "        [-0.4544,  0.6020],\n",
            "        [-0.5052,  0.6396],\n",
            "        [ 1.0034, -1.2455],\n",
            "        [ 0.4201, -0.6046],\n",
            "        [-0.9853,  1.1551],\n",
            "        [ 0.9093, -1.2733],\n",
            "        [ 0.3762, -0.5015],\n",
            "        [-0.7232,  0.4530],\n",
            "        [-1.2434,  1.7145],\n",
            "        [ 0.9935, -1.0728]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8282, -0.9912],\n",
            "        [-1.2709,  1.5047],\n",
            "        [-1.1990,  1.5391],\n",
            "        [ 0.9565, -0.8909],\n",
            "        [-0.9196,  1.2316],\n",
            "        [ 0.8317, -1.0096],\n",
            "        [ 0.7173, -0.7489],\n",
            "        [-1.4700,  1.5850],\n",
            "        [-0.1606,  0.5537],\n",
            "        [ 0.6741, -0.7210],\n",
            "        [ 0.9143, -1.1556],\n",
            "        [ 0.7676, -0.8557],\n",
            "        [-1.4876,  1.6034],\n",
            "        [-0.2185,  0.2958],\n",
            "        [-1.5156,  1.6161],\n",
            "        [ 0.6321, -0.5564],\n",
            "        [-0.9489,  1.0190],\n",
            "        [ 0.3810, -0.1631],\n",
            "        [-1.5271,  1.6429],\n",
            "        [-1.2552,  1.5183],\n",
            "        [ 1.1649, -1.1414],\n",
            "        [ 0.8575, -1.1148],\n",
            "        [ 0.9603, -1.1547],\n",
            "        [ 0.5581, -0.8436],\n",
            "        [-0.9487,  1.0356],\n",
            "        [-0.9837,  1.1626],\n",
            "        [-1.5028,  1.5689],\n",
            "        [ 0.9955, -0.9848],\n",
            "        [-1.5011,  1.5905],\n",
            "        [ 0.7328, -0.9870],\n",
            "        [ 0.4652, -0.6114],\n",
            "        [-0.7812,  0.7405]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4336, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8073, -1.1939],\n",
            "        [ 0.8521, -0.8091],\n",
            "        [ 0.5239, -0.5823],\n",
            "        [ 0.4096, -0.6927],\n",
            "        [ 0.2883, -0.7001],\n",
            "        [-0.7812,  0.8854],\n",
            "        [-1.5167,  1.3443],\n",
            "        [-1.4157,  1.2062],\n",
            "        [-1.4889,  1.3696],\n",
            "        [ 0.2517, -0.6481],\n",
            "        [ 0.9221, -1.0389],\n",
            "        [-0.0701,  0.0148],\n",
            "        [ 0.8783, -0.6083],\n",
            "        [ 0.9229, -0.8179],\n",
            "        [ 0.5873, -0.7024],\n",
            "        [-1.0980,  1.1063],\n",
            "        [ 1.1920, -0.9442],\n",
            "        [ 0.9905, -1.0740],\n",
            "        [-0.9535,  1.0064],\n",
            "        [-1.3661,  1.3135],\n",
            "        [-1.3426,  1.3553],\n",
            "        [ 0.6975, -1.0489],\n",
            "        [-0.3984,  0.4444],\n",
            "        [-1.5836,  1.5694],\n",
            "        [-1.3156,  1.5278],\n",
            "        [ 0.4368, -0.7401],\n",
            "        [ 0.6874, -1.1820],\n",
            "        [ 0.5749, -0.8522],\n",
            "        [-0.9219,  1.0046],\n",
            "        [ 0.3976, -0.8649],\n",
            "        [-1.6038,  1.5515],\n",
            "        [ 0.9611, -0.9563]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4130,  0.5492],\n",
            "        [-0.7708,  0.8352],\n",
            "        [ 0.9912, -1.0865],\n",
            "        [-1.5104,  1.4181],\n",
            "        [-0.2025,  0.3145],\n",
            "        [ 0.3080, -0.3297],\n",
            "        [-1.2773,  1.4517],\n",
            "        [-1.2219,  1.4280],\n",
            "        [ 1.0755, -0.9131],\n",
            "        [-1.5866,  1.3363],\n",
            "        [ 0.6915, -1.0108],\n",
            "        [ 0.6629, -0.7017],\n",
            "        [-1.0742,  1.0954],\n",
            "        [-1.1810,  1.2896],\n",
            "        [ 0.9532, -1.1776],\n",
            "        [ 1.0553, -1.1751],\n",
            "        [ 0.6567, -1.4860],\n",
            "        [ 0.7331, -0.9889],\n",
            "        [-0.7368,  0.9346],\n",
            "        [-1.2466,  1.3099],\n",
            "        [ 0.3941, -0.4053],\n",
            "        [ 0.3846, -0.7762],\n",
            "        [-1.3525,  1.6278],\n",
            "        [ 0.6510, -0.8253],\n",
            "        [ 0.8872, -1.0305],\n",
            "        [ 0.2731, -0.5146],\n",
            "        [ 0.7207, -0.6859],\n",
            "        [-1.2814,  1.4079],\n",
            "        [ 0.6907, -0.9424],\n",
            "        [-1.6127,  1.5347],\n",
            "        [-0.9971,  1.3030],\n",
            "        [ 1.0240, -1.0681]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2238,  1.2703],\n",
            "        [ 0.9163, -1.0220],\n",
            "        [ 0.6868, -1.0495],\n",
            "        [ 0.3381, -1.0826],\n",
            "        [ 0.7964, -0.9264],\n",
            "        [ 0.5981, -1.0616],\n",
            "        [ 0.6408, -0.7864],\n",
            "        [-1.5945,  1.4741],\n",
            "        [-1.4917,  1.2783],\n",
            "        [-0.0264,  0.0396],\n",
            "        [-0.5534,  0.4797],\n",
            "        [-1.3938,  1.4817],\n",
            "        [ 0.6091, -0.3796],\n",
            "        [ 0.1763, -0.6120],\n",
            "        [ 0.7886, -0.8635],\n",
            "        [-1.0593,  1.2028],\n",
            "        [-1.2504,  1.3695],\n",
            "        [ 0.6107, -0.9656],\n",
            "        [-0.0585,  0.0745],\n",
            "        [ 0.9295, -1.3436],\n",
            "        [-0.8260,  1.3489],\n",
            "        [ 0.8085, -1.2010],\n",
            "        [-1.3256,  1.5553],\n",
            "        [-1.5831,  1.5280],\n",
            "        [ 0.4615, -1.1208],\n",
            "        [-1.2731,  1.5766],\n",
            "        [ 0.9562, -1.0873],\n",
            "        [ 0.8085, -0.9238],\n",
            "        [ 0.8314, -1.2338],\n",
            "        [ 0.7685, -0.9952],\n",
            "        [-1.1811,  1.4515],\n",
            "        [-1.2176,  1.4146]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9207, -1.1653],\n",
            "        [-0.7917,  1.1799],\n",
            "        [-1.2963,  1.6926],\n",
            "        [-1.4094,  1.5784],\n",
            "        [-1.3583,  1.5284],\n",
            "        [-0.7006,  0.5694],\n",
            "        [-0.4291,  0.4828],\n",
            "        [ 0.7031, -0.9891],\n",
            "        [-0.7672,  0.9569],\n",
            "        [ 0.9642, -1.2767],\n",
            "        [-0.4272,  0.1845],\n",
            "        [-0.8160,  0.8530],\n",
            "        [ 1.2213, -1.1990],\n",
            "        [ 0.2572, -0.4701],\n",
            "        [ 0.9991, -1.0996],\n",
            "        [ 0.9004, -1.0078],\n",
            "        [-0.9792,  1.2579],\n",
            "        [ 0.7420, -1.1770],\n",
            "        [ 0.4113, -0.4218],\n",
            "        [-1.3305,  1.4078],\n",
            "        [ 0.9223, -0.9849],\n",
            "        [ 0.8432, -1.2837],\n",
            "        [ 0.8278, -1.0749],\n",
            "        [ 0.7618, -0.8628],\n",
            "        [ 1.0195, -1.1823],\n",
            "        [ 0.6675, -0.9548],\n",
            "        [-1.5347,  1.5801],\n",
            "        [ 0.3096, -0.1054],\n",
            "        [-1.0492,  1.4571],\n",
            "        [ 0.7515, -1.0432],\n",
            "        [ 0.8714, -0.7444],\n",
            "        [-1.4564,  1.3532]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4491, -0.6237],\n",
            "        [ 1.1957, -1.1993],\n",
            "        [ 0.8134, -1.1933],\n",
            "        [-1.3859,  1.6570],\n",
            "        [-0.7941,  0.7294],\n",
            "        [ 0.8910, -1.1136],\n",
            "        [ 0.7595, -0.8074],\n",
            "        [-1.3570,  1.3271],\n",
            "        [ 0.8319, -1.2222],\n",
            "        [ 0.7698, -0.9618],\n",
            "        [ 0.9743, -1.0117],\n",
            "        [-0.0305,  0.1472],\n",
            "        [-0.9562,  1.0036],\n",
            "        [ 0.5921, -0.8062],\n",
            "        [-1.2741,  1.5248],\n",
            "        [ 1.0077, -1.1587],\n",
            "        [-0.6025,  0.7771],\n",
            "        [-0.9797,  1.1648],\n",
            "        [-1.4995,  1.4946],\n",
            "        [-0.8189,  0.8519],\n",
            "        [ 1.0838, -0.9478],\n",
            "        [ 0.7587, -1.0873],\n",
            "        [ 1.0751, -1.2967],\n",
            "        [ 0.8646, -0.6262],\n",
            "        [-1.2733,  1.3438],\n",
            "        [-1.4959,  1.2919],\n",
            "        [-1.6491,  1.4305],\n",
            "        [ 1.0294, -0.8910],\n",
            "        [ 0.6312, -0.6056],\n",
            "        [ 1.2175, -1.0714],\n",
            "        [-0.8356,  1.1320],\n",
            "        [ 0.7288, -0.9893]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3596,  1.3628],\n",
            "        [-0.4587,  0.5510],\n",
            "        [-1.5221,  1.4307],\n",
            "        [ 0.5211, -0.7995],\n",
            "        [ 0.7916, -0.5095],\n",
            "        [ 1.0786, -1.0413],\n",
            "        [ 0.4943, -1.0058],\n",
            "        [ 0.9727, -1.1345],\n",
            "        [ 0.4907, -0.0208],\n",
            "        [-0.9819,  0.8812],\n",
            "        [-1.4696,  1.3606],\n",
            "        [ 0.9088, -1.1912],\n",
            "        [-0.0415,  0.1568],\n",
            "        [ 0.2195, -0.0350],\n",
            "        [ 0.8769, -1.1585],\n",
            "        [ 0.7179, -0.6961],\n",
            "        [-0.1497, -0.0174],\n",
            "        [-0.7740,  0.8845],\n",
            "        [ 0.4374, -0.2203],\n",
            "        [ 0.8625, -1.2636],\n",
            "        [ 0.9009, -1.0671],\n",
            "        [-1.2634,  1.4622],\n",
            "        [-0.4137,  0.3600],\n",
            "        [ 0.0483, -0.4216],\n",
            "        [ 0.1391, -0.5931],\n",
            "        [-1.1881,  1.4153],\n",
            "        [-1.1305,  1.4068],\n",
            "        [ 0.9357, -1.1941],\n",
            "        [-0.8521,  1.0935],\n",
            "        [ 1.0767, -0.7953],\n",
            "        [ 0.8082, -1.1167],\n",
            "        [ 0.0472, -0.0612]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6560, -1.0095],\n",
            "        [-1.2072,  1.0964],\n",
            "        [ 0.3483, -0.6086],\n",
            "        [ 0.8067, -0.9501],\n",
            "        [-0.1277,  0.3044],\n",
            "        [-1.3557,  1.6351],\n",
            "        [ 0.5813, -0.9142],\n",
            "        [-1.2802,  1.6699],\n",
            "        [-1.3735,  1.5339],\n",
            "        [ 0.1683,  0.0055],\n",
            "        [ 0.8473, -1.4049],\n",
            "        [ 1.0776, -1.3425],\n",
            "        [ 0.8873, -0.9728],\n",
            "        [ 0.7468, -1.1243],\n",
            "        [-1.3166,  1.2074],\n",
            "        [-0.3065, -0.0473],\n",
            "        [ 0.8660, -0.9557],\n",
            "        [ 0.7473, -1.2171],\n",
            "        [-1.5311,  1.5301],\n",
            "        [-0.0899,  0.2933],\n",
            "        [ 0.8490, -1.2246],\n",
            "        [ 1.1062, -1.1460],\n",
            "        [-1.3310,  1.3530],\n",
            "        [ 0.8986, -1.0491],\n",
            "        [ 0.5009, -0.8798],\n",
            "        [-1.6621,  1.5890],\n",
            "        [ 0.9168, -1.1511],\n",
            "        [ 0.6177, -0.9775],\n",
            "        [-0.9170,  0.9943],\n",
            "        [ 0.8534, -0.8244],\n",
            "        [ 0.8216, -1.0100],\n",
            "        [-1.3427,  1.3623]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch   150  of    191.    Elapsed: 0:03:26.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5371,  1.5357],\n",
            "        [-1.4006,  1.3754],\n",
            "        [-1.1615,  1.4648],\n",
            "        [-0.1093, -0.1365],\n",
            "        [ 1.1853, -1.1430],\n",
            "        [-1.3226,  1.3284],\n",
            "        [ 0.7417, -0.9241],\n",
            "        [-1.3923,  1.1473],\n",
            "        [ 0.8985, -1.0747],\n",
            "        [ 0.4025, -0.8845],\n",
            "        [-1.3029,  1.4155],\n",
            "        [ 0.8508, -0.9894],\n",
            "        [ 0.6597, -1.2368],\n",
            "        [ 0.9326, -1.0530],\n",
            "        [-0.9290,  1.0516],\n",
            "        [ 1.0354, -1.2097],\n",
            "        [-0.3950,  0.5950],\n",
            "        [-0.8247,  0.8386],\n",
            "        [-0.1902,  0.7617],\n",
            "        [ 0.0321, -0.2784],\n",
            "        [-0.4852,  0.4715],\n",
            "        [ 0.1038,  0.0396],\n",
            "        [-0.1490, -0.0396],\n",
            "        [ 0.9732, -1.2665],\n",
            "        [ 0.9460, -0.9656],\n",
            "        [-1.4132,  1.4923],\n",
            "        [-1.3055,  1.2772],\n",
            "        [ 0.9146, -1.2210],\n",
            "        [ 0.8185, -0.8853],\n",
            "        [ 1.2704, -0.9611],\n",
            "        [-1.4215,  1.5143],\n",
            "        [ 0.7867, -1.0081]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3224, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9907, -0.9708],\n",
            "        [-1.0629,  1.2112],\n",
            "        [ 0.1972, -0.1392],\n",
            "        [-0.9118,  0.8830],\n",
            "        [ 0.8670, -0.9835],\n",
            "        [ 0.5976, -1.0278],\n",
            "        [ 0.7302, -1.0384],\n",
            "        [ 1.0018, -1.2950],\n",
            "        [ 1.0274, -1.0863],\n",
            "        [-1.4062,  1.3075],\n",
            "        [-1.3810,  1.4417],\n",
            "        [ 0.4911, -0.6589],\n",
            "        [-1.4372,  1.6140],\n",
            "        [ 0.9351, -0.9324],\n",
            "        [ 0.6382, -0.7900],\n",
            "        [-1.2787,  1.4037],\n",
            "        [ 1.0730, -0.9686],\n",
            "        [ 0.8134, -1.1694],\n",
            "        [ 0.4798, -0.9270],\n",
            "        [ 0.4663, -0.3463],\n",
            "        [ 0.2190, -0.1762],\n",
            "        [ 0.9411, -1.0472],\n",
            "        [ 0.2450, -0.5277],\n",
            "        [-1.5324,  1.5379],\n",
            "        [ 0.0814,  0.1979],\n",
            "        [-1.3782,  1.5595],\n",
            "        [ 1.0805, -1.1332],\n",
            "        [-1.3769,  1.1789],\n",
            "        [-1.3976,  1.6085],\n",
            "        [-1.2774,  1.0336],\n",
            "        [ 0.7128, -0.7635],\n",
            "        [-1.1523,  1.3371]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9740, -1.5283],\n",
            "        [ 0.6988, -0.8396],\n",
            "        [ 1.1063, -1.4290],\n",
            "        [ 0.8045, -1.1303],\n",
            "        [ 1.0008, -1.1862],\n",
            "        [-0.2102,  0.1571],\n",
            "        [-1.2103,  1.1626],\n",
            "        [ 0.9346, -1.0671],\n",
            "        [-0.7431,  0.8093],\n",
            "        [-1.3171,  1.4843],\n",
            "        [ 0.8171, -1.1267],\n",
            "        [ 0.6426, -0.9284],\n",
            "        [ 0.3573, -0.7619],\n",
            "        [ 0.7610, -1.1096],\n",
            "        [ 0.4835, -0.7034],\n",
            "        [ 0.6589, -0.6483],\n",
            "        [ 0.8651, -1.1085],\n",
            "        [ 0.2774, -0.2900],\n",
            "        [ 0.0393, -0.3365],\n",
            "        [-1.2138,  1.2399],\n",
            "        [ 0.0422, -0.0427],\n",
            "        [ 0.9019, -0.8433],\n",
            "        [-1.2039,  1.3605],\n",
            "        [-0.4555,  0.5114],\n",
            "        [ 1.0521, -1.1717],\n",
            "        [ 0.8872, -1.3859],\n",
            "        [-0.7329,  0.9236],\n",
            "        [ 0.5171, -0.9280],\n",
            "        [-1.6113,  1.4362],\n",
            "        [ 0.8979, -1.0535],\n",
            "        [ 0.8773, -1.3463],\n",
            "        [ 0.9810, -1.0095]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9321, -1.0010],\n",
            "        [ 1.2306, -1.2073],\n",
            "        [ 0.3812, -0.9310],\n",
            "        [ 1.0366, -1.1238],\n",
            "        [ 0.3607, -0.7126],\n",
            "        [-1.0398,  1.3123],\n",
            "        [-1.5434,  1.6642],\n",
            "        [ 0.7308, -1.2519],\n",
            "        [ 0.7432, -0.7612],\n",
            "        [ 1.0041, -1.0682],\n",
            "        [ 1.1288, -1.1495],\n",
            "        [ 0.4082, -0.2882],\n",
            "        [ 0.9985, -1.1776],\n",
            "        [ 0.7303, -1.1459],\n",
            "        [ 0.6346, -1.1398],\n",
            "        [-1.0129,  1.1066],\n",
            "        [ 0.7062, -1.1193],\n",
            "        [ 0.8764, -1.0152],\n",
            "        [ 0.8200, -0.7702],\n",
            "        [-0.4991,  0.4408],\n",
            "        [-1.0672,  1.1408],\n",
            "        [-0.0462,  0.0736],\n",
            "        [ 0.7282, -0.8201],\n",
            "        [-1.5535,  1.5418],\n",
            "        [ 0.9966, -0.9245],\n",
            "        [-1.5171,  1.6256],\n",
            "        [ 0.8914, -1.1932],\n",
            "        [-1.7125,  1.6509],\n",
            "        [ 0.7898, -1.0155],\n",
            "        [ 0.7923, -1.0245],\n",
            "        [ 0.8157, -1.2799],\n",
            "        [ 0.8040, -1.0503]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9518, -1.1052],\n",
            "        [ 0.4402, -0.6545],\n",
            "        [ 0.6101, -0.9422],\n",
            "        [ 0.8753, -1.1637],\n",
            "        [-0.9808,  0.9934],\n",
            "        [ 0.6527, -0.8214],\n",
            "        [-0.1253, -0.2681],\n",
            "        [ 0.7983, -0.8123],\n",
            "        [-1.3343,  1.6745],\n",
            "        [-1.6007,  1.3793],\n",
            "        [-1.1619,  1.6051],\n",
            "        [ 1.0385, -1.2057],\n",
            "        [ 0.5988, -0.9120],\n",
            "        [-1.0776,  1.4545],\n",
            "        [ 1.2213, -1.0630],\n",
            "        [-0.1592, -0.0632],\n",
            "        [ 0.7721, -1.1843],\n",
            "        [ 0.9513, -1.0427],\n",
            "        [-1.4177,  1.3870],\n",
            "        [ 0.9967, -1.1471],\n",
            "        [-0.9843,  1.1352],\n",
            "        [ 0.9954, -0.8627],\n",
            "        [-1.3655,  1.3727],\n",
            "        [ 0.7801, -1.2499],\n",
            "        [ 0.9875, -0.8735],\n",
            "        [ 0.0778, -0.3848],\n",
            "        [ 0.6937, -0.8164],\n",
            "        [ 0.0307,  0.0413],\n",
            "        [ 0.7368, -0.8527],\n",
            "        [ 0.6011, -0.9400],\n",
            "        [ 0.7412, -0.7059],\n",
            "        [ 1.1501, -1.2827]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8356, -1.3832],\n",
            "        [ 1.0000, -1.1916],\n",
            "        [-1.6378,  1.2914],\n",
            "        [-1.3067,  1.4373],\n",
            "        [-1.3668,  1.3276],\n",
            "        [ 0.9348, -1.1297],\n",
            "        [ 0.2227, -0.4645],\n",
            "        [-0.9348,  1.1232],\n",
            "        [ 1.0640, -1.2977],\n",
            "        [ 0.9371, -1.1698],\n",
            "        [ 0.1455, -0.6020],\n",
            "        [-1.4739,  1.5259],\n",
            "        [-1.0999,  1.3257],\n",
            "        [ 1.2033, -1.1025],\n",
            "        [-0.1704,  0.3080],\n",
            "        [ 0.8824, -1.2503],\n",
            "        [-0.3785,  0.7395],\n",
            "        [ 0.9835, -0.9652],\n",
            "        [-1.1927,  1.2198],\n",
            "        [ 0.2380, -0.2388],\n",
            "        [ 0.9254, -0.8089],\n",
            "        [ 0.4098, -0.5282],\n",
            "        [ 0.9030, -1.0145],\n",
            "        [-0.8781,  0.7275],\n",
            "        [ 0.7720, -0.9419],\n",
            "        [ 0.4383, -0.5834],\n",
            "        [ 0.9610, -1.3104],\n",
            "        [ 0.8318, -1.1634],\n",
            "        [ 0.8533, -0.9858],\n",
            "        [ 0.4873, -0.7734],\n",
            "        [-0.2114,  0.2292],\n",
            "        [ 0.9695, -1.0764]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9920, -1.1770],\n",
            "        [ 0.7153, -0.9332],\n",
            "        [ 0.8404, -1.0902],\n",
            "        [-0.2308,  0.1268],\n",
            "        [ 0.6473, -1.0621],\n",
            "        [ 0.9273, -1.1547],\n",
            "        [ 0.9446, -1.2223],\n",
            "        [ 0.1237, -0.2540],\n",
            "        [ 0.6917, -0.7992],\n",
            "        [ 0.7206, -0.9718],\n",
            "        [ 1.0063, -1.0754],\n",
            "        [-1.2863,  1.4715],\n",
            "        [ 0.9921, -1.2755],\n",
            "        [-0.0482, -0.2756],\n",
            "        [ 0.8997, -0.9924],\n",
            "        [-0.4093,  0.1833],\n",
            "        [ 1.1782, -0.9813],\n",
            "        [-0.7268,  1.1683],\n",
            "        [ 0.9682, -1.1465],\n",
            "        [-1.1518,  1.5218],\n",
            "        [ 0.6868, -1.0722],\n",
            "        [-1.6360,  1.4167],\n",
            "        [-0.3964,  0.5362],\n",
            "        [ 0.7403, -1.0415],\n",
            "        [ 0.9606, -1.0871],\n",
            "        [ 0.6063, -1.0433],\n",
            "        [ 0.4764, -0.7437],\n",
            "        [ 0.7900, -0.8442],\n",
            "        [ 0.7938, -1.1408],\n",
            "        [ 0.5971, -0.9023],\n",
            "        [ 0.2746,  0.0884],\n",
            "        [ 0.9205, -1.0724]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9359,  0.9631],\n",
            "        [ 0.8271, -0.9687],\n",
            "        [-0.8491,  1.0012],\n",
            "        [-1.5108,  1.5302],\n",
            "        [ 0.3247, -0.4050],\n",
            "        [-1.3925,  1.3401],\n",
            "        [-1.4528,  1.5719],\n",
            "        [-0.1262,  0.1916],\n",
            "        [ 0.6733, -1.0357],\n",
            "        [ 0.8563, -0.9048],\n",
            "        [ 0.5458, -1.2359],\n",
            "        [ 0.9260, -1.0123],\n",
            "        [-1.0778,  1.2699],\n",
            "        [ 0.1152, -0.3057],\n",
            "        [ 0.2792, -0.5078],\n",
            "        [-1.2068,  1.4337],\n",
            "        [ 0.5491, -0.5848],\n",
            "        [ 0.9821, -1.1925],\n",
            "        [-1.1091,  1.3560],\n",
            "        [ 1.1696, -1.2535],\n",
            "        [ 1.0818, -1.4146],\n",
            "        [ 0.4945, -0.7054],\n",
            "        [ 0.1366, -0.0835],\n",
            "        [-0.0665, -0.1154],\n",
            "        [ 0.7932, -1.2433],\n",
            "        [ 0.7025, -0.8012],\n",
            "        [ 0.7437, -1.1375],\n",
            "        [ 0.5830, -0.7235],\n",
            "        [-0.4513,  0.3297],\n",
            "        [ 0.4193, -0.7885],\n",
            "        [ 0.2697, -0.2204],\n",
            "        [ 0.6123, -1.0218]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1156,  1.3444],\n",
            "        [-0.3563,  0.3474],\n",
            "        [ 0.8639, -1.0943],\n",
            "        [ 1.0694, -1.2973],\n",
            "        [ 1.1795, -1.3864],\n",
            "        [ 0.7490, -1.2193],\n",
            "        [ 0.0231,  0.2301],\n",
            "        [ 0.9703, -1.1668],\n",
            "        [ 0.1708, -0.6384],\n",
            "        [ 0.4505, -0.6321],\n",
            "        [ 0.9221, -0.8248],\n",
            "        [ 0.2334, -0.8268],\n",
            "        [-0.8286,  0.5135],\n",
            "        [ 0.4925, -0.9928],\n",
            "        [-0.1622, -0.2425],\n",
            "        [-0.1878,  0.2122],\n",
            "        [-1.0310,  1.1837],\n",
            "        [ 0.7527, -1.1375],\n",
            "        [-0.0547,  0.4044],\n",
            "        [ 0.2497, -0.4820],\n",
            "        [ 0.9939, -1.0998],\n",
            "        [ 0.9099, -1.1430],\n",
            "        [ 0.8871, -0.8570],\n",
            "        [ 1.0035, -1.3872],\n",
            "        [ 1.0659, -1.0491],\n",
            "        [-1.6086,  1.5309],\n",
            "        [ 0.8119, -1.0545],\n",
            "        [ 1.2934, -1.2959],\n",
            "        [ 0.9323, -1.1896],\n",
            "        [-1.3325,  1.5012],\n",
            "        [-0.1469,  0.0574],\n",
            "        [-0.4135,  0.6706]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0327, -1.1969],\n",
            "        [ 0.7366, -1.1508],\n",
            "        [-1.2513,  1.2258],\n",
            "        [ 0.6206, -0.8850],\n",
            "        [-0.1264,  0.3820],\n",
            "        [-1.3971,  1.4830],\n",
            "        [ 0.9397, -1.0747],\n",
            "        [-1.1830,  1.2693],\n",
            "        [ 1.0220, -1.3323],\n",
            "        [ 0.4702, -0.8491],\n",
            "        [ 0.2130, -0.5790],\n",
            "        [ 0.3547, -0.4305],\n",
            "        [ 1.1243, -1.2478],\n",
            "        [ 0.5290, -0.7136],\n",
            "        [ 1.0016, -1.0647],\n",
            "        [ 0.4310, -0.6930],\n",
            "        [ 0.9681, -0.9767],\n",
            "        [-0.8974,  0.9116],\n",
            "        [ 1.0455, -1.4501],\n",
            "        [ 0.3283, -0.7026],\n",
            "        [ 0.8006, -1.1490],\n",
            "        [ 0.9721, -0.9822],\n",
            "        [-1.6739,  1.7500],\n",
            "        [-1.3975,  1.4488],\n",
            "        [ 0.9539, -0.9383],\n",
            "        [-0.8026,  0.9762],\n",
            "        [-1.3492,  1.3072],\n",
            "        [ 0.9651, -1.1441],\n",
            "        [ 0.4450, -0.9143],\n",
            "        [ 1.0208, -1.0671],\n",
            "        [ 1.1113, -1.2998],\n",
            "        [ 0.8237, -0.9444]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9441, -0.8020],\n",
            "        [ 1.0655, -1.2354],\n",
            "        [ 0.8202, -1.0343],\n",
            "        [ 0.4345, -1.0893],\n",
            "        [-1.1004,  1.3363],\n",
            "        [-0.3748,  0.1068],\n",
            "        [ 0.9749, -0.8000],\n",
            "        [-1.4632,  1.2234],\n",
            "        [-1.3107,  1.4785],\n",
            "        [-1.5344,  1.7046],\n",
            "        [ 0.9189, -1.1315],\n",
            "        [ 1.1405, -1.4278],\n",
            "        [ 0.3799, -0.2709],\n",
            "        [ 0.4673, -0.7313],\n",
            "        [-1.5375,  1.5449],\n",
            "        [-1.4603,  1.4974],\n",
            "        [-1.5007,  1.4125],\n",
            "        [ 0.1634, -0.4540],\n",
            "        [-1.4302,  1.4243],\n",
            "        [ 0.5587, -0.4167],\n",
            "        [ 0.8296, -0.8101],\n",
            "        [-1.4656,  1.3921],\n",
            "        [-1.1707,  1.3901],\n",
            "        [ 0.8628, -1.1216],\n",
            "        [ 1.0231, -0.9738],\n",
            "        [ 0.8729, -0.8037],\n",
            "        [ 1.0496, -1.2161],\n",
            "        [ 0.9260, -1.3683],\n",
            "        [ 1.3792, -1.4545],\n",
            "        [-1.5146,  1.4384],\n",
            "        [-0.7302,  0.6921],\n",
            "        [ 1.0370, -0.9328]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4127,  1.7050],\n",
            "        [ 0.9249, -0.9963],\n",
            "        [ 0.4432, -0.8327],\n",
            "        [-0.6913,  0.7632],\n",
            "        [ 0.5026, -0.6818],\n",
            "        [ 0.8284, -0.8663],\n",
            "        [ 0.8989, -1.1260],\n",
            "        [-1.7022,  1.4212],\n",
            "        [ 0.7761, -1.0897],\n",
            "        [ 0.0533, -0.0928],\n",
            "        [-1.2616,  1.4853],\n",
            "        [ 0.0595, -0.2876],\n",
            "        [-1.3841,  1.5694],\n",
            "        [ 0.9964, -1.1302],\n",
            "        [-1.2738,  1.1906],\n",
            "        [ 0.8499, -1.0623],\n",
            "        [-1.4766,  1.5743],\n",
            "        [ 0.9884, -1.2963],\n",
            "        [-0.2242,  0.0409],\n",
            "        [-0.8733,  0.7294],\n",
            "        [ 0.7056, -0.9269],\n",
            "        [ 0.7905, -0.7785],\n",
            "        [-1.3225,  1.4245],\n",
            "        [-0.8970,  0.5175],\n",
            "        [ 1.1384, -1.1626],\n",
            "        [ 1.0873, -0.8869],\n",
            "        [-1.2816,  1.2829],\n",
            "        [ 1.0963, -1.3990],\n",
            "        [ 0.1413, -0.3756],\n",
            "        [ 0.7310, -1.0148],\n",
            "        [-1.5025,  1.5975],\n",
            "        [ 0.8647, -1.3425]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3761,  0.5065],\n",
            "        [-1.6254,  1.3978],\n",
            "        [ 0.9874, -1.0947],\n",
            "        [ 1.1239, -1.2589],\n",
            "        [ 0.6985, -0.9458],\n",
            "        [ 0.5837, -0.7316],\n",
            "        [-0.2702,  0.0513],\n",
            "        [ 0.4945, -0.6334],\n",
            "        [-1.4283,  1.5776],\n",
            "        [-0.7907,  1.2387],\n",
            "        [ 0.1502, -0.3719],\n",
            "        [-0.7762,  0.3438],\n",
            "        [ 0.8014, -0.7870],\n",
            "        [ 0.6840, -0.8053],\n",
            "        [-1.1031,  1.0902],\n",
            "        [-1.4358,  1.4372],\n",
            "        [-1.2472,  1.2888],\n",
            "        [ 0.3566, -0.5629],\n",
            "        [ 0.9705, -1.2147],\n",
            "        [ 0.3154, -0.3419],\n",
            "        [ 0.3890, -0.7506],\n",
            "        [-1.6292,  1.4081],\n",
            "        [ 0.6925, -1.1589],\n",
            "        [ 0.1933, -0.6201],\n",
            "        [-1.5961,  1.5771],\n",
            "        [ 0.4835, -0.7072],\n",
            "        [-0.0560, -0.1279],\n",
            "        [-0.7684,  0.7118],\n",
            "        [-0.6191,  0.6592],\n",
            "        [-1.0750,  1.1824],\n",
            "        [ 0.5405, -1.1980],\n",
            "        [ 0.0789, -0.1210]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3459, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1143, -0.6209],\n",
            "        [-1.3845,  1.4882],\n",
            "        [ 0.8731, -1.3952],\n",
            "        [ 1.1162, -1.2277],\n",
            "        [ 0.4804, -0.2964],\n",
            "        [ 0.8595, -1.0175],\n",
            "        [-0.5995,  0.6499],\n",
            "        [ 1.0599, -1.2728],\n",
            "        [ 0.8916, -1.0828],\n",
            "        [ 0.7739, -0.8884],\n",
            "        [-1.0414,  0.9437],\n",
            "        [ 0.8468, -1.2136],\n",
            "        [ 0.5456, -0.7027],\n",
            "        [-1.0941,  1.3181],\n",
            "        [-0.9483,  1.1455],\n",
            "        [ 1.0068, -1.2994],\n",
            "        [ 0.8093, -1.1272],\n",
            "        [ 0.2403, -0.3099],\n",
            "        [-1.2368,  1.3239],\n",
            "        [ 0.9151, -1.0476],\n",
            "        [-1.3998,  1.4842],\n",
            "        [-0.4483,  0.3944],\n",
            "        [-1.5478,  1.4982],\n",
            "        [-1.5796,  1.6190],\n",
            "        [ 0.8541, -1.1425],\n",
            "        [-0.7342,  0.8790],\n",
            "        [-1.2930,  1.7481],\n",
            "        [ 1.1154, -1.3346],\n",
            "        [ 1.0259, -1.2452],\n",
            "        [ 0.9931, -1.3995],\n",
            "        [-1.1574,  1.3653],\n",
            "        [-0.7752,  0.5558]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4943, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8409, -1.0704],\n",
            "        [ 0.9307, -0.9524],\n",
            "        [ 0.3389, -0.4975],\n",
            "        [ 1.1456, -1.0989],\n",
            "        [ 0.7530, -1.0464],\n",
            "        [-1.3749,  1.4243],\n",
            "        [-1.5327,  1.7123],\n",
            "        [ 0.8188, -1.2229],\n",
            "        [-1.2650,  1.4502],\n",
            "        [ 0.7629, -1.0805],\n",
            "        [-0.7588,  0.9991],\n",
            "        [ 0.8256, -1.1180],\n",
            "        [ 0.7574, -0.7004],\n",
            "        [ 0.5488, -0.8531],\n",
            "        [ 0.8238, -1.1836],\n",
            "        [ 0.4198, -0.9497],\n",
            "        [ 1.1239, -1.0699],\n",
            "        [ 1.0621, -1.3079],\n",
            "        [ 0.6613, -0.9734],\n",
            "        [-0.8601,  1.0183],\n",
            "        [ 1.1313, -1.0929],\n",
            "        [-1.2733,  1.2660],\n",
            "        [ 0.9902, -1.1956],\n",
            "        [-0.4180,  0.0571],\n",
            "        [ 0.6507, -1.0909],\n",
            "        [ 0.9500, -1.1123],\n",
            "        [-1.6359,  1.4647],\n",
            "        [-1.0986,  1.0848],\n",
            "        [-0.2604,  0.1922],\n",
            "        [ 1.2362, -1.2376],\n",
            "        [-0.3578,  0.3689],\n",
            "        [ 0.7570, -0.9653]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0121,  1.1873],\n",
            "        [ 0.9917, -1.3228],\n",
            "        [-1.2165,  1.2861],\n",
            "        [ 0.8256, -0.9122],\n",
            "        [-1.2468,  1.4985],\n",
            "        [-1.6681,  1.4466],\n",
            "        [-1.5860,  1.6428],\n",
            "        [ 0.5594, -0.9735],\n",
            "        [ 0.7670, -0.9888],\n",
            "        [-1.7628,  1.6040],\n",
            "        [-1.2832,  1.6146],\n",
            "        [ 0.9299, -0.8842],\n",
            "        [ 0.7333, -0.9170],\n",
            "        [ 0.1717, -0.2181],\n",
            "        [ 0.8384, -1.1279],\n",
            "        [ 0.9173, -0.9670],\n",
            "        [ 0.5708, -0.8870],\n",
            "        [ 0.6699, -1.1714],\n",
            "        [ 0.1080, -0.1415],\n",
            "        [ 0.6543, -1.1122],\n",
            "        [ 1.2013, -0.9949],\n",
            "        [ 0.1818, -0.5979],\n",
            "        [ 0.9835, -1.0521],\n",
            "        [ 1.2711, -1.1627],\n",
            "        [-1.2569,  1.3153],\n",
            "        [ 0.8491, -1.3129],\n",
            "        [ 0.8013, -1.2671],\n",
            "        [ 0.4558, -0.8579],\n",
            "        [ 1.1199, -1.3717],\n",
            "        [-1.2160,  1.5174],\n",
            "        [-1.3956,  1.1933],\n",
            "        [-1.5336,  1.4604]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2520,  0.3792],\n",
            "        [ 1.0225, -1.1879],\n",
            "        [ 0.6655, -0.6500],\n",
            "        [ 0.5638, -0.8414],\n",
            "        [-1.1900,  1.2699],\n",
            "        [ 0.9239, -1.0976],\n",
            "        [ 0.7214, -1.1391],\n",
            "        [-1.1920,  1.4748],\n",
            "        [ 0.9705, -1.2572],\n",
            "        [ 0.6976, -1.0601],\n",
            "        [-0.4879,  0.2695],\n",
            "        [ 0.5092, -0.7742],\n",
            "        [-1.0712,  1.0379],\n",
            "        [-1.2663,  1.6086],\n",
            "        [-0.6578,  0.7007],\n",
            "        [ 0.8214, -1.0404],\n",
            "        [ 0.6681, -0.6711],\n",
            "        [ 1.1017, -1.0924],\n",
            "        [ 1.0457, -0.8668],\n",
            "        [ 0.9611, -1.4028],\n",
            "        [-0.8245,  1.0283],\n",
            "        [ 0.3076, -0.6940],\n",
            "        [-0.9169,  0.8580],\n",
            "        [ 0.7544, -0.6175],\n",
            "        [ 0.9402, -1.5400],\n",
            "        [ 0.6447, -0.6970],\n",
            "        [-1.4714,  1.6898],\n",
            "        [ 0.9894, -1.0062],\n",
            "        [ 1.0625, -1.1927],\n",
            "        [-0.1646,  0.2485],\n",
            "        [ 0.7251, -1.1118],\n",
            "        [-0.8072,  0.8225]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2719, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6485, -1.1242],\n",
            "        [-0.3924,  0.2736],\n",
            "        [ 0.8754, -0.8887],\n",
            "        [ 0.6364, -0.8033],\n",
            "        [-1.3473,  1.3491],\n",
            "        [ 0.8017, -1.0057],\n",
            "        [ 0.7787, -1.0847],\n",
            "        [-1.2357,  1.1926],\n",
            "        [-1.6535,  1.4103],\n",
            "        [ 0.4292, -0.8179],\n",
            "        [-1.3676,  1.5476],\n",
            "        [ 0.4483, -0.6914],\n",
            "        [ 1.0850, -1.4730],\n",
            "        [ 1.0292, -1.2569],\n",
            "        [ 1.0860, -1.1287],\n",
            "        [ 0.6903, -0.9481],\n",
            "        [ 0.3995, -0.6260],\n",
            "        [ 0.9352, -1.1360],\n",
            "        [ 0.9583, -1.1839],\n",
            "        [ 0.8777, -1.0759],\n",
            "        [ 0.9799, -1.2576],\n",
            "        [ 0.4989, -1.0239],\n",
            "        [-0.8209,  0.9149],\n",
            "        [ 0.7793, -0.5104],\n",
            "        [-1.3870,  1.2492],\n",
            "        [ 0.9274, -1.3857],\n",
            "        [-1.3701,  1.4394],\n",
            "        [ 0.8120, -1.0138],\n",
            "        [ 0.2129, -0.4985],\n",
            "        [-0.5722,  0.3854],\n",
            "        [-1.4556,  1.4567],\n",
            "        [-1.3471,  1.5089]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6414, -0.8925],\n",
            "        [-1.3410,  1.2706],\n",
            "        [ 1.0493, -1.3259],\n",
            "        [ 0.1458, -0.4687],\n",
            "        [ 0.4931, -0.9633],\n",
            "        [ 0.6704, -1.3891],\n",
            "        [ 0.7505, -0.9576],\n",
            "        [ 0.9036, -1.3124],\n",
            "        [ 0.0309, -0.3098],\n",
            "        [ 0.6884, -0.7825],\n",
            "        [ 0.3807, -0.8564],\n",
            "        [-1.3958,  1.5918],\n",
            "        [-0.3207,  0.2432],\n",
            "        [ 1.0338, -1.1885],\n",
            "        [-1.3130,  1.5588],\n",
            "        [-0.3799,  0.5856],\n",
            "        [ 0.9268, -1.1255],\n",
            "        [-1.2425,  1.2220],\n",
            "        [ 0.9385, -0.9432],\n",
            "        [-1.3951,  1.3921],\n",
            "        [ 0.8686, -1.1113],\n",
            "        [-0.5094,  0.3928],\n",
            "        [ 0.5634, -0.8704],\n",
            "        [-1.3756,  1.6272],\n",
            "        [ 0.5867, -0.9233],\n",
            "        [-1.4067,  1.4191],\n",
            "        [-0.0655,  0.0146],\n",
            "        [-0.5026,  0.5539],\n",
            "        [ 0.9486, -0.8253],\n",
            "        [ 0.3272, -0.7769],\n",
            "        [ 0.6767, -1.0039],\n",
            "        [-1.4639,  1.6546]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4628, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0616, -0.1845],\n",
            "        [ 1.0567, -1.1649],\n",
            "        [ 0.4845, -1.0735],\n",
            "        [ 0.9046, -1.1390],\n",
            "        [-1.6370,  1.3157],\n",
            "        [ 0.8171, -1.0821],\n",
            "        [ 0.8692, -1.4852],\n",
            "        [ 0.5330, -0.8081],\n",
            "        [ 0.1677, -0.4664],\n",
            "        [-1.4353,  1.4851],\n",
            "        [ 1.0307, -0.8651],\n",
            "        [ 0.9073, -1.1504],\n",
            "        [ 0.7796, -1.1117],\n",
            "        [-0.3694,  0.3970],\n",
            "        [-1.3312,  1.4882],\n",
            "        [-0.7010,  0.8130],\n",
            "        [-0.9584,  0.9557],\n",
            "        [ 0.8169, -1.1811],\n",
            "        [-1.6188,  1.7388],\n",
            "        [-1.3198,  1.3392],\n",
            "        [-0.3735,  0.3931],\n",
            "        [ 0.8579, -1.0974],\n",
            "        [-1.4730,  1.4541],\n",
            "        [ 1.0982, -1.2775],\n",
            "        [-1.4953,  1.6950],\n",
            "        [ 0.6613, -1.1337],\n",
            "        [ 0.8010, -1.1294],\n",
            "        [-1.3895,  1.2667],\n",
            "        [ 0.7050, -0.7104],\n",
            "        [ 0.8676, -1.1977],\n",
            "        [-1.4090,  1.3308],\n",
            "        [ 0.9136, -1.2090]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9505, -0.9258],\n",
            "        [ 0.8661, -0.8333],\n",
            "        [-0.0403,  0.0509],\n",
            "        [ 0.3211, -0.0039],\n",
            "        [-1.3117,  1.1785],\n",
            "        [ 0.9449, -0.9844],\n",
            "        [-1.2822,  1.0866],\n",
            "        [-1.5893,  1.6460],\n",
            "        [-1.2487,  1.3742],\n",
            "        [ 0.9128, -1.0638],\n",
            "        [ 0.6866, -1.0017],\n",
            "        [-0.4433,  0.4660],\n",
            "        [ 0.3811, -0.6654],\n",
            "        [-1.1772,  1.2616],\n",
            "        [ 0.8761, -0.9858],\n",
            "        [-1.1336,  1.2121],\n",
            "        [ 0.3633, -0.3946],\n",
            "        [ 0.3522, -0.5534],\n",
            "        [ 1.0068, -1.1911],\n",
            "        [-1.4194,  1.5645],\n",
            "        [-0.8524,  1.2042],\n",
            "        [ 1.0641, -1.2441],\n",
            "        [-0.8404,  1.1037],\n",
            "        [ 0.7920, -1.0552],\n",
            "        [-0.9737,  1.0407],\n",
            "        [-1.0421,  1.0361],\n",
            "        [-0.0624, -0.4432],\n",
            "        [ 0.5856, -0.8489],\n",
            "        [ 0.7167, -1.0655],\n",
            "        [-1.2881,  1.3322],\n",
            "        [ 0.6474, -0.9822],\n",
            "        [ 1.0190, -0.9600]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8139,  1.1064],\n",
            "        [-1.6328,  1.4932],\n",
            "        [-1.5778,  1.6139],\n",
            "        [ 0.7550, -0.9926],\n",
            "        [ 0.4610, -0.9908],\n",
            "        [ 0.4790, -0.7353],\n",
            "        [ 0.6095, -0.9580],\n",
            "        [ 0.5839, -1.0137],\n",
            "        [-1.4828,  1.3740],\n",
            "        [ 0.5102, -0.4568],\n",
            "        [ 0.7187, -0.9880],\n",
            "        [ 0.8521, -1.3515],\n",
            "        [ 0.9838, -1.0572],\n",
            "        [ 1.1372, -1.1486],\n",
            "        [-0.3461,  0.0051],\n",
            "        [-1.4171,  1.4564],\n",
            "        [ 0.9859, -1.3009],\n",
            "        [-1.5919,  1.8830],\n",
            "        [-1.0224,  0.9111],\n",
            "        [-1.4055,  1.6964],\n",
            "        [-0.0246, -0.0942],\n",
            "        [-1.3799,  1.2159],\n",
            "        [ 0.9286, -1.0967],\n",
            "        [ 1.0981, -1.3197],\n",
            "        [-1.3152,  1.3564],\n",
            "        [-0.2191,  0.1416],\n",
            "        [-1.3432,  1.2640],\n",
            "        [-0.2829,  0.2046],\n",
            "        [-1.3276,  1.1808],\n",
            "        [ 0.8051, -1.1475],\n",
            "        [ 1.0189, -0.8916],\n",
            "        [ 0.6049, -1.0877]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7257, -1.2677],\n",
            "        [ 0.0980, -0.0277],\n",
            "        [-1.4480,  1.3015],\n",
            "        [ 0.8387, -0.9969],\n",
            "        [-1.5512,  1.6824],\n",
            "        [ 1.0561, -1.0725],\n",
            "        [-1.5850,  1.6067],\n",
            "        [-1.6950,  1.5370],\n",
            "        [ 0.6856, -0.7388],\n",
            "        [ 0.8402, -1.0550],\n",
            "        [ 1.0044, -0.8897],\n",
            "        [ 0.5654, -0.8179],\n",
            "        [-1.5446,  1.6966],\n",
            "        [-0.0947,  0.5521],\n",
            "        [ 0.1551, -0.2251],\n",
            "        [-1.4431,  1.4440],\n",
            "        [ 0.8632, -1.3015],\n",
            "        [ 0.9419, -1.2157],\n",
            "        [ 0.4179, -0.6242],\n",
            "        [-1.1602,  1.4033],\n",
            "        [ 0.7199, -0.8370],\n",
            "        [ 0.9587, -1.1360],\n",
            "        [ 1.0238, -1.1232],\n",
            "        [-0.3950,  0.5570],\n",
            "        [-0.3204,  0.6286],\n",
            "        [-1.5326,  1.6151],\n",
            "        [ 0.5083, -0.8230],\n",
            "        [-0.8348,  0.9704],\n",
            "        [-1.2787,  1.2339],\n",
            "        [-1.3026,  1.4676],\n",
            "        [ 0.8312, -1.1352],\n",
            "        [ 0.7972, -0.9902]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1262,  1.2444],\n",
            "        [ 0.7316, -0.9241],\n",
            "        [-1.4949,  1.6404],\n",
            "        [-1.3951,  1.4951],\n",
            "        [-0.6425,  0.8756],\n",
            "        [ 0.5932, -0.8792],\n",
            "        [-0.1312,  0.2902],\n",
            "        [ 1.0697, -1.2657],\n",
            "        [ 0.5923, -0.9618],\n",
            "        [-1.2731,  1.4584],\n",
            "        [ 0.2873, -0.6241],\n",
            "        [ 0.1952, -0.7589],\n",
            "        [ 0.6290, -0.9916],\n",
            "        [ 1.0572, -0.9687],\n",
            "        [ 1.0133, -1.0269],\n",
            "        [ 0.7738, -0.8268],\n",
            "        [-0.5507,  0.6879],\n",
            "        [-0.2437,  0.2883],\n",
            "        [ 0.3907, -0.7166],\n",
            "        [ 1.0308, -1.2000],\n",
            "        [ 0.9057, -1.0531],\n",
            "        [ 0.8143, -0.9076],\n",
            "        [ 0.9131, -0.8574],\n",
            "        [-1.2390,  1.6825],\n",
            "        [ 0.9280, -1.0729],\n",
            "        [ 0.6316, -0.9161],\n",
            "        [ 0.8865, -0.9649],\n",
            "        [ 0.7095, -1.0128],\n",
            "        [-0.7720,  0.8594],\n",
            "        [ 0.8988, -1.1075],\n",
            "        [ 0.8599, -1.1534],\n",
            "        [ 1.0994, -1.2562]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1463,  1.3602],\n",
            "        [-1.3351,  1.4395],\n",
            "        [-0.9065,  1.0426],\n",
            "        [ 0.5363, -0.5532],\n",
            "        [ 0.7387, -1.0731],\n",
            "        [ 0.5151, -0.5694],\n",
            "        [ 0.6455, -0.8146],\n",
            "        [ 0.6495, -1.0468],\n",
            "        [-0.9051,  1.2800],\n",
            "        [-0.1209, -0.6351],\n",
            "        [ 0.3724, -0.6072],\n",
            "        [-1.3745,  1.2363],\n",
            "        [-1.5418,  1.6106],\n",
            "        [ 1.0404, -1.1038],\n",
            "        [-0.7593,  1.1880],\n",
            "        [ 0.5428, -0.9944],\n",
            "        [-1.2803,  1.5178],\n",
            "        [ 1.0406, -1.0654],\n",
            "        [ 1.0656, -1.2974],\n",
            "        [ 0.9480, -1.1311],\n",
            "        [-1.0506,  1.2244],\n",
            "        [-1.2312,  1.3995],\n",
            "        [ 0.8301, -0.8928],\n",
            "        [ 0.7798, -1.3212],\n",
            "        [-1.1245,  1.2330],\n",
            "        [ 1.0502, -1.1363],\n",
            "        [-1.5271,  1.5811],\n",
            "        [ 0.8563, -0.9632],\n",
            "        [ 0.8632, -1.1695],\n",
            "        [ 1.0372, -1.1564],\n",
            "        [ 0.3998, -0.7218],\n",
            "        [-0.8325,  1.0456]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0373, -0.9604],\n",
            "        [ 0.3484, -0.6336],\n",
            "        [-1.3735,  1.3954],\n",
            "        [-1.7225,  1.4084],\n",
            "        [ 0.3235, -0.5273],\n",
            "        [-1.1552,  1.1238],\n",
            "        [ 0.4208, -0.8184],\n",
            "        [-0.7293,  0.9596],\n",
            "        [-0.6400,  0.8679],\n",
            "        [-1.1230,  1.4896],\n",
            "        [ 1.0415, -1.1320],\n",
            "        [ 0.9114, -1.2130],\n",
            "        [ 0.9283, -1.0819],\n",
            "        [ 0.7895, -1.1919],\n",
            "        [ 0.2999, -0.8183],\n",
            "        [-1.0289,  1.2835],\n",
            "        [ 1.0741, -1.1907],\n",
            "        [ 0.5732, -0.5476],\n",
            "        [ 0.4178, -0.7089],\n",
            "        [ 0.7814, -0.9048],\n",
            "        [ 0.3748, -0.5309],\n",
            "        [ 0.8053, -1.1478],\n",
            "        [ 0.9600, -1.1802],\n",
            "        [-0.5217,  0.6544],\n",
            "        [ 0.9244, -1.1326],\n",
            "        [ 0.8246, -0.8679],\n",
            "        [-0.8410,  0.9643],\n",
            "        [ 0.8689, -0.9609],\n",
            "        [ 0.8562, -0.7997],\n",
            "        [ 0.3895, -0.1849],\n",
            "        [ 0.9517, -1.2103],\n",
            "        [ 0.8746, -1.2589]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4861,  1.5740],\n",
            "        [ 1.0079, -1.0139],\n",
            "        [ 0.9201, -1.2435],\n",
            "        [ 0.8369, -1.0970],\n",
            "        [-0.0176,  0.0953],\n",
            "        [ 0.8491, -0.9654],\n",
            "        [ 0.7691, -1.0687],\n",
            "        [-1.4301,  1.6372],\n",
            "        [-1.5648,  1.5844],\n",
            "        [-0.9973,  1.3970],\n",
            "        [-0.3739,  0.0050],\n",
            "        [ 0.9334, -1.0315],\n",
            "        [-0.8047,  0.8727],\n",
            "        [ 0.8166, -0.9841],\n",
            "        [-0.8070,  0.7466],\n",
            "        [-1.5932,  1.4933],\n",
            "        [-0.3727,  0.4565],\n",
            "        [-1.3490,  1.7171],\n",
            "        [ 0.9000, -1.1290],\n",
            "        [ 0.7946, -1.0723],\n",
            "        [ 0.9593, -0.9225],\n",
            "        [-1.4648,  1.5466],\n",
            "        [ 0.2796, -0.6647],\n",
            "        [ 0.4618, -0.4783],\n",
            "        [ 0.4494, -0.7252],\n",
            "        [ 0.9049, -0.9815],\n",
            "        [ 0.6670, -0.6276],\n",
            "        [ 0.6183, -0.8825],\n",
            "        [-0.4235,  0.2687],\n",
            "        [ 0.0248,  0.4057],\n",
            "        [-1.0312,  1.0012],\n",
            "        [ 1.0389, -0.9107]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7202, -1.1454],\n",
            "        [-1.4929,  1.5331],\n",
            "        [ 0.6090, -0.5332],\n",
            "        [ 0.7904, -0.8366],\n",
            "        [-1.0756,  1.1369],\n",
            "        [-1.5532,  1.7245],\n",
            "        [ 1.1299, -1.3688],\n",
            "        [ 1.0756, -1.1685],\n",
            "        [-1.6044,  1.5219],\n",
            "        [ 0.9478, -1.3896],\n",
            "        [ 0.7345, -0.4697],\n",
            "        [-0.4105,  0.6324],\n",
            "        [ 0.5914, -0.9304],\n",
            "        [-0.9722,  1.1348],\n",
            "        [-1.2857,  1.4267],\n",
            "        [-1.5986,  1.4928],\n",
            "        [-0.1060,  0.0246],\n",
            "        [ 0.7064, -1.0815],\n",
            "        [-1.3477,  1.2550],\n",
            "        [-0.1246, -0.2996],\n",
            "        [-0.9512,  0.9429],\n",
            "        [-0.9492,  0.8879],\n",
            "        [ 0.7700, -1.0544],\n",
            "        [ 1.1698, -1.2863],\n",
            "        [-1.5821,  1.3830],\n",
            "        [ 0.7659, -0.8367],\n",
            "        [ 0.6912, -0.7208],\n",
            "        [ 0.6300, -0.6878],\n",
            "        [-0.7244,  0.7229],\n",
            "        [-1.4499,  1.5582],\n",
            "        [ 1.1093, -1.1841],\n",
            "        [ 0.9956, -1.0130]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0994, -1.1093],\n",
            "        [ 0.4453, -0.4432],\n",
            "        [-1.2316,  1.3563],\n",
            "        [-1.3836,  1.5920],\n",
            "        [ 0.2892, -0.2523],\n",
            "        [ 0.8078, -1.1073],\n",
            "        [ 1.1181, -1.1257],\n",
            "        [ 0.8540, -0.6641],\n",
            "        [-1.3476,  1.7074],\n",
            "        [ 1.0340, -0.8534],\n",
            "        [ 0.8757, -1.4410],\n",
            "        [ 0.5535, -0.5129],\n",
            "        [ 0.6062, -0.9322],\n",
            "        [ 0.8253, -1.0135],\n",
            "        [ 1.1037, -1.0659],\n",
            "        [-0.2538,  0.4473],\n",
            "        [ 0.7990, -1.0616],\n",
            "        [-0.2378,  0.0471],\n",
            "        [-0.7255,  0.5084],\n",
            "        [-0.5328,  0.6594],\n",
            "        [-0.7758,  0.8630],\n",
            "        [-0.3306,  0.0445],\n",
            "        [ 1.0283, -1.1931],\n",
            "        [ 0.9342, -1.0313],\n",
            "        [ 0.8082, -1.2750],\n",
            "        [-0.8802,  0.9032],\n",
            "        [-1.1529,  1.1528],\n",
            "        [-1.1176,  1.3938],\n",
            "        [-1.2054,  1.4147],\n",
            "        [ 0.6025, -1.3997],\n",
            "        [ 0.6326, -1.1020],\n",
            "        [-1.4248,  1.2779]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9306, -1.1138],\n",
            "        [ 0.3330, -0.7119],\n",
            "        [-0.7844,  0.5197],\n",
            "        [-1.7637,  1.4701],\n",
            "        [-0.6263,  0.3562],\n",
            "        [ 0.5723, -1.1459],\n",
            "        [ 0.6998, -0.8453],\n",
            "        [ 0.3181, -0.3767],\n",
            "        [ 0.8123, -0.8326],\n",
            "        [-1.4750,  1.4549],\n",
            "        [ 0.4258, -0.4443],\n",
            "        [ 0.7447, -0.9885],\n",
            "        [ 0.6959, -1.0043],\n",
            "        [ 0.7570, -1.0345],\n",
            "        [ 0.8297, -1.1577],\n",
            "        [ 0.2090, -0.1870],\n",
            "        [ 0.9594, -1.1721],\n",
            "        [ 1.0993, -1.2448],\n",
            "        [-1.3695,  1.3104],\n",
            "        [-1.6255,  1.4305],\n",
            "        [ 0.4189, -0.7663],\n",
            "        [ 1.0688, -1.0507],\n",
            "        [ 0.4538, -1.1211],\n",
            "        [ 0.7229, -0.8516],\n",
            "        [-1.2457,  1.4550],\n",
            "        [ 1.0465, -1.0535],\n",
            "        [-1.2517,  1.3155],\n",
            "        [ 0.7086, -0.9558],\n",
            "        [ 0.8519, -1.1324],\n",
            "        [ 1.1084, -1.1000],\n",
            "        [-1.1162,  1.0745],\n",
            "        [-1.6936,  1.5945]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4385, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0127,  1.0635],\n",
            "        [ 0.3383,  0.0276],\n",
            "        [-1.4336,  1.1641],\n",
            "        [ 0.5666, -0.9254],\n",
            "        [-0.0956,  0.3995],\n",
            "        [ 0.7685, -0.9721],\n",
            "        [-1.4119,  1.4760],\n",
            "        [-1.1549,  1.5717],\n",
            "        [ 1.2315, -1.2765],\n",
            "        [-1.1452,  1.1709],\n",
            "        [-1.4064,  1.4621],\n",
            "        [-1.3533,  1.3726],\n",
            "        [ 0.9032, -1.0092],\n",
            "        [-1.3252,  1.4148],\n",
            "        [-1.1995,  1.1839],\n",
            "        [-1.4757,  1.6055],\n",
            "        [-1.5773,  1.5839],\n",
            "        [ 0.7072, -0.8713],\n",
            "        [ 1.3016, -1.5266],\n",
            "        [-1.2000,  1.4107],\n",
            "        [ 0.4541, -0.8667],\n",
            "        [ 0.4356, -0.8212],\n",
            "        [-1.5502,  1.6206],\n",
            "        [-1.2318,  1.5350],\n",
            "        [ 0.7114, -1.0603],\n",
            "        [ 0.6996, -0.9660],\n",
            "        [ 0.2938, -0.3473],\n",
            "        [ 0.6239, -1.0135],\n",
            "        [ 0.1257, -0.1938],\n",
            "        [-1.2570,  1.4319],\n",
            "        [ 0.8504, -1.1881],\n",
            "        [-1.0951,  1.3901]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1131, -1.1633],\n",
            "        [-1.3812,  1.5006],\n",
            "        [ 0.0415,  0.0233],\n",
            "        [-1.5855,  1.5709],\n",
            "        [-1.5107,  1.6759],\n",
            "        [-1.1369,  0.9173],\n",
            "        [ 0.7149, -1.1464],\n",
            "        [ 1.0216, -1.0771],\n",
            "        [ 0.3003, -0.5907],\n",
            "        [ 0.4788, -0.8760],\n",
            "        [-0.2044,  0.2963],\n",
            "        [ 0.8827, -1.2328],\n",
            "        [ 0.7210, -0.8095],\n",
            "        [ 0.9339, -1.2565],\n",
            "        [ 0.8754, -1.1473],\n",
            "        [ 0.7339, -1.0490],\n",
            "        [-1.1077,  1.0747],\n",
            "        [ 0.8128, -1.0115],\n",
            "        [-1.4235,  1.6792],\n",
            "        [ 0.8503, -1.0462],\n",
            "        [-0.9832,  1.0200],\n",
            "        [-1.3864,  1.5638],\n",
            "        [-1.0015,  1.1249],\n",
            "        [-1.6905,  1.5275],\n",
            "        [-0.7375,  0.3923],\n",
            "        [ 1.4286, -1.3493],\n",
            "        [ 0.4808, -0.8729],\n",
            "        [ 0.9380, -1.1300],\n",
            "        [-1.6003,  1.8988],\n",
            "        [ 0.8517, -0.8950],\n",
            "        [-1.3182,  1.5476],\n",
            "        [-1.3833,  1.4418]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7448, -1.1365],\n",
            "        [-1.4308,  1.5468],\n",
            "        [-1.5845,  1.4200],\n",
            "        [-1.4755,  1.4961],\n",
            "        [ 0.0474,  0.0403],\n",
            "        [ 0.9327, -1.1772],\n",
            "        [-1.1251,  1.1084],\n",
            "        [ 0.3553, -0.1724],\n",
            "        [-0.2227,  0.2505],\n",
            "        [ 1.0903, -1.1552],\n",
            "        [ 0.7094, -1.0206],\n",
            "        [ 0.7576, -1.4119],\n",
            "        [-0.0297,  0.2310],\n",
            "        [ 0.5326, -1.1665],\n",
            "        [ 1.1591, -1.0681],\n",
            "        [-0.7409,  0.6673],\n",
            "        [-0.6143,  0.6380],\n",
            "        [-1.6828,  1.5792],\n",
            "        [ 0.2060, -0.3074],\n",
            "        [ 0.4071, -0.7847],\n",
            "        [ 0.6365, -0.9604],\n",
            "        [ 0.0349, -0.1685],\n",
            "        [ 0.5509, -0.7706],\n",
            "        [ 0.9106, -1.1808],\n",
            "        [-0.1512,  0.1732],\n",
            "        [-0.6301,  0.6772],\n",
            "        [-0.6867,  1.2726],\n",
            "        [ 0.8363, -1.1216],\n",
            "        [-0.5921,  0.5717],\n",
            "        [-1.4760,  1.4859],\n",
            "        [ 0.8373, -1.0346],\n",
            "        [-0.1387,  0.0817]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8974, -0.8587],\n",
            "        [-0.1510,  0.0615],\n",
            "        [-1.0985,  1.3678],\n",
            "        [ 0.7042, -0.8187],\n",
            "        [-1.2918,  1.2917],\n",
            "        [ 0.8751, -1.0780],\n",
            "        [ 0.9314, -1.0702],\n",
            "        [ 0.9006, -0.8826],\n",
            "        [-1.3303,  1.6035],\n",
            "        [-1.2237,  1.4048],\n",
            "        [ 0.5389, -0.7577],\n",
            "        [ 0.6081, -0.8224],\n",
            "        [ 1.2606, -1.1238],\n",
            "        [-1.1530,  1.2020],\n",
            "        [ 0.8933, -1.0105],\n",
            "        [ 0.7579, -0.7226],\n",
            "        [ 0.2203, -0.3710],\n",
            "        [ 0.9184, -0.7572],\n",
            "        [-0.2695,  0.2976],\n",
            "        [ 0.8260, -0.7355],\n",
            "        [ 0.4920, -0.8222],\n",
            "        [-1.5351,  1.6974],\n",
            "        [ 0.9487, -1.4259],\n",
            "        [-0.8764,  0.7571],\n",
            "        [ 0.2298, -0.0882],\n",
            "        [-1.0807,  1.4695],\n",
            "        [ 0.9862, -1.0750],\n",
            "        [ 1.2787, -1.1070],\n",
            "        [ 0.7075, -1.1279],\n",
            "        [-1.0648,  0.6978],\n",
            "        [ 0.9315, -1.1960],\n",
            "        [ 0.8344, -1.0224]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3404, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2556, -0.3001],\n",
            "        [ 0.1146, -0.2052],\n",
            "        [-0.0271, -0.2372],\n",
            "        [ 0.7150, -0.9593],\n",
            "        [ 1.0249, -1.0073],\n",
            "        [ 1.0134, -1.0918],\n",
            "        [-1.5611,  1.2156],\n",
            "        [-0.4999,  0.7863],\n",
            "        [ 1.2626, -1.1269],\n",
            "        [-1.4220,  1.5083],\n",
            "        [ 1.0779, -1.2807],\n",
            "        [ 0.9844, -1.1978],\n",
            "        [-1.6281,  1.4081],\n",
            "        [ 0.6315, -0.9240],\n",
            "        [ 0.6037, -0.9025],\n",
            "        [ 1.0797, -1.2391],\n",
            "        [ 0.5174, -0.9891],\n",
            "        [ 0.7650, -0.8983],\n",
            "        [ 0.0160,  0.0691],\n",
            "        [-0.0269, -0.1104],\n",
            "        [-1.1759,  1.1245],\n",
            "        [-1.0500,  1.1638],\n",
            "        [-0.2448,  0.2333],\n",
            "        [-0.3957,  0.4768],\n",
            "        [ 0.8450, -0.9251],\n",
            "        [ 0.9544, -1.0985],\n",
            "        [-1.4825,  1.5196],\n",
            "        [ 0.8389, -1.0928],\n",
            "        [-1.3122,  1.1906],\n",
            "        [-1.6420,  1.4877],\n",
            "        [ 0.1218, -0.5353],\n",
            "        [ 0.8422, -1.2398]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4334, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5892,  0.4566],\n",
            "        [-1.4179,  1.4023],\n",
            "        [ 0.8646, -1.0780],\n",
            "        [-0.9478,  0.9005],\n",
            "        [ 0.2270, -0.5207],\n",
            "        [ 0.9036, -0.9547],\n",
            "        [ 1.0174, -1.1681],\n",
            "        [ 0.6907, -0.4680],\n",
            "        [ 1.0199, -1.1517],\n",
            "        [ 0.5808, -1.1569],\n",
            "        [ 0.8259, -1.1806],\n",
            "        [-1.0697,  1.2285],\n",
            "        [ 0.8918, -1.2870],\n",
            "        [ 1.0633, -1.1447],\n",
            "        [ 0.5456, -0.4838],\n",
            "        [-1.4924,  1.3211],\n",
            "        [ 0.9292, -1.1676],\n",
            "        [ 0.0059,  0.3548],\n",
            "        [ 0.9058, -0.9742],\n",
            "        [ 0.7827, -1.0041],\n",
            "        [ 0.1949, -0.3215],\n",
            "        [ 0.9519, -1.2938],\n",
            "        [ 0.8750, -1.0769],\n",
            "        [ 0.7939, -0.7113],\n",
            "        [ 0.3436, -0.8274],\n",
            "        [-1.5073,  1.6052],\n",
            "        [-1.4609,  1.7740],\n",
            "        [-0.2889,  0.2773],\n",
            "        [-0.8857,  0.8270],\n",
            "        [ 0.5114, -0.7694],\n",
            "        [ 0.6417, -1.1431],\n",
            "        [ 1.0823, -1.2993]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.6399,  1.4628],\n",
            "        [ 0.3870, -0.5405],\n",
            "        [ 0.5057, -0.8370],\n",
            "        [ 1.1046, -1.0053],\n",
            "        [-1.1786,  1.1221],\n",
            "        [-1.3202,  1.4102],\n",
            "        [ 0.9612, -1.1857],\n",
            "        [ 0.9930, -1.1693],\n",
            "        [-0.3504,  0.6750],\n",
            "        [-1.5926,  1.8111],\n",
            "        [ 0.6771, -0.7261],\n",
            "        [ 0.5174, -0.6788],\n",
            "        [-1.5571,  1.6641],\n",
            "        [ 1.1216, -1.1253],\n",
            "        [-1.0273,  1.2235],\n",
            "        [ 0.6132, -1.1377],\n",
            "        [-0.4782,  0.2731],\n",
            "        [ 0.8180, -1.0753],\n",
            "        [-0.3469,  0.4729],\n",
            "        [-1.2111,  1.1224],\n",
            "        [-1.6855,  1.6226],\n",
            "        [ 0.5268, -0.6512],\n",
            "        [-0.3386,  0.1505],\n",
            "        [ 1.2086, -1.1899],\n",
            "        [ 0.5408, -0.7765],\n",
            "        [-1.1967,  1.3552],\n",
            "        [ 1.0935, -1.1823],\n",
            "        [ 1.0395, -0.8419],\n",
            "        [ 0.9528, -1.1172],\n",
            "        [-0.1385,  0.3094],\n",
            "        [ 1.0338, -1.1346],\n",
            "        [ 0.8084, -1.0050]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4156, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6338, -0.8666],\n",
            "        [ 1.0623, -1.1085],\n",
            "        [ 1.1743, -1.4844],\n",
            "        [-1.5218,  1.5536],\n",
            "        [ 0.1603, -0.6849],\n",
            "        [-1.4760,  1.3208],\n",
            "        [-1.0184,  1.0068],\n",
            "        [ 0.9552, -0.9711],\n",
            "        [ 0.7036, -1.1250],\n",
            "        [-0.7404,  0.7378],\n",
            "        [ 0.8870, -1.0837],\n",
            "        [ 0.3873, -0.9366],\n",
            "        [ 0.3447, -0.6736],\n",
            "        [ 0.9281, -1.1090],\n",
            "        [-0.1237, -0.2133],\n",
            "        [ 0.5762, -0.9596],\n",
            "        [ 0.2466, -0.2298],\n",
            "        [ 0.7090, -1.0844],\n",
            "        [ 0.9030, -1.2682],\n",
            "        [ 1.1957, -1.3105],\n",
            "        [ 0.4958, -1.2226],\n",
            "        [-1.3993,  1.4947],\n",
            "        [-0.1219, -0.0535],\n",
            "        [ 1.1703, -0.9657],\n",
            "        [-1.6773,  1.6830],\n",
            "        [-1.2903,  1.4570],\n",
            "        [-0.0950, -0.2617],\n",
            "        [ 0.7835, -1.0904],\n",
            "        [ 0.9349, -1.0636],\n",
            "        [ 0.9470, -1.3026],\n",
            "        [ 0.5346, -0.8047],\n",
            "        [-0.0235,  0.1464]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0213, -0.2932],\n",
            "        [ 0.4231, -0.6076],\n",
            "        [ 0.8585, -0.9659],\n",
            "        [-1.3099,  1.2754],\n",
            "        [-1.2340,  1.2785],\n",
            "        [-1.2716,  1.5560],\n",
            "        [ 0.8035, -0.9655],\n",
            "        [-1.4886,  1.5847],\n",
            "        [ 1.1107, -1.2512],\n",
            "        [-1.1119,  1.1508],\n",
            "        [-1.4506,  1.6118],\n",
            "        [ 0.9479, -1.5124],\n",
            "        [ 1.0293, -1.0733],\n",
            "        [ 0.6466, -0.7180],\n",
            "        [ 0.5037, -0.3500],\n",
            "        [ 1.1874, -1.1273],\n",
            "        [ 0.9259, -1.2120],\n",
            "        [-0.8714,  1.1259],\n",
            "        [ 1.0658, -1.3302],\n",
            "        [ 0.4915, -0.5634],\n",
            "        [ 0.8297, -1.0009],\n",
            "        [ 0.9322, -1.2513],\n",
            "        [ 0.8878, -1.0340],\n",
            "        [-1.2608,  1.4141],\n",
            "        [-0.2412,  0.1947],\n",
            "        [ 0.9696, -1.0872],\n",
            "        [ 0.8827, -0.9372],\n",
            "        [ 0.9817, -1.4463],\n",
            "        [ 0.7466, -1.3783],\n",
            "        [-1.2073,  1.1207],\n",
            "        [ 1.1068, -1.1115],\n",
            "        [ 0.7097, -0.4919]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1843, -0.4219],\n",
            "        [ 0.7371, -1.0773],\n",
            "        [ 0.9276, -0.9981],\n",
            "        [-0.3072,  0.2074],\n",
            "        [ 0.6179, -0.5135],\n",
            "        [ 0.9239, -1.0993],\n",
            "        [ 0.7364, -1.2573],\n",
            "        [ 0.3033, -0.1260],\n",
            "        [ 0.7897, -0.8416],\n",
            "        [-0.4832,  0.6746],\n",
            "        [ 0.8521, -0.9965],\n",
            "        [-1.3988,  1.6054],\n",
            "        [ 0.9930, -1.0568],\n",
            "        [-0.3767,  0.4652],\n",
            "        [ 0.6351, -1.0349],\n",
            "        [-0.2478, -0.0526],\n",
            "        [ 0.9459, -1.1237],\n",
            "        [ 0.9581, -0.9343],\n",
            "        [ 0.5617, -1.0314],\n",
            "        [-1.2520,  1.2420],\n",
            "        [ 0.2665, -0.9924],\n",
            "        [ 0.8273, -0.7965],\n",
            "        [-1.4992,  1.1077],\n",
            "        [-1.4843,  1.7890],\n",
            "        [ 0.9018, -1.3913],\n",
            "        [ 0.2397, -0.4375],\n",
            "        [ 1.2920, -1.1618],\n",
            "        [ 0.9752, -1.1356],\n",
            "        [ 0.8211, -0.8070],\n",
            "        [ 0.4274, -0.7545],\n",
            "        [ 0.8746, -1.2566],\n",
            "        [ 0.1452, -0.2497]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3503, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8310,  0.9832],\n",
            "        [ 0.6452, -1.0565],\n",
            "        [ 1.1608, -1.2260],\n",
            "        [ 0.5750, -0.7455],\n",
            "        [-0.1079,  0.1466],\n",
            "        [ 0.9736, -1.1669],\n",
            "        [-1.2836,  1.5550],\n",
            "        [-1.5663,  1.6022],\n",
            "        [ 0.6093, -0.7981],\n",
            "        [ 0.7082, -1.0733]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0:04:22\n",
            "\n",
            "Running Validation...\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6580, device='cuda:0'), logits=tensor([[ 0.7017, -0.9443],\n",
            "        [-1.3368,  1.5011],\n",
            "        [-1.1817,  1.2991],\n",
            "        [ 0.4542, -0.8718],\n",
            "        [ 0.1826, -0.4959],\n",
            "        [-1.2573,  1.3023],\n",
            "        [-0.8035,  0.9664],\n",
            "        [ 1.0039, -1.0953],\n",
            "        [ 1.0053, -1.2307],\n",
            "        [ 1.0970, -1.1988],\n",
            "        [-1.3566,  1.5354],\n",
            "        [-1.4412,  1.4478],\n",
            "        [-1.3558,  1.5296],\n",
            "        [-1.3214,  1.3525],\n",
            "        [-1.5068,  1.6725],\n",
            "        [ 0.8734, -1.0192],\n",
            "        [ 0.5223, -0.8016],\n",
            "        [ 0.9979, -1.1862],\n",
            "        [ 0.4963, -0.5242],\n",
            "        [ 1.0899, -1.2391],\n",
            "        [-1.2663,  1.5228],\n",
            "        [-0.6858,  0.7053],\n",
            "        [-1.4803,  1.5924],\n",
            "        [ 1.0045, -1.2343],\n",
            "        [-1.1544,  1.2549],\n",
            "        [ 0.9565, -1.1791],\n",
            "        [-1.5593,  1.6920],\n",
            "        [-0.7984,  0.8972],\n",
            "        [-1.4701,  1.6418],\n",
            "        [-0.3184,  0.4393],\n",
            "        [-0.9470,  0.8950],\n",
            "        [ 1.0972, -1.1853]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3026, device='cuda:0'), logits=tensor([[-1.1106,  1.2800],\n",
            "        [ 0.7788, -1.1389],\n",
            "        [-0.1303,  0.0677],\n",
            "        [ 0.8898, -0.9370],\n",
            "        [-1.3586,  1.4354],\n",
            "        [ 0.9648, -1.1175],\n",
            "        [ 0.6612, -0.9467],\n",
            "        [ 0.6166, -0.8847],\n",
            "        [ 0.8358, -1.1398],\n",
            "        [ 0.9710, -1.2193],\n",
            "        [ 0.8774, -1.0851],\n",
            "        [ 0.3697, -0.4510],\n",
            "        [ 0.9828, -1.1357],\n",
            "        [ 0.8491, -1.1344],\n",
            "        [-1.1133,  1.1998],\n",
            "        [ 1.0104, -1.1442],\n",
            "        [ 0.8197, -1.0020],\n",
            "        [-1.2996,  1.4694],\n",
            "        [-1.1322,  1.1760],\n",
            "        [ 0.9156, -1.2127],\n",
            "        [-0.7200,  0.7836],\n",
            "        [-0.5160,  0.2888],\n",
            "        [-0.7760,  0.8935],\n",
            "        [ 0.8009, -1.1002],\n",
            "        [-1.0056,  1.2293],\n",
            "        [ 0.1349, -0.0852],\n",
            "        [ 1.1043, -1.2901],\n",
            "        [ 0.6808, -0.9752],\n",
            "        [ 0.8154, -0.9804],\n",
            "        [-0.3279,  0.3162],\n",
            "        [ 0.9349, -1.2077],\n",
            "        [-0.8128,  1.1326]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3589, device='cuda:0'), logits=tensor([[ 1.0072, -1.1589],\n",
            "        [ 0.9836, -1.1414],\n",
            "        [ 0.4303, -0.6967],\n",
            "        [-1.4313,  1.4924],\n",
            "        [-1.4135,  1.5227],\n",
            "        [-1.5810,  1.6487],\n",
            "        [ 0.8467, -0.9723],\n",
            "        [ 0.9813, -1.1990],\n",
            "        [ 1.0112, -1.0640],\n",
            "        [ 0.5142, -0.7199],\n",
            "        [-0.5991,  0.4582],\n",
            "        [ 0.7872, -1.1048],\n",
            "        [ 0.7974, -1.0284],\n",
            "        [ 1.0807, -1.1942],\n",
            "        [-1.4604,  1.4339],\n",
            "        [-0.4221,  0.4671],\n",
            "        [-1.4111,  1.4855],\n",
            "        [ 0.8557, -0.9651],\n",
            "        [-0.1469,  0.0890],\n",
            "        [ 1.0037, -1.2161],\n",
            "        [ 0.9049, -1.1429],\n",
            "        [-1.3754,  1.6025],\n",
            "        [ 0.9482, -1.2116],\n",
            "        [ 0.9494, -1.1122],\n",
            "        [ 0.9690, -1.1468],\n",
            "        [ 0.1841, -0.3544],\n",
            "        [ 1.0334, -1.1506],\n",
            "        [ 0.7728, -1.0054],\n",
            "        [-1.6067,  1.6542],\n",
            "        [-1.5333,  1.6532],\n",
            "        [ 0.8985, -1.0905],\n",
            "        [-1.3281,  1.4458]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3580, device='cuda:0'), logits=tensor([[ 0.5683, -0.6834],\n",
            "        [-0.5265,  0.5724],\n",
            "        [-0.4288,  0.0941],\n",
            "        [ 0.9763, -1.2513],\n",
            "        [ 0.9687, -1.2284],\n",
            "        [ 0.9049, -1.1156],\n",
            "        [-1.5553,  1.6327],\n",
            "        [-0.9271,  1.1820],\n",
            "        [ 0.9374, -1.1360],\n",
            "        [ 0.8670, -1.0389],\n",
            "        [ 0.7969, -0.9853],\n",
            "        [-1.4505,  1.6239],\n",
            "        [-1.3770,  1.2819],\n",
            "        [ 0.7800, -0.9189],\n",
            "        [ 0.4205, -0.6169],\n",
            "        [-1.0267,  1.1952],\n",
            "        [-1.4468,  1.6119],\n",
            "        [ 1.0510, -1.2126],\n",
            "        [-1.4786,  1.6650],\n",
            "        [-1.5536,  1.6365],\n",
            "        [ 0.0375, -0.1755],\n",
            "        [-0.3767,  0.3345],\n",
            "        [ 0.9926, -1.0666],\n",
            "        [-0.1107,  0.0035],\n",
            "        [ 0.4868, -0.6376],\n",
            "        [-0.1244,  0.1086],\n",
            "        [ 0.9004, -1.0257],\n",
            "        [-0.3718,  0.6373],\n",
            "        [-0.4873,  0.4508],\n",
            "        [ 0.9013, -1.0088],\n",
            "        [-1.6200,  1.5815],\n",
            "        [-1.4196,  1.5967]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.1787, device='cuda:0'), logits=tensor([[ 0.7745, -1.1698],\n",
            "        [ 0.4240, -0.7916],\n",
            "        [-1.5105,  1.6983],\n",
            "        [-1.6023,  1.6656],\n",
            "        [ 0.9958, -1.1852],\n",
            "        [ 0.1937, -0.4259],\n",
            "        [ 0.6344, -0.8012],\n",
            "        [-1.5788,  1.6929],\n",
            "        [-1.5489,  1.5488],\n",
            "        [-0.6925,  0.8097],\n",
            "        [ 0.6314, -0.7775],\n",
            "        [-1.4435,  1.6272],\n",
            "        [-1.5356,  1.4747],\n",
            "        [ 1.0508, -1.2348],\n",
            "        [-0.9955,  1.1643],\n",
            "        [-1.3999,  1.5634],\n",
            "        [ 0.9610, -1.0485],\n",
            "        [-1.2156,  1.3249],\n",
            "        [ 1.0119, -1.1888],\n",
            "        [ 1.0163, -1.0731],\n",
            "        [ 0.3706, -0.4225],\n",
            "        [-0.8267,  0.9364],\n",
            "        [-0.6740,  0.7160],\n",
            "        [ 0.9165, -1.0614],\n",
            "        [ 0.9951, -1.1649],\n",
            "        [ 1.0161, -1.1105],\n",
            "        [ 0.9719, -1.2143],\n",
            "        [ 0.4678, -0.7416],\n",
            "        [ 0.7914, -1.0755],\n",
            "        [ 0.8840, -1.1458],\n",
            "        [ 0.8936, -1.1224],\n",
            "        [ 0.8388, -1.0424]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3353, device='cuda:0'), logits=tensor([[-1.5981,  1.6468],\n",
            "        [-1.3386,  1.4212],\n",
            "        [ 0.2346, -0.3874],\n",
            "        [ 0.3346, -0.5655],\n",
            "        [ 0.6743, -0.9280],\n",
            "        [ 0.9616, -1.0530],\n",
            "        [-1.6180,  1.6696],\n",
            "        [-1.3576,  1.5565],\n",
            "        [-1.4678,  1.6378],\n",
            "        [ 0.9562, -1.1554],\n",
            "        [-1.5304,  1.5506],\n",
            "        [ 0.9768, -1.2022],\n",
            "        [-0.6986,  0.5142],\n",
            "        [-1.3869,  1.5450],\n",
            "        [-0.6145,  0.6555],\n",
            "        [ 0.8553, -0.9975],\n",
            "        [-0.1467,  0.3989],\n",
            "        [-0.1104, -0.0368],\n",
            "        [-1.4040,  1.5107],\n",
            "        [ 0.4206, -0.6809],\n",
            "        [ 0.9627, -1.1509],\n",
            "        [ 0.2951, -0.5726],\n",
            "        [-0.9523,  1.0542],\n",
            "        [-1.4351,  1.5019],\n",
            "        [ 0.8386, -1.0512],\n",
            "        [-1.5957,  1.5929],\n",
            "        [ 1.1203, -1.2929],\n",
            "        [ 0.7976, -0.9745],\n",
            "        [-1.1744,  1.4237],\n",
            "        [ 0.9546, -1.1733],\n",
            "        [ 0.8985, -0.9739],\n",
            "        [ 0.2986, -0.6477]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4403, device='cuda:0'), logits=tensor([[ 0.7570, -0.9888],\n",
            "        [-1.3936,  1.5924],\n",
            "        [ 1.0472, -1.0773],\n",
            "        [-0.7293,  0.9173],\n",
            "        [ 1.0029, -1.2481],\n",
            "        [-1.4821,  1.5073],\n",
            "        [-1.6386,  1.6140],\n",
            "        [-1.6423,  1.6164],\n",
            "        [ 0.6252, -0.9261],\n",
            "        [-0.3609,  0.3690],\n",
            "        [ 1.0954, -1.3182],\n",
            "        [ 0.8553, -0.9732],\n",
            "        [-1.3837,  1.4971],\n",
            "        [ 0.1156, -0.4651],\n",
            "        [ 0.8974, -1.2061],\n",
            "        [ 0.6973, -0.9971],\n",
            "        [-1.3628,  1.5049],\n",
            "        [-1.3975,  1.4949],\n",
            "        [ 0.8015, -1.0961],\n",
            "        [-1.5595,  1.6722],\n",
            "        [ 0.2710, -0.6304],\n",
            "        [ 0.4221, -0.7484],\n",
            "        [ 1.0261, -1.1976],\n",
            "        [ 0.1159, -0.4547],\n",
            "        [-1.2321,  1.4544],\n",
            "        [-1.3454,  1.4796],\n",
            "        [ 1.0115, -1.1100],\n",
            "        [ 0.9189, -1.1052],\n",
            "        [ 0.6086, -0.3070],\n",
            "        [ 0.9125, -1.0428],\n",
            "        [ 0.9255, -1.1039],\n",
            "        [ 0.8218, -0.9840]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5113, device='cuda:0'), logits=tensor([[ 0.7232, -0.9489],\n",
            "        [ 0.9382, -1.1824],\n",
            "        [-1.4287,  1.5664],\n",
            "        [-1.5526,  1.6653],\n",
            "        [ 0.8830, -1.0971],\n",
            "        [ 0.8917, -1.0817],\n",
            "        [ 0.6928, -0.9205],\n",
            "        [ 0.4994, -0.8117],\n",
            "        [ 0.2494, -0.5066],\n",
            "        [-0.5563,  0.7730],\n",
            "        [ 0.7384, -0.8935],\n",
            "        [ 1.0473, -1.0316],\n",
            "        [ 0.9134, -1.1928],\n",
            "        [-1.6060,  1.6061],\n",
            "        [-1.4749,  1.6521],\n",
            "        [ 1.0975, -1.2667],\n",
            "        [-0.4011,  0.7310],\n",
            "        [ 0.6969, -0.9815],\n",
            "        [ 0.3381, -0.7538],\n",
            "        [ 0.8217, -1.0565],\n",
            "        [-1.5532,  1.6169],\n",
            "        [-1.0851,  1.0363],\n",
            "        [-0.5293,  0.4390],\n",
            "        [ 0.9634, -1.1399],\n",
            "        [-1.5600,  1.6412],\n",
            "        [ 1.0087, -1.1556],\n",
            "        [ 1.0462, -1.2555],\n",
            "        [ 0.9251, -1.2441],\n",
            "        [-1.3324,  1.4784],\n",
            "        [-0.9333,  0.9058],\n",
            "        [ 0.2224, -0.4592],\n",
            "        [ 0.9177, -1.2199]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3469, device='cuda:0'), logits=tensor([[ 0.1083, -0.3885],\n",
            "        [ 0.9750, -1.1097],\n",
            "        [ 0.8854, -1.1311],\n",
            "        [ 0.9294, -1.2202],\n",
            "        [-1.2486,  1.3430],\n",
            "        [ 0.7643, -0.8848],\n",
            "        [-0.2241,  0.3901],\n",
            "        [-1.0913,  1.2511],\n",
            "        [-1.6079,  1.6782],\n",
            "        [-1.4126,  1.4746],\n",
            "        [ 1.0430, -1.1834],\n",
            "        [ 0.6441, -1.0087],\n",
            "        [ 0.1266, -0.5267],\n",
            "        [ 0.7588, -1.0767],\n",
            "        [ 0.1701, -0.5846],\n",
            "        [ 0.8428, -1.0535],\n",
            "        [ 0.5582, -0.8043],\n",
            "        [ 0.2463, -0.2114],\n",
            "        [ 0.0472, -0.0076],\n",
            "        [ 0.9171, -1.0547],\n",
            "        [-1.2197,  1.2168],\n",
            "        [-1.3777,  1.6372],\n",
            "        [-1.5882,  1.6041],\n",
            "        [-0.9771,  0.8669],\n",
            "        [ 0.5001, -0.7274],\n",
            "        [-0.3111,  0.4244],\n",
            "        [-1.2134,  1.3248],\n",
            "        [ 0.9436, -1.1851],\n",
            "        [-0.9914,  1.2891],\n",
            "        [ 0.8408, -1.2096],\n",
            "        [ 0.2224, -0.4614],\n",
            "        [ 0.5889, -0.8317]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4498, device='cuda:0'), logits=tensor([[ 0.2364, -0.4061],\n",
            "        [-1.4107,  1.5514],\n",
            "        [-1.5803,  1.6794],\n",
            "        [ 0.9648, -1.1827],\n",
            "        [-0.6976,  1.0878],\n",
            "        [-1.5879,  1.6777],\n",
            "        [-1.3379,  1.3158],\n",
            "        [-1.6383,  1.6820],\n",
            "        [-0.2730,  0.3329],\n",
            "        [-1.4480,  1.4925],\n",
            "        [-0.6952,  0.5837],\n",
            "        [ 0.0785, -0.0801],\n",
            "        [ 0.1790, -0.5775],\n",
            "        [-1.4971,  1.5665],\n",
            "        [ 0.7423, -0.6336],\n",
            "        [ 0.8268, -0.9619],\n",
            "        [-1.4526,  1.4608],\n",
            "        [ 0.9252, -1.0880],\n",
            "        [-0.6965,  0.6406],\n",
            "        [-1.5262,  1.4604],\n",
            "        [-0.8684,  0.8326],\n",
            "        [ 0.9865, -1.1644],\n",
            "        [-0.3475,  0.3946],\n",
            "        [-1.4524,  1.4346],\n",
            "        [-1.0122,  1.0645],\n",
            "        [-1.5483,  1.6858],\n",
            "        [ 0.7249, -1.1042],\n",
            "        [ 0.8174, -1.0025],\n",
            "        [-1.4350,  1.5700],\n",
            "        [ 0.6267, -0.8255],\n",
            "        [ 0.9136, -1.0661],\n",
            "        [ 0.7769, -0.9760]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4430, device='cuda:0'), logits=tensor([[-1.4905,  1.6007],\n",
            "        [ 0.6531, -0.8656],\n",
            "        [ 0.8967, -1.0721],\n",
            "        [ 0.9134, -1.0208],\n",
            "        [-1.4545,  1.6252],\n",
            "        [-0.2790, -0.0548],\n",
            "        [-1.5660,  1.6883],\n",
            "        [ 0.3965, -0.7409],\n",
            "        [ 0.9747, -1.1967],\n",
            "        [ 0.9802, -1.1409],\n",
            "        [-0.1689,  0.2248],\n",
            "        [-1.5393,  1.6048],\n",
            "        [ 0.8753, -1.0853],\n",
            "        [ 0.7444, -0.8424],\n",
            "        [ 0.3270, -0.6007],\n",
            "        [ 0.9215, -1.1865],\n",
            "        [ 0.7943, -1.1307],\n",
            "        [-1.0257,  1.1352],\n",
            "        [-1.3880,  1.6728],\n",
            "        [ 0.2249, -0.3713],\n",
            "        [-1.5788,  1.6602],\n",
            "        [ 0.8350, -0.8754],\n",
            "        [-1.4838,  1.6315],\n",
            "        [ 0.5468, -0.7836],\n",
            "        [-1.1833,  1.4217],\n",
            "        [ 0.9557, -1.2101],\n",
            "        [ 1.0637, -1.0201],\n",
            "        [-1.4990,  1.6650],\n",
            "        [-0.6650,  0.8448],\n",
            "        [ 0.6970, -0.9037],\n",
            "        [ 0.8037, -1.0050],\n",
            "        [-0.5515,  0.7117]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4485, device='cuda:0'), logits=tensor([[ 0.7058, -0.9620],\n",
            "        [-1.2169,  1.0640],\n",
            "        [-0.9858,  0.8329],\n",
            "        [ 0.6807, -0.8266],\n",
            "        [-1.3632,  1.4880],\n",
            "        [-0.3034,  0.2823],\n",
            "        [-0.7900,  0.9043],\n",
            "        [ 1.0078, -1.2459],\n",
            "        [ 0.4692, -0.7568],\n",
            "        [-1.4945,  1.5972],\n",
            "        [-0.2575,  0.0267],\n",
            "        [-1.5116,  1.5888],\n",
            "        [-1.4267,  1.5734],\n",
            "        [ 0.9785, -1.1776],\n",
            "        [ 0.0298, -0.0967],\n",
            "        [-1.4204,  1.5461],\n",
            "        [ 1.0377, -1.2107],\n",
            "        [ 0.8127, -0.9867],\n",
            "        [ 0.9757, -1.2471],\n",
            "        [ 0.9200, -1.0882],\n",
            "        [-1.4840,  1.6023],\n",
            "        [ 1.0546, -1.2480],\n",
            "        [ 0.9531, -1.2430],\n",
            "        [ 0.9369, -1.0896],\n",
            "        [-1.5313,  1.5553],\n",
            "        [-0.5161,  0.6732],\n",
            "        [ 1.0882, -1.2440],\n",
            "        [ 0.9636, -1.1755],\n",
            "        [ 0.8721, -1.1386],\n",
            "        [-0.1943,  0.2341],\n",
            "        [ 1.0481, -1.1367],\n",
            "        [-1.5687,  1.6937]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4413, device='cuda:0'), logits=tensor([[-0.2544,  0.1642],\n",
            "        [-0.1357,  0.0281],\n",
            "        [-1.2261,  1.4153],\n",
            "        [ 0.9690, -1.1840],\n",
            "        [-0.3826,  0.4475],\n",
            "        [ 0.8863, -1.0795],\n",
            "        [ 1.0329, -1.1765],\n",
            "        [ 0.6831, -0.9763],\n",
            "        [ 0.8687, -1.1043],\n",
            "        [-1.5367,  1.6700],\n",
            "        [ 0.6744, -0.9718],\n",
            "        [-1.4410,  1.6443],\n",
            "        [ 0.7821, -0.9617],\n",
            "        [ 0.7850, -1.1594],\n",
            "        [ 0.8574, -1.0433],\n",
            "        [-1.3679,  1.4655],\n",
            "        [ 0.9792, -1.0459],\n",
            "        [ 1.0476, -1.1610],\n",
            "        [ 1.0072, -1.1582],\n",
            "        [ 0.9809, -1.3043],\n",
            "        [ 0.9783, -1.2839],\n",
            "        [-1.1501,  1.2634],\n",
            "        [ 0.7720, -1.0664],\n",
            "        [ 0.5643, -0.8392],\n",
            "        [-1.1203,  1.2207],\n",
            "        [ 1.0121, -1.1812],\n",
            "        [ 0.8209, -1.1459],\n",
            "        [-1.1771,  1.0861],\n",
            "        [ 1.0594, -1.2296],\n",
            "        [ 0.7960, -1.0603],\n",
            "        [ 0.6485, -0.9830],\n",
            "        [ 0.1674, -0.1812]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4767, device='cuda:0'), logits=tensor([[-1.5110,  1.6089],\n",
            "        [-0.5292,  0.4688],\n",
            "        [ 0.1845, -0.4423],\n",
            "        [-1.5347,  1.5927],\n",
            "        [ 0.8318, -1.1171],\n",
            "        [ 0.7916, -1.0676],\n",
            "        [-1.3371,  1.3429],\n",
            "        [-1.4971,  1.6457],\n",
            "        [ 0.9247, -1.2058],\n",
            "        [-1.5400,  1.6612],\n",
            "        [-0.2974,  0.2095],\n",
            "        [ 0.4382, -0.8014],\n",
            "        [ 0.9792, -1.1223],\n",
            "        [ 0.8808, -1.1822],\n",
            "        [ 0.5697, -0.4126],\n",
            "        [-1.4059,  1.5559],\n",
            "        [ 0.4238, -0.7948],\n",
            "        [-1.1415,  1.4095],\n",
            "        [-0.5664,  0.6964],\n",
            "        [-0.5319,  0.4692],\n",
            "        [-1.1132,  1.1444],\n",
            "        [ 1.0460, -1.1821],\n",
            "        [-1.4527,  1.4993],\n",
            "        [ 0.6203, -0.9977],\n",
            "        [ 0.6853, -0.9087],\n",
            "        [ 0.8201, -1.0555],\n",
            "        [-1.4367,  1.5847],\n",
            "        [ 0.9247, -1.0703],\n",
            "        [ 1.0136, -1.1810],\n",
            "        [ 0.8421, -0.9722],\n",
            "        [ 0.3354, -0.5497],\n",
            "        [ 0.8341, -1.0162]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4516, device='cuda:0'), logits=tensor([[-1.3476,  1.5023],\n",
            "        [-1.1526,  1.1891],\n",
            "        [ 0.9530, -1.2169],\n",
            "        [ 0.9596, -1.0284],\n",
            "        [ 0.6336, -0.7304],\n",
            "        [ 0.8084, -0.9243],\n",
            "        [-1.0972,  1.1919],\n",
            "        [-1.2274,  1.5046],\n",
            "        [-0.7421,  0.6943],\n",
            "        [ 0.7085, -0.7789],\n",
            "        [ 0.0866, -0.0891],\n",
            "        [-1.4479,  1.5927],\n",
            "        [ 0.7604, -0.8216],\n",
            "        [ 1.0181, -1.1572],\n",
            "        [ 0.6606, -0.9720],\n",
            "        [-1.4328,  1.5920],\n",
            "        [-0.2097,  0.4187],\n",
            "        [ 0.9749, -1.0023],\n",
            "        [-1.5561,  1.5770],\n",
            "        [ 0.8951, -1.0858],\n",
            "        [-1.5387,  1.7143],\n",
            "        [-1.4881,  1.4652],\n",
            "        [ 0.8965, -1.0786],\n",
            "        [ 0.9581, -1.1024],\n",
            "        [ 0.8946, -1.2283],\n",
            "        [-1.1296,  1.3315],\n",
            "        [-1.3839,  1.4611],\n",
            "        [-0.9812,  1.2235],\n",
            "        [ 0.8559, -1.0958],\n",
            "        [ 0.9454, -1.1702],\n",
            "        [-1.4475,  1.4483],\n",
            "        [ 0.8413, -1.0518]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5945, device='cuda:0'), logits=tensor([[ 0.8022, -1.0976],\n",
            "        [-1.2123,  1.3498],\n",
            "        [-1.6423,  1.6100],\n",
            "        [ 0.8366, -1.0389],\n",
            "        [ 0.8454, -1.0012],\n",
            "        [-0.6124,  0.6785],\n",
            "        [-0.0667, -0.0986],\n",
            "        [ 0.8842, -1.2400],\n",
            "        [-0.5808,  0.6320],\n",
            "        [-0.4658,  0.3864],\n",
            "        [ 0.5900, -0.9254],\n",
            "        [ 0.7220, -0.9866],\n",
            "        [ 0.9434, -1.1698],\n",
            "        [ 0.6037, -0.8839],\n",
            "        [-1.4941,  1.6739],\n",
            "        [-1.4784,  1.6195],\n",
            "        [ 0.7589, -0.7213],\n",
            "        [-0.3030,  0.1742],\n",
            "        [-1.5940,  1.6352],\n",
            "        [ 0.9831, -1.0925],\n",
            "        [ 0.9186, -0.8868],\n",
            "        [ 1.0641, -1.2206],\n",
            "        [ 0.9689, -1.2061],\n",
            "        [-0.9145,  0.9480],\n",
            "        [-1.0954,  1.2073],\n",
            "        [ 0.3905, -0.6348],\n",
            "        [-0.4676,  0.4854],\n",
            "        [ 0.4723, -0.5474],\n",
            "        [ 0.6964, -0.8637],\n",
            "        [ 0.0753, -0.4720],\n",
            "        [ 0.7597, -1.0162],\n",
            "        [ 1.1241, -1.2897]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3728, device='cuda:0'), logits=tensor([[-0.7500,  0.8793],\n",
            "        [-1.5176,  1.5849],\n",
            "        [-1.3754,  1.5279],\n",
            "        [ 0.3474, -0.3513],\n",
            "        [-1.4734,  1.5615],\n",
            "        [-1.4308,  1.4414],\n",
            "        [-1.5510,  1.6434],\n",
            "        [ 1.0344, -1.2169],\n",
            "        [ 0.3835, -0.4869],\n",
            "        [ 0.8449, -1.0977],\n",
            "        [-1.5078,  1.6741],\n",
            "        [-0.7032,  0.7575],\n",
            "        [-0.9302,  0.8258],\n",
            "        [ 0.7332, -0.9020],\n",
            "        [ 0.9772, -0.9873],\n",
            "        [ 0.9602, -1.1341],\n",
            "        [ 0.2250, -0.4421],\n",
            "        [-0.5539,  0.3172],\n",
            "        [ 0.9431, -1.1613],\n",
            "        [-0.6049,  0.7270],\n",
            "        [ 0.7670, -1.0473],\n",
            "        [ 0.9946, -1.2466],\n",
            "        [-1.4184,  1.3677],\n",
            "        [ 0.7008, -0.7537],\n",
            "        [ 0.9934, -1.1484],\n",
            "        [ 0.8259, -0.8485],\n",
            "        [-1.2284,  1.5587],\n",
            "        [ 0.8811, -0.9328],\n",
            "        [ 0.9432, -1.0776],\n",
            "        [ 0.2606, -0.4136],\n",
            "        [-1.3631,  1.5802],\n",
            "        [ 0.9022, -1.2251]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4208, device='cuda:0'), logits=tensor([[-1.3454,  1.2718],\n",
            "        [ 0.9728, -1.2365],\n",
            "        [-1.4877,  1.5628],\n",
            "        [ 0.9849, -1.1488],\n",
            "        [ 0.4471, -0.7267],\n",
            "        [-1.4959,  1.4736],\n",
            "        [ 0.9554, -1.2235],\n",
            "        [-0.5716,  0.5680],\n",
            "        [ 1.0374, -1.2551],\n",
            "        [-1.5221,  1.6213],\n",
            "        [ 0.8986, -1.0009],\n",
            "        [-0.0479, -0.2320],\n",
            "        [ 1.0563, -1.2071],\n",
            "        [ 0.6786, -1.0022],\n",
            "        [-1.5546,  1.6398],\n",
            "        [ 0.9241, -1.1294],\n",
            "        [ 0.5724, -0.8595],\n",
            "        [-1.5808,  1.6403],\n",
            "        [-1.3317,  1.1923],\n",
            "        [ 0.9694, -1.2114],\n",
            "        [ 0.7868, -0.9649],\n",
            "        [ 0.5151, -0.7334],\n",
            "        [ 0.7640, -0.9733],\n",
            "        [-1.3091,  1.5058],\n",
            "        [ 0.6694, -0.8935],\n",
            "        [ 0.9198, -0.9492],\n",
            "        [ 0.7666, -0.8420],\n",
            "        [ 0.8305, -0.8609],\n",
            "        [ 0.9573, -1.1629],\n",
            "        [ 0.9015, -1.0781],\n",
            "        [ 0.9258, -1.1641],\n",
            "        [-1.4189,  1.6451]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3558, device='cuda:0'), logits=tensor([[ 0.8742, -0.9935],\n",
            "        [-0.4513,  0.4428],\n",
            "        [ 0.9162, -1.1234],\n",
            "        [ 0.9361, -1.2154],\n",
            "        [-0.6731,  0.5254],\n",
            "        [-1.4051,  1.6434],\n",
            "        [ 0.2284, -0.5867],\n",
            "        [-0.3784,  0.2882],\n",
            "        [-0.8788,  1.2851],\n",
            "        [-1.4395,  1.5604],\n",
            "        [ 0.9252, -1.0429],\n",
            "        [ 0.8828, -1.0186],\n",
            "        [-1.3297,  1.5690],\n",
            "        [ 0.9233, -1.1256],\n",
            "        [ 0.5471, -0.7979],\n",
            "        [ 0.6919, -0.9574],\n",
            "        [-0.8098,  0.7666],\n",
            "        [-1.4792,  1.5551],\n",
            "        [ 0.9444, -1.2147],\n",
            "        [-1.4372,  1.5039],\n",
            "        [-1.5151,  1.5240],\n",
            "        [ 0.8653, -1.0169],\n",
            "        [-0.7515,  0.6124],\n",
            "        [-1.4265,  1.6905],\n",
            "        [ 0.1415, -0.3297],\n",
            "        [ 0.6901, -0.9755],\n",
            "        [ 0.2284, -0.2537],\n",
            "        [ 0.8260, -1.0796],\n",
            "        [-1.5955,  1.6056],\n",
            "        [ 0.7000, -0.9900],\n",
            "        [ 0.0022, -0.0326],\n",
            "        [-0.6791,  0.6680]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3589, device='cuda:0'), logits=tensor([[-1.5060,  1.6382],\n",
            "        [ 0.5176, -0.8710],\n",
            "        [ 0.5061, -0.7279],\n",
            "        [ 1.0391, -1.2295],\n",
            "        [-1.5521,  1.7192],\n",
            "        [-0.3071,  0.1307],\n",
            "        [-1.5404,  1.7168],\n",
            "        [ 1.0099, -1.1385],\n",
            "        [ 1.0747, -1.1754],\n",
            "        [ 0.4077, -0.6786],\n",
            "        [ 0.6569, -0.7451],\n",
            "        [ 0.7332, -0.9922],\n",
            "        [-1.5396,  1.6626],\n",
            "        [ 0.2769, -0.4964],\n",
            "        [ 0.7497, -1.0062],\n",
            "        [ 0.3861, -0.6433],\n",
            "        [ 0.9650, -1.0399],\n",
            "        [-1.4808,  1.5426],\n",
            "        [ 0.1821, -0.3212],\n",
            "        [-1.4941,  1.5439],\n",
            "        [-1.2162,  1.2183],\n",
            "        [ 0.9721, -1.1455],\n",
            "        [ 0.9053, -1.1608],\n",
            "        [ 0.9142, -1.0882],\n",
            "        [-0.3167,  0.3698],\n",
            "        [ 0.5792, -0.8582],\n",
            "        [ 0.7607, -0.8258],\n",
            "        [-0.7251,  0.8688],\n",
            "        [-0.0182,  0.1440],\n",
            "        [-0.2867,  0.3613],\n",
            "        [-1.4736,  1.6429],\n",
            "        [-1.3972,  1.5870]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3630, device='cuda:0'), logits=tensor([[-1.4817,  1.4589],\n",
            "        [-0.3582,  0.4892],\n",
            "        [ 0.9959, -1.1698],\n",
            "        [-1.4783,  1.4905],\n",
            "        [ 0.1800, -0.3792],\n",
            "        [ 1.1011, -1.1904],\n",
            "        [-0.8824,  0.7813],\n",
            "        [ 0.9913, -1.1814],\n",
            "        [ 0.7909, -1.0201],\n",
            "        [ 0.3386, -0.5459],\n",
            "        [ 0.9131, -1.1500],\n",
            "        [ 0.7700, -0.9004],\n",
            "        [ 0.5341, -0.8473],\n",
            "        [-1.3872,  1.4862],\n",
            "        [ 0.9450, -1.1985],\n",
            "        [ 1.0374, -1.1639],\n",
            "        [ 0.8399, -1.0932],\n",
            "        [ 0.4310, -0.7089],\n",
            "        [ 1.0762, -1.2500],\n",
            "        [-1.2246,  1.3388],\n",
            "        [-0.9790,  0.8892],\n",
            "        [-1.4687,  1.5396],\n",
            "        [-1.5721,  1.6231],\n",
            "        [-0.5273,  0.2720],\n",
            "        [ 0.7390, -0.9662],\n",
            "        [-1.3972,  1.5172],\n",
            "        [ 0.3312, -0.6256],\n",
            "        [ 0.9199, -1.1500],\n",
            "        [ 0.2513, -0.3490],\n",
            "        [ 0.3972, -0.3315],\n",
            "        [ 0.8582, -1.1262],\n",
            "        [ 0.5065, -0.6732]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3879, device='cuda:0'), logits=tensor([[-1.6132,  1.7356],\n",
            "        [ 0.9006, -1.0376],\n",
            "        [ 0.8434, -1.0329],\n",
            "        [ 0.8918, -0.9379],\n",
            "        [ 1.0046, -1.1205],\n",
            "        [-1.3816,  1.4025],\n",
            "        [ 0.4715, -0.6719],\n",
            "        [ 0.9100, -1.1382],\n",
            "        [-1.3925,  1.5876],\n",
            "        [ 0.9509, -1.2000],\n",
            "        [-0.9594,  0.9585],\n",
            "        [-0.0564, -0.0556],\n",
            "        [-1.3190,  1.4630],\n",
            "        [-0.5728,  0.9431],\n",
            "        [-0.5932,  0.6486],\n",
            "        [-1.5545,  1.6975],\n",
            "        [-1.5384,  1.5808],\n",
            "        [-0.5410,  0.4737],\n",
            "        [ 0.9761, -1.2114],\n",
            "        [ 0.7900, -1.1083],\n",
            "        [ 0.7940, -1.1081],\n",
            "        [-1.4353,  1.5745],\n",
            "        [ 0.9514, -1.1956],\n",
            "        [-1.3871,  1.2641],\n",
            "        [-1.2397,  1.4432],\n",
            "        [-1.6233,  1.7387],\n",
            "        [-0.9596,  0.9457],\n",
            "        [-1.0062,  1.1585],\n",
            "        [-1.5377,  1.5850],\n",
            "        [ 0.4312, -0.8039],\n",
            "        [-1.3037,  1.4153],\n",
            "        [ 0.6267, -0.9707]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.7024, device='cuda:0'), logits=tensor([[ 0.9986, -1.2407],\n",
            "        [-0.9299,  0.8114],\n",
            "        [ 0.8336, -1.1612],\n",
            "        [-0.6113,  0.5759],\n",
            "        [-1.5229,  1.6771],\n",
            "        [-1.0512,  1.2829],\n",
            "        [ 0.8207, -1.0661],\n",
            "        [-0.5331,  0.5105],\n",
            "        [-1.5357,  1.4448],\n",
            "        [-0.9495,  0.8700],\n",
            "        [ 0.4893, -0.8772],\n",
            "        [ 0.6750, -0.8609],\n",
            "        [-0.8929,  0.9845],\n",
            "        [-1.1051,  0.8989],\n",
            "        [ 1.1318, -1.2684],\n",
            "        [-1.4771,  1.5502],\n",
            "        [ 0.9101, -1.0218],\n",
            "        [ 0.9345, -1.2708],\n",
            "        [-1.5422,  1.5149],\n",
            "        [-1.4821,  1.4946],\n",
            "        [-1.2246,  1.2208],\n",
            "        [ 1.0223, -1.1539],\n",
            "        [ 0.5199, -0.7613],\n",
            "        [-1.1435,  1.1811],\n",
            "        [ 0.7184, -1.0340],\n",
            "        [-1.2149,  1.3323],\n",
            "        [-0.9082,  1.2892],\n",
            "        [ 0.9667, -1.1321],\n",
            "        [ 0.3330, -0.6347],\n",
            "        [ 1.0679, -1.2176],\n",
            "        [ 0.9125, -1.1750],\n",
            "        [ 0.4333, -0.7621]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2063, device='cuda:0'), logits=tensor([[-0.6552,  0.8111],\n",
            "        [ 0.8592, -1.0273],\n",
            "        [-1.4636,  1.5507],\n",
            "        [ 0.9566, -1.0078],\n",
            "        [-1.4994,  1.5497],\n",
            "        [ 0.9251, -1.1191],\n",
            "        [ 0.9099, -1.1193],\n",
            "        [ 0.7543, -0.8840],\n",
            "        [ 0.5790, -0.8494],\n",
            "        [-1.6036,  1.6989],\n",
            "        [ 1.0236, -1.2421],\n",
            "        [-1.1901,  1.4913],\n",
            "        [ 0.3885, -0.4495],\n",
            "        [-1.5536,  1.6190],\n",
            "        [-0.1303, -0.1550],\n",
            "        [ 0.9096, -1.0746],\n",
            "        [ 0.8742, -0.9476],\n",
            "        [-1.5433,  1.5284],\n",
            "        [-1.0830,  1.0401],\n",
            "        [-1.1012,  1.1361],\n",
            "        [ 1.0485, -1.1442],\n",
            "        [ 0.5712, -0.7101],\n",
            "        [ 0.8204, -1.1252],\n",
            "        [ 0.7304, -1.0167],\n",
            "        [ 0.6469, -0.7274],\n",
            "        [ 0.0377, -0.4641],\n",
            "        [ 0.2363, -0.5118],\n",
            "        [-1.6244,  1.5909],\n",
            "        [-1.4367,  1.5354],\n",
            "        [ 0.8140, -1.0562],\n",
            "        [ 0.8420, -1.0543],\n",
            "        [-1.5929,  1.7298]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3071, device='cuda:0'), logits=tensor([[-0.2888,  0.1471],\n",
            "        [ 0.9329, -1.1707],\n",
            "        [ 1.0221, -1.2966],\n",
            "        [-1.3318,  1.3362],\n",
            "        [ 0.9120, -1.2173],\n",
            "        [ 0.2903, -0.6279],\n",
            "        [ 0.7167, -0.9311],\n",
            "        [-0.9240,  1.1773],\n",
            "        [-1.4964,  1.6394],\n",
            "        [-1.4885,  1.5130],\n",
            "        [ 0.2964, -0.5001],\n",
            "        [-0.6909,  0.7957],\n",
            "        [-0.0727,  0.2656],\n",
            "        [ 0.9326, -1.0913],\n",
            "        [-0.3200,  0.1813],\n",
            "        [ 0.6279, -0.9501],\n",
            "        [-1.4317,  1.4586],\n",
            "        [ 0.9669, -1.2507],\n",
            "        [ 0.9573, -1.1295],\n",
            "        [-1.3748,  1.5466],\n",
            "        [-1.2814,  1.3389],\n",
            "        [ 0.8860, -1.1205],\n",
            "        [-1.4358,  1.5966],\n",
            "        [ 0.9863, -1.1645],\n",
            "        [-1.4515,  1.6250],\n",
            "        [ 0.5796, -0.8021],\n",
            "        [ 0.9707, -1.0889],\n",
            "        [ 1.0464, -1.1968],\n",
            "        [-0.2264,  0.0676],\n",
            "        [ 0.8819, -0.9844],\n",
            "        [ 0.6391, -0.9113],\n",
            "        [-1.4496,  1.6253]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3338, device='cuda:0'), logits=tensor([[ 0.8083, -1.0442],\n",
            "        [ 0.9238, -1.2434],\n",
            "        [ 0.7787, -1.0684],\n",
            "        [ 0.5636, -0.8416],\n",
            "        [ 0.8900, -1.1246],\n",
            "        [ 0.9083, -1.0482],\n",
            "        [ 1.0969, -1.2958],\n",
            "        [ 0.9343, -1.1100],\n",
            "        [ 0.8159, -1.0971],\n",
            "        [ 0.9695, -1.2168],\n",
            "        [ 0.7311, -0.8725],\n",
            "        [ 0.5304, -0.6866],\n",
            "        [ 1.0815, -1.2385],\n",
            "        [-1.6185,  1.6907],\n",
            "        [-1.2872,  1.4244],\n",
            "        [ 0.7715, -1.1712],\n",
            "        [ 0.5919, -0.8559],\n",
            "        [-1.5194,  1.6446],\n",
            "        [ 0.7223, -1.0584],\n",
            "        [-1.5160,  1.4901],\n",
            "        [ 0.9921, -1.1508],\n",
            "        [ 0.8004, -1.1613],\n",
            "        [-1.0167,  0.9539],\n",
            "        [-1.3675,  1.4360],\n",
            "        [-1.4632,  1.5563],\n",
            "        [ 0.0830, -0.5026],\n",
            "        [ 0.7755, -1.0601],\n",
            "        [ 0.9481, -1.2058],\n",
            "        [-1.3573,  1.6236],\n",
            "        [ 0.3551, -0.6980],\n",
            "        [ 0.9901, -1.2067],\n",
            "        [-0.1093,  0.2955]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4199, device='cuda:0'), logits=tensor([[-1.3210,  1.5792],\n",
            "        [-1.0865,  1.0782],\n",
            "        [ 0.2835, -0.5244],\n",
            "        [ 0.9975, -1.0511],\n",
            "        [ 0.9760, -1.2123],\n",
            "        [ 0.8471, -1.1673],\n",
            "        [ 0.9573, -1.2113],\n",
            "        [ 0.9256, -1.1994],\n",
            "        [ 0.7571, -0.8962],\n",
            "        [-1.1791,  1.1378],\n",
            "        [ 1.0032, -1.2473],\n",
            "        [ 1.0468, -1.2603],\n",
            "        [ 0.6330, -0.9085],\n",
            "        [-1.2396,  1.3648],\n",
            "        [ 0.0439, -0.2139],\n",
            "        [ 0.8539, -1.1535],\n",
            "        [-0.6886,  0.6458],\n",
            "        [-1.5225,  1.6306],\n",
            "        [-1.5791,  1.6674],\n",
            "        [ 0.4563, -0.5380],\n",
            "        [ 0.4685, -0.8268],\n",
            "        [-1.4733,  1.6405],\n",
            "        [ 0.9046, -1.1511],\n",
            "        [ 1.0556, -1.1313],\n",
            "        [ 0.9622, -1.1622],\n",
            "        [-1.5590,  1.7173],\n",
            "        [ 0.9749, -1.2355],\n",
            "        [ 0.7388, -1.0178],\n",
            "        [-1.6260,  1.7223],\n",
            "        [-0.2279,  0.4425],\n",
            "        [ 0.1604, -0.0706],\n",
            "        [-0.2672,  0.4596]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4672, device='cuda:0'), logits=tensor([[ 1.0517, -1.2668],\n",
            "        [ 0.9954, -1.1692],\n",
            "        [-1.3106,  1.2424],\n",
            "        [ 0.8814, -1.1528],\n",
            "        [ 0.9887, -1.1725],\n",
            "        [ 1.0327, -1.1245],\n",
            "        [ 0.8429, -1.1132],\n",
            "        [-0.4946,  0.6868],\n",
            "        [-1.3352,  1.4298],\n",
            "        [ 0.2858, -0.5364],\n",
            "        [ 0.9603, -1.2442],\n",
            "        [ 0.9299, -1.2163],\n",
            "        [ 1.0400, -1.1815],\n",
            "        [-1.5433,  1.6428],\n",
            "        [ 0.1563, -0.4005],\n",
            "        [-1.4398,  1.5539],\n",
            "        [-0.0667, -0.1470],\n",
            "        [ 1.1051, -1.1958],\n",
            "        [ 0.9307, -1.1204],\n",
            "        [ 0.7577, -1.0999],\n",
            "        [-1.1513,  1.2875],\n",
            "        [ 0.9979, -1.2473],\n",
            "        [ 1.0701, -1.2385],\n",
            "        [-1.0272,  0.8531],\n",
            "        [ 0.3028, -0.6508],\n",
            "        [-0.0748, -0.1477],\n",
            "        [ 0.9669, -1.2408],\n",
            "        [-1.5041,  1.5870],\n",
            "        [-1.5851,  1.7368],\n",
            "        [ 0.0740, -0.4023],\n",
            "        [ 0.4202, -0.6474],\n",
            "        [-1.2986,  1.5384]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4619, device='cuda:0'), logits=tensor([[-0.4631,  0.5303],\n",
            "        [ 0.9456, -1.1099],\n",
            "        [-1.4776,  1.7020],\n",
            "        [ 0.9186, -0.9930],\n",
            "        [ 0.7140, -1.0050],\n",
            "        [ 0.6197, -0.8942],\n",
            "        [-1.2886,  1.4273],\n",
            "        [ 0.9940, -1.1106],\n",
            "        [ 0.7647, -1.0347],\n",
            "        [ 1.0622, -1.2300],\n",
            "        [ 0.8384, -1.1333],\n",
            "        [-0.7023,  0.5697],\n",
            "        [-1.4738,  1.6424],\n",
            "        [-1.0790,  1.2967],\n",
            "        [-1.4376,  1.6063],\n",
            "        [ 0.9078, -0.9769],\n",
            "        [-1.4616,  1.6301],\n",
            "        [ 0.8199, -1.0905],\n",
            "        [ 0.5326, -0.9307],\n",
            "        [-1.3477,  1.5580],\n",
            "        [ 1.0770, -1.1361],\n",
            "        [ 1.0983, -1.1375],\n",
            "        [-1.4960,  1.4435],\n",
            "        [ 0.9586, -1.1711],\n",
            "        [ 0.3556, -0.6581],\n",
            "        [ 1.0510, -0.9909],\n",
            "        [-1.4918,  1.5688],\n",
            "        [ 0.1933, -0.3148],\n",
            "        [-0.9104,  0.8112],\n",
            "        [-1.3888,  1.6157],\n",
            "        [ 0.2704, -0.3787],\n",
            "        [-1.5090,  1.5746]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3072, device='cuda:0'), logits=tensor([[ 0.5720, -0.9351],\n",
            "        [-0.7182,  0.8879],\n",
            "        [ 1.0138, -1.2243],\n",
            "        [-1.4632,  1.6036],\n",
            "        [-0.2151,  0.1446],\n",
            "        [-1.6286,  1.7122],\n",
            "        [-0.0671,  0.2325],\n",
            "        [ 0.9699, -1.1574],\n",
            "        [-1.6202,  1.6103],\n",
            "        [ 0.5182, -0.8056],\n",
            "        [-0.2369,  0.1333],\n",
            "        [ 0.0056, -0.1664],\n",
            "        [-1.3047,  1.5005],\n",
            "        [ 0.4155, -0.7117],\n",
            "        [ 0.4309, -0.7139],\n",
            "        [-1.0428,  1.2509],\n",
            "        [-1.0740,  1.2330],\n",
            "        [ 1.0750, -1.1849],\n",
            "        [ 0.0385, -0.1987],\n",
            "        [ 0.5526, -0.9320],\n",
            "        [ 0.7512, -0.9953],\n",
            "        [ 0.8773, -1.0016],\n",
            "        [ 0.9016, -1.0604],\n",
            "        [ 0.9851, -1.2624],\n",
            "        [-1.4644,  1.6387],\n",
            "        [ 0.9394, -1.1013],\n",
            "        [-0.7242,  0.7702],\n",
            "        [ 0.8343, -1.0527],\n",
            "        [ 0.4986, -0.2668],\n",
            "        [ 0.9923, -1.1918],\n",
            "        [ 1.0489, -1.2184],\n",
            "        [ 0.9289, -1.0145]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5124, device='cuda:0'), logits=tensor([[ 1.0369, -1.2405],\n",
            "        [ 0.9349, -1.2563],\n",
            "        [ 0.9273, -1.1432],\n",
            "        [ 0.1409, -0.5427],\n",
            "        [-0.2336,  0.3811],\n",
            "        [-0.3937,  0.4299],\n",
            "        [ 0.9399, -1.0542],\n",
            "        [ 0.2855, -0.5315],\n",
            "        [-1.4001,  1.5378],\n",
            "        [ 0.9415, -1.2449],\n",
            "        [ 0.6506, -0.9162],\n",
            "        [ 0.9555, -1.0924],\n",
            "        [-0.8124,  0.9155],\n",
            "        [ 0.9308, -1.0623],\n",
            "        [-0.6863,  0.6074],\n",
            "        [ 1.1046, -1.1807],\n",
            "        [ 0.7058, -1.0372],\n",
            "        [ 0.8635, -1.1384],\n",
            "        [ 0.9690, -1.1217],\n",
            "        [ 0.1373, -0.2294],\n",
            "        [ 0.6291, -0.7517],\n",
            "        [ 1.0442, -1.2620],\n",
            "        [ 0.9006, -1.0698],\n",
            "        [ 0.2424,  0.0520],\n",
            "        [ 1.0673, -1.2340],\n",
            "        [-1.1278,  0.9878],\n",
            "        [-1.5352,  1.6403],\n",
            "        [-1.4900,  1.5603],\n",
            "        [ 1.0114, -1.1849],\n",
            "        [ 0.8336, -1.0831],\n",
            "        [ 0.6087, -0.7453],\n",
            "        [ 0.9929, -1.1433]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3104, device='cuda:0'), logits=tensor([[ 0.6709, -0.9206],\n",
            "        [ 0.8882, -1.1398],\n",
            "        [ 1.0415, -1.2171],\n",
            "        [-1.5983,  1.6371],\n",
            "        [ 0.8355, -1.1218],\n",
            "        [-0.1959, -0.0632],\n",
            "        [ 0.9709, -1.1031],\n",
            "        [-1.6479,  1.6475],\n",
            "        [-0.7377,  0.7354],\n",
            "        [-0.8487,  0.9696],\n",
            "        [-0.2130,  0.0208],\n",
            "        [-0.4949,  0.4532],\n",
            "        [ 0.7076, -0.5354],\n",
            "        [ 0.9020, -1.1414],\n",
            "        [ 0.9626, -1.1227],\n",
            "        [-0.0684, -0.4216],\n",
            "        [ 1.0437, -1.1707],\n",
            "        [ 0.6727, -0.8649],\n",
            "        [-0.5235,  0.7117],\n",
            "        [ 0.5395, -0.5855],\n",
            "        [ 0.3318, -0.7959],\n",
            "        [-0.4299,  0.3746],\n",
            "        [ 0.9951, -1.2704],\n",
            "        [-1.4886,  1.5213],\n",
            "        [-1.2694,  1.4346],\n",
            "        [-1.5091,  1.5951],\n",
            "        [ 0.9354, -1.1029],\n",
            "        [ 1.0304, -1.1728],\n",
            "        [-1.4676,  1.4546],\n",
            "        [ 0.6965, -0.9744],\n",
            "        [-0.3027,  0.2193],\n",
            "        [ 1.0486, -1.2033]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6066, device='cuda:0'), logits=tensor([[ 0.7384, -0.8867],\n",
            "        [-1.5222,  1.6248],\n",
            "        [ 0.9600, -1.0730],\n",
            "        [-1.3681,  1.6816],\n",
            "        [ 0.7936, -0.9865],\n",
            "        [ 0.9008, -1.1802],\n",
            "        [ 0.6215, -1.0023],\n",
            "        [ 0.6023, -0.4178],\n",
            "        [ 0.8699, -1.1566],\n",
            "        [-0.9929,  0.8859],\n",
            "        [ 0.6233, -0.8842],\n",
            "        [ 0.5559, -0.8786],\n",
            "        [-1.5977,  1.6812],\n",
            "        [ 1.1522, -1.2127],\n",
            "        [-0.5347,  0.3056],\n",
            "        [ 1.0308, -1.2653],\n",
            "        [ 0.4732, -0.8467],\n",
            "        [-1.0172,  1.1026],\n",
            "        [ 0.8681, -1.1587],\n",
            "        [-0.0446, -0.2066],\n",
            "        [-1.3444,  1.5028],\n",
            "        [ 0.9803, -1.1524],\n",
            "        [-1.5448,  1.5297],\n",
            "        [-1.4511,  1.4956],\n",
            "        [ 0.7317, -0.9741],\n",
            "        [-1.0655,  0.8437],\n",
            "        [-0.1539,  0.2883],\n",
            "        [-1.2895,  1.4919],\n",
            "        [ 0.5571, -0.6089],\n",
            "        [-1.0908,  1.3720],\n",
            "        [ 0.9467, -1.1670],\n",
            "        [ 0.8963, -1.0351]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2180, device='cuda:0'), logits=tensor([[ 0.9553, -1.1689],\n",
            "        [-0.9829,  1.2033],\n",
            "        [ 0.5520, -0.7559],\n",
            "        [-0.5077,  0.3618],\n",
            "        [-0.9249,  1.0427],\n",
            "        [ 0.5747, -0.7186],\n",
            "        [ 0.2822, -0.5741],\n",
            "        [-1.5371,  1.5557],\n",
            "        [ 0.5261, -0.8328],\n",
            "        [ 0.8559, -1.1097],\n",
            "        [ 1.0274, -1.1635],\n",
            "        [ 0.8068, -0.8967],\n",
            "        [ 1.0507, -1.1734],\n",
            "        [ 0.4989, -0.8303],\n",
            "        [-1.4831,  1.5963],\n",
            "        [ 0.8157, -0.9907],\n",
            "        [ 0.0681, -0.3805],\n",
            "        [-1.6204,  1.6939],\n",
            "        [ 0.9407, -1.1565],\n",
            "        [ 0.8334, -1.1476],\n",
            "        [-0.8138,  0.8568],\n",
            "        [ 0.7438, -1.0182],\n",
            "        [ 0.6064, -0.7954],\n",
            "        [-1.3748,  1.6029],\n",
            "        [-1.3265,  1.4066],\n",
            "        [-1.4130,  1.5027],\n",
            "        [ 0.9230, -1.2423],\n",
            "        [ 0.7663, -1.0728],\n",
            "        [ 0.9393, -1.1450],\n",
            "        [-1.3666,  1.5979],\n",
            "        [-1.1748,  1.4408],\n",
            "        [ 0.6044, -1.0081]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4043, device='cuda:0'), logits=tensor([[ 1.0762, -1.2916],\n",
            "        [ 0.1916, -0.2862],\n",
            "        [ 0.3989, -0.7816],\n",
            "        [ 0.7742, -1.0803],\n",
            "        [ 0.8144, -1.1465],\n",
            "        [-1.4849,  1.5651],\n",
            "        [-1.5181,  1.7001],\n",
            "        [ 0.7072, -0.9366],\n",
            "        [-1.4655,  1.5066],\n",
            "        [-0.4662,  0.5710],\n",
            "        [-1.5268,  1.6284],\n",
            "        [ 0.8737, -1.1996],\n",
            "        [-1.4058,  1.4194],\n",
            "        [ 0.6808, -0.8695],\n",
            "        [ 0.4153, -0.5599],\n",
            "        [ 1.0616, -1.1744],\n",
            "        [ 0.5995, -0.8880],\n",
            "        [-1.5629,  1.6448],\n",
            "        [-1.4135,  1.5130],\n",
            "        [ 0.2488, -0.3972],\n",
            "        [-1.1895,  1.2471],\n",
            "        [ 0.6869, -0.9679],\n",
            "        [-1.3364,  1.6226],\n",
            "        [ 0.8407, -1.1004],\n",
            "        [-1.1832,  1.4041],\n",
            "        [ 0.5681, -0.6534],\n",
            "        [-0.3732,  0.1972],\n",
            "        [ 0.9819, -1.1969],\n",
            "        [ 0.8957, -1.1768],\n",
            "        [ 0.7594, -1.0084],\n",
            "        [ 0.3246, -0.6674],\n",
            "        [ 0.9000, -1.0361]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3903, device='cuda:0'), logits=tensor([[ 1.0650, -1.2571],\n",
            "        [-1.2419,  1.3162],\n",
            "        [ 0.5456, -0.8375],\n",
            "        [ 1.0161, -1.1682],\n",
            "        [ 0.9707, -1.1047],\n",
            "        [ 0.7070, -0.9814],\n",
            "        [ 0.5191, -0.8450],\n",
            "        [-1.5702,  1.6080],\n",
            "        [-1.4108,  1.6193],\n",
            "        [-1.4101,  1.5804],\n",
            "        [-1.5636,  1.6305],\n",
            "        [-0.4391,  0.4312],\n",
            "        [-1.3426,  1.4287],\n",
            "        [-1.0094,  1.1864],\n",
            "        [ 1.0579, -1.1967],\n",
            "        [-1.5726,  1.5805],\n",
            "        [ 0.9351, -1.0726],\n",
            "        [ 0.0925, -0.0215],\n",
            "        [ 0.7244, -1.0102],\n",
            "        [-0.1561, -0.1820],\n",
            "        [ 0.3961, -0.5518],\n",
            "        [-0.8394,  0.9269],\n",
            "        [ 0.5597, -0.9214],\n",
            "        [-0.4191,  0.4473],\n",
            "        [ 0.5536, -0.9043],\n",
            "        [-0.1152,  0.1343],\n",
            "        [ 0.9640, -1.0504],\n",
            "        [ 0.7036, -0.9394],\n",
            "        [-1.5868,  1.6988],\n",
            "        [-0.5313,  0.5770],\n",
            "        [-1.4975,  1.7083],\n",
            "        [ 0.8814, -1.1773]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2547, device='cuda:0'), logits=tensor([[ 0.6790, -0.8719],\n",
            "        [ 0.2663, -0.5714],\n",
            "        [-1.3952,  1.6888],\n",
            "        [-1.5584,  1.6469],\n",
            "        [ 0.9681, -1.0891],\n",
            "        [-1.3886,  1.4564],\n",
            "        [ 0.9546, -1.2231],\n",
            "        [ 1.0327, -1.2188],\n",
            "        [-1.4893,  1.6791],\n",
            "        [-0.6606,  0.6997],\n",
            "        [ 1.0636, -1.2546],\n",
            "        [-0.0164, -0.2349],\n",
            "        [-1.5762,  1.6238],\n",
            "        [ 0.4520, -0.7441],\n",
            "        [ 0.8788, -1.1239],\n",
            "        [ 0.9149, -1.0873],\n",
            "        [-1.5849,  1.6951],\n",
            "        [ 0.5091, -0.7905],\n",
            "        [ 0.6753, -0.9665],\n",
            "        [-1.4479,  1.6255],\n",
            "        [-1.4815,  1.7098],\n",
            "        [-1.5060,  1.6315],\n",
            "        [ 0.7937, -1.1592],\n",
            "        [ 0.8576, -1.1726],\n",
            "        [ 0.9917, -1.0240],\n",
            "        [ 0.4947, -0.8238],\n",
            "        [-1.5303,  1.4838],\n",
            "        [-1.4604,  1.5748],\n",
            "        [ 0.9740, -1.1728],\n",
            "        [-1.1501,  1.2721],\n",
            "        [-1.5767,  1.6867],\n",
            "        [ 0.0462, -0.3373]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6195, device='cuda:0'), logits=tensor([[ 1.0041, -1.2349],\n",
            "        [ 0.0826, -0.3753],\n",
            "        [ 0.7069, -0.9265],\n",
            "        [-1.3926,  1.5479],\n",
            "        [ 0.7142, -0.6925],\n",
            "        [ 0.5282, -0.7743],\n",
            "        [ 1.0537, -1.1173],\n",
            "        [ 0.6383, -0.7979],\n",
            "        [-1.4533,  1.5676],\n",
            "        [ 0.9358, -1.0101],\n",
            "        [-0.2469,  0.3495],\n",
            "        [-1.3938,  1.5065],\n",
            "        [ 0.8601, -1.1897],\n",
            "        [ 0.9965, -1.1940],\n",
            "        [ 0.4238, -0.7948],\n",
            "        [-1.5402,  1.6568],\n",
            "        [-0.8767,  0.8757],\n",
            "        [ 0.7406, -0.7232],\n",
            "        [ 0.6720, -0.9819],\n",
            "        [-0.6024,  0.8116],\n",
            "        [ 0.3932, -0.7340],\n",
            "        [ 0.5418, -0.9405],\n",
            "        [-1.3252,  1.4517],\n",
            "        [-1.0940,  1.2306],\n",
            "        [ 1.0851, -1.1712],\n",
            "        [ 0.5969, -0.8695],\n",
            "        [ 0.7385, -0.9541],\n",
            "        [ 0.9909, -1.2965],\n",
            "        [-1.3911,  1.4922],\n",
            "        [-0.7797,  0.8470],\n",
            "        [ 0.4837, -0.7299],\n",
            "        [ 0.4003, -0.7786]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4394, device='cuda:0'), logits=tensor([[ 0.8516, -0.9951],\n",
            "        [-1.1367,  1.4334],\n",
            "        [-0.4414,  0.3268],\n",
            "        [ 0.7880, -0.9665],\n",
            "        [ 0.8965, -1.2198],\n",
            "        [ 1.0371, -1.2784],\n",
            "        [ 0.9340, -0.9327],\n",
            "        [ 0.7887, -0.8686],\n",
            "        [-0.8985,  0.9718],\n",
            "        [ 0.4550, -0.6037],\n",
            "        [ 0.9284, -1.0988],\n",
            "        [-1.2569,  1.3638],\n",
            "        [ 0.8351, -1.1016],\n",
            "        [ 0.4156, -0.6872],\n",
            "        [-1.5045,  1.6038],\n",
            "        [ 0.8709, -0.8147],\n",
            "        [ 0.4920, -0.5016],\n",
            "        [ 0.5707, -0.8681],\n",
            "        [-1.0199,  1.0293],\n",
            "        [ 0.8698, -1.1534],\n",
            "        [-1.4746,  1.6550],\n",
            "        [-0.6003,  0.5076],\n",
            "        [-1.0714,  1.2111],\n",
            "        [ 0.8072, -0.9878],\n",
            "        [ 1.0076, -1.2173],\n",
            "        [ 0.8583, -1.1085],\n",
            "        [ 0.0242, -0.2559],\n",
            "        [-1.3810,  1.5811],\n",
            "        [ 0.1362, -0.1259],\n",
            "        [ 0.5022, -0.7331],\n",
            "        [ 0.6743, -0.9481],\n",
            "        [ 0.9881, -1.1285]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4098, device='cuda:0'), logits=tensor([[ 1.4417e-02, -1.9890e-01],\n",
            "        [-1.4328e+00,  1.5920e+00],\n",
            "        [-1.2244e+00,  1.2757e+00],\n",
            "        [ 9.0614e-01, -9.8978e-01],\n",
            "        [ 9.4279e-01, -1.1101e+00],\n",
            "        [ 5.7307e-01, -9.6896e-01],\n",
            "        [ 6.1055e-01, -8.8504e-01],\n",
            "        [-1.4718e+00,  1.6217e+00],\n",
            "        [ 7.4745e-01, -8.8443e-01],\n",
            "        [ 6.6473e-01, -8.7679e-01],\n",
            "        [ 9.8951e-01, -1.1686e+00],\n",
            "        [ 8.2810e-01, -9.5150e-01],\n",
            "        [ 7.9674e-01, -8.4272e-01],\n",
            "        [-2.4872e-01, -1.1672e-01],\n",
            "        [ 4.5259e-01, -7.7610e-01],\n",
            "        [-3.6192e-01,  4.9477e-01],\n",
            "        [-1.5070e+00,  1.6709e+00],\n",
            "        [ 7.6575e-01, -1.0387e+00],\n",
            "        [-1.3032e-03, -2.1174e-01],\n",
            "        [ 9.3938e-01, -1.1312e+00],\n",
            "        [ 3.2667e-01, -6.4973e-01],\n",
            "        [-1.3871e+00,  1.5328e+00],\n",
            "        [ 8.8936e-01, -1.1637e+00],\n",
            "        [ 5.6843e-01, -9.1491e-01],\n",
            "        [ 1.0302e+00, -1.2189e+00],\n",
            "        [-4.0275e-01,  3.6766e-01],\n",
            "        [-4.9925e-01,  3.8776e-01],\n",
            "        [ 8.5197e-01, -1.1650e+00],\n",
            "        [-1.0790e+00,  9.3803e-01],\n",
            "        [ 1.0022e+00, -1.2119e+00],\n",
            "        [-1.6005e+00,  1.6708e+00],\n",
            "        [ 8.7707e-01, -9.7615e-01]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5295, device='cuda:0'), logits=tensor([[ 0.9892, -1.2657],\n",
            "        [ 0.4626, -0.6955],\n",
            "        [ 1.0318, -1.2700],\n",
            "        [-1.4544,  1.6585],\n",
            "        [ 0.7601, -1.1179],\n",
            "        [ 0.5184, -0.9083],\n",
            "        [ 0.6712, -0.9252],\n",
            "        [ 0.9574, -1.1364],\n",
            "        [ 0.3078, -0.2498],\n",
            "        [-1.5828,  1.6893],\n",
            "        [ 1.0389, -1.1892],\n",
            "        [ 0.8105, -1.1445],\n",
            "        [-0.8769,  1.0913],\n",
            "        [ 0.5970, -0.9388],\n",
            "        [-1.2142,  1.3409],\n",
            "        [-1.4684,  1.6089],\n",
            "        [-1.4119,  1.4384],\n",
            "        [-1.3743,  1.5450],\n",
            "        [-1.4717,  1.6055],\n",
            "        [-0.7984,  0.8972],\n",
            "        [-1.4906,  1.5080],\n",
            "        [ 0.8355, -1.0758],\n",
            "        [ 0.8194, -1.1410],\n",
            "        [ 1.0145, -1.1525],\n",
            "        [ 0.7471, -1.0002],\n",
            "        [ 0.9064, -1.1036],\n",
            "        [ 0.4636, -0.7055],\n",
            "        [ 0.8526, -1.0827],\n",
            "        [-1.0271,  1.0648],\n",
            "        [ 0.6646, -0.9936],\n",
            "        [ 0.9224, -1.1499],\n",
            "        [ 0.6292, -0.9288]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3617, device='cuda:0'), logits=tensor([[-1.2627,  1.3624],\n",
            "        [-1.0678,  1.0610],\n",
            "        [ 0.1355, -0.1155],\n",
            "        [-1.5392,  1.6114],\n",
            "        [ 0.5406, -0.8500],\n",
            "        [ 0.5485, -0.7737],\n",
            "        [ 0.2215, -0.3184],\n",
            "        [ 1.0316, -1.1083],\n",
            "        [ 0.5015, -0.7729],\n",
            "        [-1.3105,  1.2178],\n",
            "        [ 0.4710, -0.7972],\n",
            "        [ 1.0946, -1.2574],\n",
            "        [-1.5085,  1.6661],\n",
            "        [ 1.0668, -1.1847],\n",
            "        [-1.4837,  1.5458],\n",
            "        [-1.1025,  1.0924],\n",
            "        [ 0.7715, -1.0534],\n",
            "        [-1.3253,  1.5010],\n",
            "        [ 0.7560, -1.0829],\n",
            "        [ 0.7580, -1.0730],\n",
            "        [ 0.5255, -0.5273],\n",
            "        [ 0.9510, -1.1737],\n",
            "        [-0.1693,  0.1321],\n",
            "        [-0.2223,  0.2772],\n",
            "        [ 0.1232, -0.2762],\n",
            "        [-0.9237,  1.0488],\n",
            "        [-1.5343,  1.6196],\n",
            "        [-1.5045,  1.6088],\n",
            "        [ 0.9853, -1.1854],\n",
            "        [ 0.5671, -0.7714],\n",
            "        [-1.3706,  1.4227],\n",
            "        [-1.4078,  1.5184]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3386, device='cuda:0'), logits=tensor([[ 1.0851, -1.2903],\n",
            "        [ 1.0371, -1.2645],\n",
            "        [ 0.1939, -0.1986],\n",
            "        [ 1.0260, -1.2519],\n",
            "        [-0.7540,  0.7357],\n",
            "        [ 0.9509, -1.1708],\n",
            "        [ 0.9870, -1.1613],\n",
            "        [ 0.2306, -0.5030],\n",
            "        [-1.4010,  1.7005],\n",
            "        [ 0.6804, -0.8895],\n",
            "        [ 0.5361, -0.7973],\n",
            "        [ 0.5820, -0.6943],\n",
            "        [-1.2344,  1.4576],\n",
            "        [ 0.7962, -1.0753],\n",
            "        [-1.2060,  1.2610],\n",
            "        [-1.5852,  1.7088],\n",
            "        [ 0.7137, -0.8198],\n",
            "        [ 0.9657, -1.1048],\n",
            "        [ 0.9014, -1.0503],\n",
            "        [ 0.9430, -1.2094],\n",
            "        [ 0.6911, -1.0329],\n",
            "        [-0.2229,  0.0616],\n",
            "        [ 0.8785, -1.1571],\n",
            "        [-0.0613, -0.1375],\n",
            "        [-0.7543,  0.7834],\n",
            "        [ 0.6397, -0.9270],\n",
            "        [-0.1504,  0.3099],\n",
            "        [-1.1005,  1.3366],\n",
            "        [ 0.9959, -1.2767],\n",
            "        [-1.5502,  1.6512],\n",
            "        [ 0.9763, -1.1920],\n",
            "        [ 0.8622, -1.1198]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5277, device='cuda:0'), logits=tensor([[-1.4844,  1.6521],\n",
            "        [ 0.8300, -0.9641],\n",
            "        [ 0.8501, -1.0291],\n",
            "        [-1.6059,  1.6432],\n",
            "        [ 0.4785, -0.9061],\n",
            "        [ 0.9859, -1.0398],\n",
            "        [ 0.7868, -0.9269],\n",
            "        [ 0.6990, -0.8700],\n",
            "        [ 0.5612, -0.8624],\n",
            "        [ 1.0339, -1.2569],\n",
            "        [-0.3857,  0.2633],\n",
            "        [ 0.3762, -0.6258],\n",
            "        [ 0.8025, -0.7403],\n",
            "        [ 0.6717, -0.9133],\n",
            "        [ 0.6989, -0.9257],\n",
            "        [ 0.1789, -0.3890],\n",
            "        [ 0.3730, -0.6312],\n",
            "        [ 0.0912, -0.3029],\n",
            "        [-1.3832,  1.5389],\n",
            "        [-1.4337,  1.4972],\n",
            "        [ 0.6964, -0.9348],\n",
            "        [ 0.5997, -0.8725],\n",
            "        [ 0.8018, -1.0169],\n",
            "        [-0.1993,  0.2827],\n",
            "        [-0.0300, -0.2299],\n",
            "        [-0.6530,  0.5618],\n",
            "        [ 0.9625, -1.1953],\n",
            "        [ 0.8140, -1.0112],\n",
            "        [-1.0700,  1.2292],\n",
            "        [-0.6835,  0.5911],\n",
            "        [-1.3287,  1.3327],\n",
            "        [-1.4917,  1.6998]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2721, device='cuda:0'), logits=tensor([[-1.4565,  1.6204],\n",
            "        [ 1.0498, -1.2411],\n",
            "        [ 0.6154, -0.7148],\n",
            "        [ 1.0936, -1.2224],\n",
            "        [ 0.9271, -1.1939],\n",
            "        [-1.2033,  1.4728],\n",
            "        [ 1.0265, -1.1979],\n",
            "        [ 0.9527, -1.2841],\n",
            "        [-1.3687,  1.4225],\n",
            "        [-0.6518,  0.5647],\n",
            "        [ 0.7330, -0.8415],\n",
            "        [ 0.7375, -1.0379],\n",
            "        [-1.2219,  1.3918],\n",
            "        [ 0.8652, -1.1352],\n",
            "        [ 0.9039, -0.9232],\n",
            "        [-1.2163,  1.2597],\n",
            "        [ 1.0756, -1.2718],\n",
            "        [ 0.3176, -0.5425],\n",
            "        [ 0.9564, -1.0931],\n",
            "        [-0.2314,  0.1841],\n",
            "        [-1.3687,  1.4225],\n",
            "        [ 0.0263,  0.0501],\n",
            "        [ 0.9679, -1.2302],\n",
            "        [-1.4444,  1.4391],\n",
            "        [-1.1257,  1.2885],\n",
            "        [-1.3554,  1.4254],\n",
            "        [-1.3917,  1.3667],\n",
            "        [-1.1601,  1.3155],\n",
            "        [ 0.7594, -0.9561],\n",
            "        [ 1.0193, -1.2491],\n",
            "        [-1.4491,  1.5168],\n",
            "        [ 0.8056, -0.8012]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4628, device='cuda:0'), logits=tensor([[-0.2281,  0.3804],\n",
            "        [ 0.9735, -1.2856],\n",
            "        [-1.5328,  1.6675],\n",
            "        [ 0.8267, -1.1279],\n",
            "        [ 0.5078, -0.9434],\n",
            "        [ 1.0625, -1.1583],\n",
            "        [-1.4778,  1.6433],\n",
            "        [ 0.7786, -1.1193],\n",
            "        [-0.5792,  0.5716],\n",
            "        [-0.7921,  0.7228],\n",
            "        [ 0.8248, -0.8488],\n",
            "        [ 0.5865, -0.7951],\n",
            "        [ 0.6741, -0.9792],\n",
            "        [ 0.1644, -0.3408],\n",
            "        [ 0.7852, -1.1529],\n",
            "        [ 0.8066, -1.0336],\n",
            "        [ 0.7713, -0.9870],\n",
            "        [-0.9888,  1.0420],\n",
            "        [ 0.9866, -1.1443],\n",
            "        [ 0.7085, -0.6489],\n",
            "        [ 0.9973, -1.1789],\n",
            "        [ 0.8892, -1.1801],\n",
            "        [ 0.9180, -1.0677],\n",
            "        [ 0.0655, -0.1051],\n",
            "        [ 1.0033, -1.2327],\n",
            "        [ 0.7765, -1.0825],\n",
            "        [-0.3608,  0.2952],\n",
            "        [ 0.4809, -0.7392],\n",
            "        [ 0.4234, -0.7299],\n",
            "        [ 1.0859, -1.1539],\n",
            "        [ 0.8880, -1.1504],\n",
            "        [ 0.4305, -0.7638]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5475, device='cuda:0'), logits=tensor([[ 0.1193, -0.3041],\n",
            "        [-1.4585,  1.7112],\n",
            "        [-1.3321,  1.3812],\n",
            "        [-0.8139,  1.0571],\n",
            "        [ 0.4825, -0.9069],\n",
            "        [-0.8361,  0.8913],\n",
            "        [ 0.2441, -0.5754],\n",
            "        [ 0.5717, -0.9186],\n",
            "        [ 0.7405, -0.9037],\n",
            "        [-0.3172,  0.4303],\n",
            "        [-1.4323,  1.6452],\n",
            "        [ 0.8848, -1.0464],\n",
            "        [ 0.7803, -0.8860],\n",
            "        [ 1.0571, -1.1877],\n",
            "        [-0.8705,  0.9668],\n",
            "        [ 0.9195, -1.1195],\n",
            "        [ 0.9522, -1.2156],\n",
            "        [ 0.6181, -0.9169],\n",
            "        [ 0.9549, -1.1227],\n",
            "        [ 0.8890, -1.1173],\n",
            "        [ 1.0067, -1.2086],\n",
            "        [-1.5295,  1.6864],\n",
            "        [-0.3029,  0.2088],\n",
            "        [-1.5794,  1.6220],\n",
            "        [-1.5601,  1.6248],\n",
            "        [ 1.0072, -1.0628],\n",
            "        [ 0.8331, -1.1035],\n",
            "        [-0.7313,  0.5844],\n",
            "        [ 0.4414, -0.8101],\n",
            "        [ 0.8405, -1.1710],\n",
            "        [ 0.7302, -1.0116],\n",
            "        [ 0.9845, -1.1537]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5215, device='cuda:0'), logits=tensor([[-0.1484, -0.0485],\n",
            "        [ 0.5852, -0.6876],\n",
            "        [ 0.9510, -0.9354],\n",
            "        [ 0.4900, -0.7528],\n",
            "        [ 0.9256, -0.9374],\n",
            "        [ 1.0576, -1.0847],\n",
            "        [ 0.9195, -1.1619],\n",
            "        [ 0.9238, -1.2223],\n",
            "        [ 0.1745, -0.5545],\n",
            "        [ 1.0393, -1.1631],\n",
            "        [ 0.8187, -1.0519],\n",
            "        [-1.5010,  1.6870],\n",
            "        [ 0.5610, -0.6635],\n",
            "        [ 0.8235, -0.9762],\n",
            "        [ 1.0273, -1.2531],\n",
            "        [ 0.2975, -0.3221],\n",
            "        [-0.6834,  0.5911],\n",
            "        [ 0.8234, -0.9697],\n",
            "        [ 0.9813, -1.1958]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "  Accuracy: 0.82\n",
            "  F1: 0.77\n",
            "  Validation Loss: 0.42\n",
            "  Validation took: 0:00:20\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5776, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4310, -0.7446],\n",
            "        [ 1.0309, -1.3371],\n",
            "        [ 0.8056, -0.5500],\n",
            "        [ 0.8715, -1.1687],\n",
            "        [ 0.6344, -0.8128],\n",
            "        [ 0.7779, -0.9720],\n",
            "        [ 1.0485, -1.4943],\n",
            "        [ 0.2177, -0.6236],\n",
            "        [ 0.5293, -0.7881],\n",
            "        [ 0.7592, -1.0099],\n",
            "        [ 0.9502, -1.1406],\n",
            "        [-1.3197,  1.5484],\n",
            "        [ 0.4076, -1.0825],\n",
            "        [ 0.6812, -1.1499],\n",
            "        [-0.6888,  1.0700],\n",
            "        [-0.7651,  0.7620],\n",
            "        [ 0.7933, -1.0309],\n",
            "        [ 0.4369, -0.6413],\n",
            "        [ 0.7604, -0.7780],\n",
            "        [ 0.0273, -0.1160],\n",
            "        [-0.5843,  0.6155],\n",
            "        [ 0.6962, -0.7778],\n",
            "        [ 0.5899, -0.5987],\n",
            "        [ 1.1066, -1.3042],\n",
            "        [ 0.5918, -0.8060],\n",
            "        [-1.5654,  1.5110],\n",
            "        [ 0.8361, -1.0196],\n",
            "        [ 1.1914, -1.1686],\n",
            "        [ 0.8392, -1.2023],\n",
            "        [ 0.8705, -0.7874],\n",
            "        [-1.3836,  1.5799],\n",
            "        [ 0.6008, -1.1581]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7446, -0.9688],\n",
            "        [ 0.8901, -1.1675],\n",
            "        [ 0.2891, -0.2915],\n",
            "        [ 0.9378, -1.0298],\n",
            "        [ 0.9465, -1.3619],\n",
            "        [ 0.6990, -1.2130],\n",
            "        [-1.6735,  1.2435],\n",
            "        [-0.7030,  1.3938],\n",
            "        [ 0.3310, -0.9375],\n",
            "        [ 1.0753, -1.1649],\n",
            "        [-1.0227,  1.1555],\n",
            "        [ 1.0042, -1.2788],\n",
            "        [ 0.7891, -0.8840],\n",
            "        [ 1.1962, -1.2811],\n",
            "        [ 0.7654, -1.1362],\n",
            "        [-1.3058,  1.3012],\n",
            "        [-0.7087,  0.8136],\n",
            "        [ 0.8990, -1.3712],\n",
            "        [-1.3108,  1.5208],\n",
            "        [ 0.6478, -0.8093],\n",
            "        [-1.0997,  1.1409],\n",
            "        [ 0.7952, -1.0015],\n",
            "        [ 0.1718, -0.3806],\n",
            "        [-1.3610,  1.7381],\n",
            "        [-0.9340,  1.0868],\n",
            "        [-1.1702,  1.3704],\n",
            "        [ 0.9835, -1.2935],\n",
            "        [ 0.5250, -0.8982],\n",
            "        [ 0.2729, -0.7019],\n",
            "        [-1.6052,  1.4562],\n",
            "        [ 0.9016, -1.5397],\n",
            "        [-0.9466,  0.9077]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3363, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6279, -0.6525],\n",
            "        [-0.9443,  0.9567],\n",
            "        [-1.4883,  1.2826],\n",
            "        [-1.3355,  1.4267],\n",
            "        [ 0.7113, -0.7216],\n",
            "        [ 0.9250, -1.1115],\n",
            "        [ 0.4444, -0.6803],\n",
            "        [ 0.4805, -0.5779],\n",
            "        [-0.4860,  0.4117],\n",
            "        [ 0.8369, -0.8768],\n",
            "        [-1.2738,  1.3546],\n",
            "        [-1.5343,  1.4788],\n",
            "        [-1.6106,  1.6463],\n",
            "        [ 0.0700,  0.1726],\n",
            "        [ 0.5613, -0.3752],\n",
            "        [ 0.0446, -0.4504],\n",
            "        [ 0.2904, -0.6181],\n",
            "        [ 0.7949, -1.0431],\n",
            "        [-1.3794,  1.2452],\n",
            "        [ 1.0144, -1.1803],\n",
            "        [-1.3555,  1.5982],\n",
            "        [ 0.9705, -1.1879],\n",
            "        [-1.5139,  1.6912],\n",
            "        [ 0.5720, -1.0264],\n",
            "        [-0.8414,  0.9753],\n",
            "        [ 0.7864, -1.1801],\n",
            "        [ 1.0267, -1.2303],\n",
            "        [ 1.0980, -1.3842],\n",
            "        [-1.3025,  1.2796],\n",
            "        [ 0.6609, -1.1838],\n",
            "        [-0.1866, -0.2675],\n",
            "        [ 0.1010, -0.4432]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2953, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6022, -0.7497],\n",
            "        [-1.2113,  0.9106],\n",
            "        [ 0.3977, -0.7400],\n",
            "        [ 0.8913, -1.0645],\n",
            "        [-1.3435,  1.4889],\n",
            "        [ 0.6082, -1.0279],\n",
            "        [-0.5328,  0.5249],\n",
            "        [ 0.6280, -0.8384],\n",
            "        [ 0.7963, -1.2481],\n",
            "        [ 0.4285, -0.8550],\n",
            "        [-1.0470,  1.1319],\n",
            "        [-1.5704,  1.5966],\n",
            "        [-1.1107,  1.0981],\n",
            "        [ 0.9246, -1.1365],\n",
            "        [-1.0828,  1.2122],\n",
            "        [ 1.1785, -1.1309],\n",
            "        [ 0.4486, -0.8512],\n",
            "        [-0.9762,  0.8897],\n",
            "        [ 0.0381, -0.1433],\n",
            "        [-1.4876,  1.8107],\n",
            "        [ 0.5181, -0.8536],\n",
            "        [ 1.0755, -1.0581],\n",
            "        [ 0.3885, -0.6719],\n",
            "        [-1.2753,  1.3476],\n",
            "        [-1.1895,  1.4160],\n",
            "        [ 0.0818, -0.4215],\n",
            "        [ 0.8333, -0.9312],\n",
            "        [-1.0435,  1.1639],\n",
            "        [-0.7712,  0.6085],\n",
            "        [ 0.8727, -0.8050],\n",
            "        [-1.3968,  1.6749],\n",
            "        [ 0.8724, -0.8890]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5445, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1116,  1.6161],\n",
            "        [-1.6105,  1.3601],\n",
            "        [ 0.2211, -0.7131],\n",
            "        [ 0.2559, -0.8037],\n",
            "        [-1.4237,  1.4911],\n",
            "        [-1.4614,  1.4844],\n",
            "        [ 0.6464, -1.1272],\n",
            "        [ 0.5770, -1.3366],\n",
            "        [ 0.9065, -1.0148],\n",
            "        [ 1.1811, -1.2717],\n",
            "        [ 1.0392, -1.0479],\n",
            "        [ 0.9937, -1.2687],\n",
            "        [ 1.0203, -1.2890],\n",
            "        [-1.2465,  1.4485],\n",
            "        [ 0.1124, -0.1018],\n",
            "        [ 0.6661, -1.2096],\n",
            "        [ 0.1281, -0.2472],\n",
            "        [-1.5483,  1.4826],\n",
            "        [-0.8582,  1.0326],\n",
            "        [ 0.8929, -1.4038],\n",
            "        [-1.5743,  1.5097],\n",
            "        [ 0.8485, -0.9152],\n",
            "        [ 0.3194,  0.0110],\n",
            "        [ 0.7561, -0.9737],\n",
            "        [ 0.1429, -0.1001],\n",
            "        [ 0.9765, -1.0506],\n",
            "        [ 0.7993, -0.8489],\n",
            "        [ 0.9672, -1.0109],\n",
            "        [ 0.8128, -0.9329],\n",
            "        [ 0.2488, -0.2937],\n",
            "        [-1.4525,  1.6061],\n",
            "        [ 0.4476, -0.8060]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4460,  0.2948],\n",
            "        [-1.5105,  1.6440],\n",
            "        [ 0.5573, -0.7635],\n",
            "        [-1.3601,  1.5777],\n",
            "        [ 1.0411, -1.0384],\n",
            "        [ 0.6203, -0.6056],\n",
            "        [ 1.1175, -1.0131],\n",
            "        [ 0.9728, -1.2370],\n",
            "        [ 0.5756, -1.0882],\n",
            "        [-0.0552, -0.2165],\n",
            "        [-0.8275,  0.9136],\n",
            "        [ 0.5827, -0.8232],\n",
            "        [ 0.9185, -1.1088],\n",
            "        [ 0.6248, -1.0266],\n",
            "        [ 0.9393, -1.1663],\n",
            "        [ 1.1044, -1.1046],\n",
            "        [-1.2932,  1.3126],\n",
            "        [ 0.2388, -0.6042],\n",
            "        [ 0.9082, -0.9412],\n",
            "        [-1.4383,  1.3989],\n",
            "        [-1.1975,  1.0791],\n",
            "        [ 0.8307, -0.9638],\n",
            "        [-0.0894,  0.1862],\n",
            "        [ 0.9187, -1.1148],\n",
            "        [ 0.7559, -1.2035],\n",
            "        [ 0.9814, -1.2362],\n",
            "        [-1.4409,  1.5483],\n",
            "        [ 0.9562, -1.1582],\n",
            "        [ 0.7798, -1.3430],\n",
            "        [ 0.6737, -1.1148],\n",
            "        [ 0.2327, -0.5138],\n",
            "        [-1.5226,  1.5358]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4849, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3081e+00,  1.6447e+00],\n",
            "        [ 9.7662e-01, -1.2841e+00],\n",
            "        [ 6.6069e-01, -8.0624e-01],\n",
            "        [-6.8743e-01,  9.6497e-01],\n",
            "        [ 7.1282e-01, -7.5724e-01],\n",
            "        [-1.3735e+00,  1.4443e+00],\n",
            "        [ 2.7654e-01, -5.4829e-01],\n",
            "        [ 5.0964e-01, -9.7301e-01],\n",
            "        [-2.1393e-01, -3.3408e-02],\n",
            "        [ 1.0392e+00, -1.2298e+00],\n",
            "        [ 9.5593e-01, -1.1200e+00],\n",
            "        [-1.1920e+00,  1.3490e+00],\n",
            "        [-1.3521e+00,  1.4731e+00],\n",
            "        [ 1.0755e+00, -9.4280e-01],\n",
            "        [ 1.0326e+00, -1.1712e+00],\n",
            "        [ 5.3802e-01, -4.4968e-01],\n",
            "        [ 1.3392e-01, -4.3419e-01],\n",
            "        [ 1.2163e+00, -1.0953e+00],\n",
            "        [ 8.9845e-01, -1.2098e+00],\n",
            "        [-2.5384e-01,  7.2624e-02],\n",
            "        [ 2.9408e-01, -6.5456e-01],\n",
            "        [-2.2164e-01,  2.0675e-02],\n",
            "        [-4.6253e-01, -1.1459e-01],\n",
            "        [-2.9257e-01,  2.8990e-01],\n",
            "        [ 9.7921e-01, -1.1145e+00],\n",
            "        [ 6.4283e-01, -1.0273e+00],\n",
            "        [ 1.4365e-03,  1.0752e-01],\n",
            "        [ 9.0571e-01, -1.0170e+00],\n",
            "        [ 7.7976e-01, -1.1461e+00],\n",
            "        [-1.5670e+00,  1.5053e+00],\n",
            "        [ 1.1145e+00, -1.0738e+00],\n",
            "        [-3.8740e-01,  4.3635e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1509, -0.2174],\n",
            "        [-1.6505,  1.4397],\n",
            "        [ 0.7555, -0.5894],\n",
            "        [-1.4216,  1.5116],\n",
            "        [ 0.9867, -1.0532],\n",
            "        [ 0.5170, -0.8935],\n",
            "        [-1.2356,  1.6375],\n",
            "        [ 0.8794, -0.6498],\n",
            "        [-1.6363,  1.4690],\n",
            "        [ 1.0359, -1.2402],\n",
            "        [ 0.8443, -1.1617],\n",
            "        [ 0.7845, -1.0749],\n",
            "        [ 0.7390, -1.3387],\n",
            "        [ 0.4003, -0.9488],\n",
            "        [ 0.6108, -1.2130],\n",
            "        [-0.5853,  0.3778],\n",
            "        [ 0.9885, -0.9767],\n",
            "        [-1.3222,  1.6490],\n",
            "        [ 0.7878, -0.7705],\n",
            "        [-1.5311,  1.7199],\n",
            "        [ 0.9321, -1.0372],\n",
            "        [-0.7266,  0.4665],\n",
            "        [ 0.2725, -0.1259],\n",
            "        [ 0.6730, -1.2776],\n",
            "        [ 0.7695, -0.9264],\n",
            "        [ 1.0307, -1.2567],\n",
            "        [-1.3490,  1.5359],\n",
            "        [-0.9649,  1.1926],\n",
            "        [ 0.8000, -0.9992],\n",
            "        [ 0.6635, -1.0142],\n",
            "        [-0.1199,  0.3027],\n",
            "        [-1.2749,  1.2427]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9758, -1.2008],\n",
            "        [ 0.0877, -1.0126],\n",
            "        [ 0.7534, -0.8541],\n",
            "        [ 0.1579, -0.3343],\n",
            "        [-1.4803,  1.3238],\n",
            "        [ 0.8894, -1.0335],\n",
            "        [ 0.7393, -1.1891],\n",
            "        [-0.1875, -0.0664],\n",
            "        [ 1.0158, -1.2244],\n",
            "        [ 1.0392, -1.0467],\n",
            "        [ 0.1046, -0.5443],\n",
            "        [ 0.8795, -1.2362],\n",
            "        [ 0.7979, -1.1638],\n",
            "        [-0.2573,  0.1031],\n",
            "        [-1.2803,  1.1672],\n",
            "        [-0.6165,  0.6360],\n",
            "        [-1.1810,  1.5369],\n",
            "        [-0.7327,  0.5618],\n",
            "        [-0.5545,  0.6739],\n",
            "        [ 1.1453, -0.9197],\n",
            "        [ 0.8643, -1.3736],\n",
            "        [-1.0504,  1.2058],\n",
            "        [-1.1896,  1.4918],\n",
            "        [ 0.8117, -1.1041],\n",
            "        [ 0.4175, -0.3299],\n",
            "        [ 0.8725, -1.0129],\n",
            "        [ 0.8348, -1.1846],\n",
            "        [ 0.9119, -1.0344],\n",
            "        [ 0.1587, -0.5597],\n",
            "        [-1.6401,  1.5592],\n",
            "        [ 0.6838, -1.0799],\n",
            "        [ 0.9750, -0.8742]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9760, -1.1419],\n",
            "        [ 0.9161, -1.1579],\n",
            "        [ 0.7660, -1.3152],\n",
            "        [-1.2871,  1.3882],\n",
            "        [-1.2220,  1.3401],\n",
            "        [ 0.7665, -1.0748],\n",
            "        [ 0.9118, -0.9650],\n",
            "        [-1.3290,  1.3384],\n",
            "        [ 0.9598, -1.2051],\n",
            "        [ 0.8205, -0.7567],\n",
            "        [ 1.1637, -1.3358],\n",
            "        [ 0.1800, -0.5736],\n",
            "        [ 0.8399, -0.8862],\n",
            "        [ 0.4700, -0.6277],\n",
            "        [ 0.3486, -0.7933],\n",
            "        [ 1.0598, -0.8502],\n",
            "        [-0.3885,  0.5994],\n",
            "        [ 0.8546, -1.0744],\n",
            "        [ 1.1136, -1.1158],\n",
            "        [-1.2866,  1.4794],\n",
            "        [ 0.6415, -0.8329],\n",
            "        [ 0.5753, -0.7732],\n",
            "        [ 0.7267, -0.9628],\n",
            "        [ 0.9492, -1.0731],\n",
            "        [ 0.3915, -0.5009],\n",
            "        [-0.8821,  0.6937],\n",
            "        [ 0.8377, -1.1999],\n",
            "        [ 0.4795, -0.7174],\n",
            "        [ 0.8638, -1.2022],\n",
            "        [ 1.3065, -1.2698],\n",
            "        [ 0.9573, -0.9408],\n",
            "        [ 1.2083, -1.2515]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7445, -1.0538],\n",
            "        [ 0.7353, -1.0107],\n",
            "        [ 0.7872, -1.3896],\n",
            "        [ 0.9123, -1.1328],\n",
            "        [ 0.4214, -0.4396],\n",
            "        [ 0.8734, -1.0006],\n",
            "        [-0.3790,  0.4471],\n",
            "        [ 1.0042, -1.0737],\n",
            "        [ 0.8633, -1.0772],\n",
            "        [-1.3312,  1.4393],\n",
            "        [ 0.6456, -0.6425],\n",
            "        [-1.1327,  1.2735],\n",
            "        [ 0.5717, -1.1800],\n",
            "        [ 0.8827, -0.9190],\n",
            "        [ 0.9518, -1.2039],\n",
            "        [ 0.5069, -1.1002],\n",
            "        [ 0.9534, -1.1345],\n",
            "        [-1.0222,  1.3315],\n",
            "        [-1.3761,  1.4698],\n",
            "        [-1.4790,  1.1734],\n",
            "        [-1.3259,  1.5156],\n",
            "        [-1.5664,  1.7177],\n",
            "        [ 0.8922, -1.2214],\n",
            "        [ 0.5965, -0.6677],\n",
            "        [-1.1335,  1.1678],\n",
            "        [-0.9427,  0.8095],\n",
            "        [ 0.4406, -0.7608],\n",
            "        [-0.7804,  0.8972],\n",
            "        [-0.4544,  0.2846],\n",
            "        [-1.6553,  1.6077],\n",
            "        [ 0.7645, -0.9726],\n",
            "        [ 1.1200, -1.2305]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4071, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3309,  1.5195],\n",
            "        [-1.3110,  1.4796],\n",
            "        [ 1.1759, -1.1417],\n",
            "        [ 0.9591, -0.9448],\n",
            "        [-1.4362,  1.4495],\n",
            "        [ 0.8242, -1.0522],\n",
            "        [ 0.7042, -0.7827],\n",
            "        [ 0.8757, -1.1346],\n",
            "        [ 0.6365, -1.1594],\n",
            "        [ 1.1482, -1.3515],\n",
            "        [ 0.2763, -0.4342],\n",
            "        [ 0.9847, -1.4833],\n",
            "        [ 1.0907, -1.2253],\n",
            "        [-1.6440,  1.4298],\n",
            "        [-0.1080,  0.1790],\n",
            "        [ 0.0516, -0.0676],\n",
            "        [ 0.8323, -1.3642],\n",
            "        [-1.5846,  1.4990],\n",
            "        [ 0.9455, -1.2338],\n",
            "        [-1.5651,  1.6206],\n",
            "        [-0.0494, -0.0836],\n",
            "        [ 0.3045, -0.6063],\n",
            "        [ 0.9292, -1.1961],\n",
            "        [ 0.5611, -0.8427],\n",
            "        [ 0.9729, -1.0676],\n",
            "        [ 0.5801, -0.9775],\n",
            "        [ 0.9881, -1.0839],\n",
            "        [-0.6549,  0.8938],\n",
            "        [ 0.9566, -1.1847],\n",
            "        [-0.1261,  0.1705],\n",
            "        [ 0.7172, -1.2738],\n",
            "        [ 0.7080, -1.0539]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5272,  1.3603],\n",
            "        [ 0.8600, -1.0979],\n",
            "        [-1.4145,  1.5941],\n",
            "        [ 0.8207, -1.1832],\n",
            "        [ 0.9603, -0.9907],\n",
            "        [ 0.7905, -0.8082],\n",
            "        [ 0.9815, -1.0977],\n",
            "        [ 0.7641, -1.3031],\n",
            "        [-1.2095,  1.2089],\n",
            "        [-1.4197,  1.6045],\n",
            "        [-1.5984,  1.5693],\n",
            "        [ 0.8336, -1.2216],\n",
            "        [ 0.9029, -0.6361],\n",
            "        [-1.2017,  1.3113],\n",
            "        [ 0.3375, -0.6156],\n",
            "        [-1.3405,  1.3397],\n",
            "        [ 0.9295, -1.0698],\n",
            "        [-0.6508,  0.5835],\n",
            "        [ 0.8519, -0.9676],\n",
            "        [ 1.0066, -1.4532],\n",
            "        [ 1.0309, -0.9597],\n",
            "        [ 0.4099, -0.6402],\n",
            "        [ 0.2937, -0.3697],\n",
            "        [-1.3880,  1.4046],\n",
            "        [-0.2713,  0.0867],\n",
            "        [ 1.0697, -0.7892],\n",
            "        [-0.2063,  0.4174],\n",
            "        [-0.5841,  0.3327],\n",
            "        [ 0.3306, -0.7688],\n",
            "        [ 0.6683, -1.0188],\n",
            "        [-1.1871,  1.1607],\n",
            "        [-0.9333,  0.6827]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9026, -1.4448],\n",
            "        [-0.6444,  0.7849],\n",
            "        [ 0.9836, -0.9617],\n",
            "        [ 0.2079, -0.2585],\n",
            "        [-0.1331,  0.1792],\n",
            "        [ 0.4164, -0.5447],\n",
            "        [-0.9974,  1.1855],\n",
            "        [ 0.8726, -0.6742],\n",
            "        [-1.6681,  1.4333],\n",
            "        [-1.2927,  1.4549],\n",
            "        [ 0.9035, -1.3155],\n",
            "        [-0.7333,  0.5166],\n",
            "        [ 0.9581, -1.4298],\n",
            "        [ 0.9872, -1.2792],\n",
            "        [ 1.1330, -1.0278],\n",
            "        [-1.5862,  1.5062],\n",
            "        [ 0.9968, -1.3955],\n",
            "        [ 0.7439, -0.9010],\n",
            "        [ 1.0501, -1.0843],\n",
            "        [ 0.6696, -0.6757],\n",
            "        [ 0.8721, -1.1909],\n",
            "        [ 0.9642, -1.2026],\n",
            "        [-1.5263,  1.5006],\n",
            "        [ 1.2568, -1.4700],\n",
            "        [ 1.1141, -1.0455],\n",
            "        [-1.4291,  1.3902],\n",
            "        [ 0.7926, -0.9920],\n",
            "        [ 0.6178, -1.1571],\n",
            "        [ 0.7211, -0.9911],\n",
            "        [ 0.7049, -1.0238],\n",
            "        [ 0.6787, -0.9495],\n",
            "        [ 0.6946, -1.1788]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1896,  1.4888],\n",
            "        [ 1.0796, -1.1998],\n",
            "        [ 0.5108, -0.8302],\n",
            "        [ 1.0842, -1.4092],\n",
            "        [-0.9670,  1.2306],\n",
            "        [ 1.0413, -1.2954],\n",
            "        [-0.0064, -0.3014],\n",
            "        [-0.0890, -0.1016],\n",
            "        [-1.6031,  1.7480],\n",
            "        [-0.6823,  0.9453],\n",
            "        [ 0.8676, -1.0811],\n",
            "        [-0.9985,  1.1056],\n",
            "        [ 0.8178, -1.0081],\n",
            "        [ 0.8695, -0.9527],\n",
            "        [ 0.9146, -1.0404],\n",
            "        [ 0.4559, -0.8291],\n",
            "        [ 0.9824, -1.1394],\n",
            "        [ 0.1068, -0.5053],\n",
            "        [-1.4487,  1.1783],\n",
            "        [ 0.9975, -1.4056],\n",
            "        [ 0.1934, -0.6089],\n",
            "        [ 0.9171, -1.0916],\n",
            "        [ 0.3461, -0.5011],\n",
            "        [ 0.4694, -0.9056],\n",
            "        [ 0.9260, -1.0532],\n",
            "        [ 1.1190, -1.2828],\n",
            "        [ 1.1776, -1.1786],\n",
            "        [ 1.2270, -1.3291],\n",
            "        [ 0.6894, -0.9158],\n",
            "        [-0.8217,  0.8689],\n",
            "        [ 0.6178, -0.8095],\n",
            "        [ 0.8311, -0.9783]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4098, -0.8575],\n",
            "        [ 0.1787, -0.3287],\n",
            "        [ 0.4423, -1.0104],\n",
            "        [ 0.5653, -1.0498],\n",
            "        [-0.6694,  0.3983],\n",
            "        [ 0.8875, -1.1712],\n",
            "        [ 0.9873, -1.1355],\n",
            "        [-0.1048, -0.1165],\n",
            "        [ 0.4987, -1.1434],\n",
            "        [ 1.1054, -1.3132],\n",
            "        [ 0.5914, -0.8983],\n",
            "        [ 0.7160, -1.1726],\n",
            "        [-1.4587,  1.6622],\n",
            "        [-0.6193,  0.3232],\n",
            "        [-1.4251,  1.5948],\n",
            "        [ 0.8464, -0.7318],\n",
            "        [ 1.0483, -1.1162],\n",
            "        [ 0.7301, -1.0095],\n",
            "        [-0.4358,  0.5430],\n",
            "        [ 0.6226, -0.7786],\n",
            "        [-1.1766,  1.0568],\n",
            "        [-1.5232,  1.5780],\n",
            "        [ 0.8781, -0.9281],\n",
            "        [ 0.8909, -1.0726],\n",
            "        [-1.4344,  1.2912],\n",
            "        [ 0.8971, -0.9657],\n",
            "        [-1.7478,  1.6227],\n",
            "        [ 0.9493, -0.9664],\n",
            "        [ 0.2434, -0.5374],\n",
            "        [-0.4489,  0.7029],\n",
            "        [-1.1201,  1.0660],\n",
            "        [ 0.8702, -1.2704]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5382,  1.6612],\n",
            "        [ 0.0390, -0.2439],\n",
            "        [-1.4590,  1.7084],\n",
            "        [-0.8548,  1.1579],\n",
            "        [ 0.9369, -1.3041],\n",
            "        [ 0.4269, -0.8083],\n",
            "        [ 0.2756, -0.5336],\n",
            "        [-0.0259, -0.1904],\n",
            "        [ 0.7646, -1.3068],\n",
            "        [-0.4442,  0.3242],\n",
            "        [ 0.8571, -1.1918],\n",
            "        [ 0.2705, -0.2032],\n",
            "        [ 0.9109, -0.9389],\n",
            "        [ 0.5890, -1.0822],\n",
            "        [ 0.8824, -1.2287],\n",
            "        [ 0.9847, -1.1754],\n",
            "        [ 0.8302, -1.4362],\n",
            "        [ 0.4233, -0.8849],\n",
            "        [-0.2799,  0.4873],\n",
            "        [ 0.6284, -0.6181],\n",
            "        [-0.7038,  1.0991],\n",
            "        [-1.5464,  1.4648],\n",
            "        [-1.3512,  1.3486],\n",
            "        [ 1.0441, -1.2392],\n",
            "        [ 1.0887, -1.1997],\n",
            "        [-1.4097,  1.5658],\n",
            "        [-1.2726,  1.3472],\n",
            "        [-1.1140,  1.3626],\n",
            "        [ 0.3780, -0.5018],\n",
            "        [-1.2325,  1.3451],\n",
            "        [-1.3842,  1.6591],\n",
            "        [ 0.4042, -0.6474]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4049,  1.5015],\n",
            "        [ 0.7070, -0.6795],\n",
            "        [ 0.0029,  0.0996],\n",
            "        [ 0.9518, -1.2574],\n",
            "        [-0.4952,  0.4591],\n",
            "        [ 0.7295, -0.8924],\n",
            "        [-1.5251,  1.6151],\n",
            "        [ 0.7618, -1.0344],\n",
            "        [-1.3196,  1.2622],\n",
            "        [ 0.2834, -0.6075],\n",
            "        [ 0.6842, -1.1246],\n",
            "        [-1.3809,  1.5024],\n",
            "        [ 0.7482, -1.1847],\n",
            "        [ 0.7878, -1.1459],\n",
            "        [ 0.7474, -1.0216],\n",
            "        [-0.4132,  0.2786],\n",
            "        [-0.0714,  0.2630],\n",
            "        [ 0.8972, -1.0294],\n",
            "        [ 0.9610, -1.3321],\n",
            "        [-1.2615,  1.2346],\n",
            "        [ 0.5294, -0.8637],\n",
            "        [-0.2471,  0.7425],\n",
            "        [-1.4799,  1.3422],\n",
            "        [ 0.2029, -0.3098],\n",
            "        [ 0.7895, -0.9809],\n",
            "        [-1.7829,  1.7348],\n",
            "        [ 1.0657, -1.0935],\n",
            "        [ 0.8661, -0.9384],\n",
            "        [ 0.7869, -1.2125],\n",
            "        [-1.4077,  1.3089],\n",
            "        [ 0.9762, -1.3062],\n",
            "        [ 0.4857, -0.8995]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.6100,  1.4568],\n",
            "        [ 1.1091, -1.0164],\n",
            "        [ 0.6056, -0.9272],\n",
            "        [ 0.2306, -0.6397],\n",
            "        [ 0.9855, -1.1111],\n",
            "        [-0.3213,  0.0781],\n",
            "        [-0.9951,  1.0870],\n",
            "        [-1.6082,  1.6513],\n",
            "        [-1.2709,  1.2740],\n",
            "        [-1.5561,  1.6319],\n",
            "        [-0.8961,  1.0806],\n",
            "        [ 0.7733, -0.8647],\n",
            "        [ 0.3385, -0.3786],\n",
            "        [-1.4858,  1.4954],\n",
            "        [ 1.0488, -1.1748],\n",
            "        [ 0.0322,  0.0717],\n",
            "        [-1.1651,  1.2271],\n",
            "        [ 0.7930, -1.1138],\n",
            "        [ 1.1797, -1.2304],\n",
            "        [ 0.5488, -1.1016],\n",
            "        [-1.1829,  1.1509],\n",
            "        [ 1.0120, -1.0367],\n",
            "        [ 0.5968, -0.9530],\n",
            "        [ 0.8215, -0.8298],\n",
            "        [ 0.6698, -0.5792],\n",
            "        [ 0.4558, -0.6981],\n",
            "        [ 0.5998, -1.1985],\n",
            "        [-1.6306,  1.6910],\n",
            "        [ 0.7823, -1.2563],\n",
            "        [-1.1600,  1.5915],\n",
            "        [-0.6920,  0.5172],\n",
            "        [ 0.6460, -0.9729]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7052, -1.3337],\n",
            "        [ 0.9847, -1.2441],\n",
            "        [ 1.0711, -1.0930],\n",
            "        [ 0.2583, -0.6125],\n",
            "        [ 0.7432, -0.9696],\n",
            "        [ 1.0325, -1.0318],\n",
            "        [ 0.9985, -1.3801],\n",
            "        [-1.0185,  0.6675],\n",
            "        [-1.3746,  0.9993],\n",
            "        [ 0.9879, -1.0779],\n",
            "        [ 0.7317, -1.1080],\n",
            "        [ 0.8128, -1.0019],\n",
            "        [-0.1864,  0.2417],\n",
            "        [ 0.2193, -0.3294],\n",
            "        [ 1.0068, -1.1588],\n",
            "        [-1.3843,  1.5418],\n",
            "        [-1.3166,  1.2422],\n",
            "        [ 1.0346, -0.8131],\n",
            "        [-0.4821,  0.2574],\n",
            "        [ 0.9388, -1.1076],\n",
            "        [-1.5674,  1.4920],\n",
            "        [ 0.0592, -0.2127],\n",
            "        [-1.0493,  1.0953],\n",
            "        [ 0.3112, -0.6633],\n",
            "        [ 1.0986, -1.0609],\n",
            "        [ 0.9823, -1.0697],\n",
            "        [ 0.7034, -1.0295],\n",
            "        [ 0.2798, -0.7604],\n",
            "        [ 0.7253, -1.0625],\n",
            "        [ 0.8086, -0.7503],\n",
            "        [-1.4072,  1.6196],\n",
            "        [-0.6828,  0.6924]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3758, -0.4774],\n",
            "        [ 0.8129, -0.9357],\n",
            "        [ 1.0348, -1.2525],\n",
            "        [-0.0548, -0.1150],\n",
            "        [ 0.4281, -0.9959],\n",
            "        [ 0.7022, -1.1012],\n",
            "        [-0.3504,  0.3874],\n",
            "        [ 0.5666, -0.9506],\n",
            "        [-0.9791,  1.1546],\n",
            "        [ 0.1127, -0.4394],\n",
            "        [ 1.3412, -1.2661],\n",
            "        [-0.8066,  1.1610],\n",
            "        [ 0.5578, -0.8042],\n",
            "        [-1.7582,  1.7049],\n",
            "        [-1.1000,  1.4442],\n",
            "        [ 1.0553, -1.0756],\n",
            "        [ 0.5325, -0.8067],\n",
            "        [-1.3076,  1.4753],\n",
            "        [-1.7208,  1.6151],\n",
            "        [-1.2824,  1.4819],\n",
            "        [ 0.8328, -1.1451],\n",
            "        [ 0.9539, -1.1737],\n",
            "        [ 1.0259, -1.1156],\n",
            "        [-0.7475,  0.5578],\n",
            "        [-0.1349, -0.0370],\n",
            "        [-0.5458,  0.6377],\n",
            "        [ 1.1508, -1.2990],\n",
            "        [ 0.6482, -0.9892],\n",
            "        [-1.0345,  1.1663],\n",
            "        [-1.2520,  1.3709],\n",
            "        [-1.3371,  1.3458],\n",
            "        [ 0.7943, -1.1465]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4426,  1.4650],\n",
            "        [-1.5193,  1.5619],\n",
            "        [ 0.2515, -0.6223],\n",
            "        [ 0.8788, -1.1900],\n",
            "        [-0.7112,  1.1022],\n",
            "        [-1.2201,  1.3635],\n",
            "        [-1.2369,  1.4277],\n",
            "        [ 0.1288, -0.4991],\n",
            "        [ 0.9601, -1.2334],\n",
            "        [ 0.9990, -1.1406],\n",
            "        [ 0.6442, -1.0258],\n",
            "        [ 0.4549, -0.7937],\n",
            "        [-0.1888, -0.0987],\n",
            "        [ 1.0403, -1.1392],\n",
            "        [-1.6816,  1.5550],\n",
            "        [-0.0882, -0.4126],\n",
            "        [ 0.1356, -0.1426],\n",
            "        [ 0.0830, -0.3224],\n",
            "        [ 0.5865, -0.7507],\n",
            "        [-1.3255,  1.2931],\n",
            "        [ 0.5742, -0.8264],\n",
            "        [ 0.6907, -0.7205],\n",
            "        [-1.5245,  1.5529],\n",
            "        [ 0.9821, -1.1657],\n",
            "        [ 1.1600, -1.1385],\n",
            "        [ 1.0001, -1.1594],\n",
            "        [-1.5719,  1.6744],\n",
            "        [-0.4401,  0.5705],\n",
            "        [ 0.6811, -0.9841],\n",
            "        [ 1.1192, -1.3406],\n",
            "        [-1.2989,  1.5081],\n",
            "        [ 0.6966, -1.1676]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5659, -1.0746],\n",
            "        [-0.8677,  1.2602],\n",
            "        [ 0.3335, -0.6351],\n",
            "        [-1.2834,  1.2597],\n",
            "        [ 0.8955, -1.1358],\n",
            "        [ 0.9285, -0.9578],\n",
            "        [ 0.5359, -0.9842],\n",
            "        [-0.3487,  0.1040],\n",
            "        [-1.0701,  1.1321],\n",
            "        [ 0.3703, -0.5930],\n",
            "        [ 0.3508, -0.6698],\n",
            "        [ 0.9138, -1.0524],\n",
            "        [ 0.3853, -0.6816],\n",
            "        [ 0.7643, -1.0742],\n",
            "        [ 0.5865, -0.6892],\n",
            "        [ 1.0491, -1.1333],\n",
            "        [-0.8440,  0.8938],\n",
            "        [-0.4356,  0.3652],\n",
            "        [-0.6494,  0.5399],\n",
            "        [ 1.1834, -1.3061],\n",
            "        [ 0.5066, -1.0514],\n",
            "        [ 0.8912, -1.1879],\n",
            "        [-0.9520,  1.2147],\n",
            "        [-1.5100,  1.5415],\n",
            "        [-1.1551,  0.9093],\n",
            "        [ 0.3046, -0.8257],\n",
            "        [-0.8097,  0.8487],\n",
            "        [ 0.7537, -0.9926],\n",
            "        [-0.8309,  0.8562],\n",
            "        [ 0.7839, -0.8936],\n",
            "        [ 1.1351, -1.3133],\n",
            "        [ 0.4481, -0.8265]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1326, -1.2002],\n",
            "        [ 0.9875, -1.0937],\n",
            "        [ 0.4259, -0.2454],\n",
            "        [ 0.8792, -1.0045],\n",
            "        [ 0.7507, -0.9715],\n",
            "        [ 0.7426, -1.2039],\n",
            "        [-1.5226,  1.6669],\n",
            "        [-1.0365,  1.0888],\n",
            "        [ 1.0607, -1.1106],\n",
            "        [-1.5158,  1.4383],\n",
            "        [-1.1391,  1.1467],\n",
            "        [-1.3145,  1.5045],\n",
            "        [ 1.0322, -1.2092],\n",
            "        [-1.3797,  1.4350],\n",
            "        [-1.0755,  1.2150],\n",
            "        [-0.4380,  0.3568],\n",
            "        [ 0.6750, -1.3487],\n",
            "        [ 1.0022, -1.1043],\n",
            "        [-0.4604,  0.5746],\n",
            "        [-0.8909,  0.7088],\n",
            "        [ 0.7638, -0.8139],\n",
            "        [-1.3913,  1.5792],\n",
            "        [-1.4977,  1.6875],\n",
            "        [ 1.0122, -1.3436],\n",
            "        [ 0.3372, -0.6517],\n",
            "        [-0.0457, -0.0027],\n",
            "        [-1.1270,  1.1507],\n",
            "        [-1.2528,  1.4764],\n",
            "        [ 0.7967, -1.1388],\n",
            "        [ 0.5652, -1.0462],\n",
            "        [ 0.2393, -0.1632],\n",
            "        [ 0.8656, -1.0651]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5512,  1.3917],\n",
            "        [-1.3279,  1.5518],\n",
            "        [ 0.7642, -1.1281],\n",
            "        [-1.6879,  1.4033],\n",
            "        [ 1.1179, -1.2362],\n",
            "        [-0.7637,  0.8595],\n",
            "        [ 1.0044, -1.0244],\n",
            "        [-1.3812,  1.3115],\n",
            "        [ 0.4635, -0.6719],\n",
            "        [ 0.8197, -0.8619],\n",
            "        [ 1.1372, -1.0939],\n",
            "        [-0.9918,  0.9662],\n",
            "        [-1.0850,  0.8635],\n",
            "        [-1.6984,  1.5931],\n",
            "        [-1.6750,  1.6791],\n",
            "        [ 0.9589, -1.0035],\n",
            "        [-1.3721,  1.4231],\n",
            "        [ 0.8795, -1.1645],\n",
            "        [ 0.5095, -0.7508],\n",
            "        [-1.2301,  1.3295],\n",
            "        [ 0.4003, -0.5784],\n",
            "        [ 1.1174, -1.3682],\n",
            "        [-1.6401,  1.4280],\n",
            "        [-1.1386,  1.2778],\n",
            "        [ 0.9271, -1.1705],\n",
            "        [ 0.7996, -1.1141],\n",
            "        [ 1.0100, -0.9552],\n",
            "        [ 0.7615, -1.1643],\n",
            "        [ 0.9720, -1.0578],\n",
            "        [ 0.9002, -1.0715],\n",
            "        [-1.3949,  1.5837],\n",
            "        [ 0.8033, -1.0703]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6114,  1.0917],\n",
            "        [-0.9091,  1.0638],\n",
            "        [-1.2321,  1.3442],\n",
            "        [ 0.7223, -1.0113],\n",
            "        [ 0.9202, -1.3696],\n",
            "        [ 0.7195, -1.0752],\n",
            "        [-1.4955,  1.6760],\n",
            "        [ 0.9149, -1.0645],\n",
            "        [-0.0238,  0.0621],\n",
            "        [ 0.6445, -0.6888],\n",
            "        [-1.3309,  1.6964],\n",
            "        [ 0.9146, -0.8547],\n",
            "        [ 0.7342, -0.9654],\n",
            "        [ 0.3199, -0.3473],\n",
            "        [ 0.6135, -0.7430],\n",
            "        [ 0.2152, -0.1852],\n",
            "        [-0.9851,  1.6042],\n",
            "        [ 0.8698, -1.4258],\n",
            "        [ 0.7552, -0.9086],\n",
            "        [-1.3970,  1.2620],\n",
            "        [-1.6248,  1.4463],\n",
            "        [ 0.8979, -1.2600],\n",
            "        [-0.7516,  0.4947],\n",
            "        [-0.9960,  0.9782],\n",
            "        [-1.4502,  1.6830],\n",
            "        [ 0.2190, -0.6289],\n",
            "        [ 0.2478, -0.8834],\n",
            "        [-1.5737,  1.7061],\n",
            "        [-0.8516,  1.2048],\n",
            "        [ 0.3706, -0.6151],\n",
            "        [-1.1166,  1.5541],\n",
            "        [-0.7216,  0.7714]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4308, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6325, -0.8142],\n",
            "        [-1.5236,  1.6334],\n",
            "        [ 0.9488, -0.8139],\n",
            "        [ 0.4720, -0.7619],\n",
            "        [ 0.2279, -0.4553],\n",
            "        [-1.3710,  1.4840],\n",
            "        [ 0.7009, -0.6204],\n",
            "        [-0.4576,  0.1554],\n",
            "        [ 0.3694, -0.5687],\n",
            "        [ 0.0796, -0.2778],\n",
            "        [-1.4325,  1.1730],\n",
            "        [-1.3823,  1.5418],\n",
            "        [ 0.7944, -1.2440],\n",
            "        [ 0.9231, -0.8622],\n",
            "        [ 1.1895, -1.3689],\n",
            "        [ 0.3519, -0.9751],\n",
            "        [ 0.2626, -0.5904],\n",
            "        [ 0.4779, -0.9179],\n",
            "        [-1.2648,  1.2792],\n",
            "        [-1.5699,  1.6521],\n",
            "        [ 0.8533, -0.9961],\n",
            "        [-1.6091,  1.6339],\n",
            "        [ 1.0229, -1.2343],\n",
            "        [ 0.1853, -0.5003],\n",
            "        [ 0.9733, -1.0155],\n",
            "        [ 1.1647, -1.0688],\n",
            "        [ 0.9333, -0.8882],\n",
            "        [-1.5194,  1.4367],\n",
            "        [-0.5100,  0.7830],\n",
            "        [-1.0463,  1.2341],\n",
            "        [ 0.7769, -1.0822],\n",
            "        [-1.4023,  1.3082]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2496,  0.3531],\n",
            "        [-1.5072,  1.6017],\n",
            "        [ 0.9069, -1.1935],\n",
            "        [ 1.1213, -1.1448],\n",
            "        [-1.3099,  1.6865],\n",
            "        [-0.9922,  1.0751],\n",
            "        [-1.5008,  1.3894],\n",
            "        [-0.6982,  0.3406],\n",
            "        [ 1.1000, -1.3737],\n",
            "        [ 0.9127, -1.0027],\n",
            "        [ 0.8876, -1.1483],\n",
            "        [ 0.7514, -0.9872],\n",
            "        [ 1.0047, -1.1597],\n",
            "        [-1.2524,  1.5429],\n",
            "        [ 0.9760, -1.0724],\n",
            "        [-1.2725,  1.4658],\n",
            "        [-1.4696,  1.5400],\n",
            "        [ 0.7710, -0.9825],\n",
            "        [ 0.8933, -0.9411],\n",
            "        [ 0.7177, -0.8171],\n",
            "        [-1.3860,  1.6847],\n",
            "        [-0.5060,  0.3200],\n",
            "        [ 0.9081, -1.1207],\n",
            "        [ 0.3427, -0.7755],\n",
            "        [ 0.9477, -1.3983],\n",
            "        [ 0.9130, -1.1539],\n",
            "        [ 0.9781, -0.9827],\n",
            "        [ 0.9374, -1.4175],\n",
            "        [ 1.0432, -1.0531],\n",
            "        [ 1.0156, -1.0545],\n",
            "        [-0.4730,  0.4100],\n",
            "        [ 1.1351, -1.3260]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3932, -0.7892],\n",
            "        [-1.2488,  1.4988],\n",
            "        [ 0.8236, -1.2677],\n",
            "        [-1.3616,  1.7102],\n",
            "        [ 0.8129, -0.6667],\n",
            "        [ 1.0267, -1.1725],\n",
            "        [ 0.9706, -1.5199],\n",
            "        [-1.3262,  1.5464],\n",
            "        [-1.5388,  1.7783],\n",
            "        [ 1.0581, -1.3543],\n",
            "        [-0.7165,  0.9210],\n",
            "        [ 0.9362, -1.2121],\n",
            "        [ 0.9970, -1.1493],\n",
            "        [-1.3663,  1.7461],\n",
            "        [-1.3553,  1.3713],\n",
            "        [ 0.7422, -1.0578],\n",
            "        [ 0.3920, -0.8091],\n",
            "        [ 1.0241, -1.1202],\n",
            "        [ 0.6852, -1.2492],\n",
            "        [-1.2595,  1.3987],\n",
            "        [ 1.0145, -1.1013],\n",
            "        [ 1.0249, -1.1570],\n",
            "        [ 0.3570, -0.3204],\n",
            "        [-1.7411,  1.8136],\n",
            "        [-0.2867, -0.1430],\n",
            "        [-0.9516,  0.9880],\n",
            "        [-1.5977,  1.3878],\n",
            "        [ 0.8823, -1.2663],\n",
            "        [ 0.4209, -0.4941],\n",
            "        [ 1.0432, -1.1226],\n",
            "        [-1.3002,  1.3473],\n",
            "        [ 1.2327, -1.0861]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8952, -1.1117],\n",
            "        [ 1.0659, -1.2014],\n",
            "        [-0.5388,  0.9273],\n",
            "        [ 0.4033, -0.9770],\n",
            "        [ 0.7029, -1.1734],\n",
            "        [-0.5311,  0.5126],\n",
            "        [ 0.7956, -0.9030],\n",
            "        [-0.4373,  0.1590],\n",
            "        [-1.3687,  1.6493],\n",
            "        [ 0.0506, -0.2594],\n",
            "        [ 0.9732, -1.2796],\n",
            "        [ 1.0771, -1.4474],\n",
            "        [ 0.8838, -1.3620],\n",
            "        [-0.5151,  0.7679],\n",
            "        [-1.6001,  1.5624],\n",
            "        [ 0.2333, -0.7124],\n",
            "        [-1.1353,  1.2068],\n",
            "        [-0.0677,  0.0131],\n",
            "        [ 0.6920, -0.8717],\n",
            "        [-0.2000, -0.0591],\n",
            "        [ 1.0770, -1.2532],\n",
            "        [-0.7812,  0.8097],\n",
            "        [ 0.7687, -1.0990],\n",
            "        [ 0.9747, -1.0406],\n",
            "        [ 0.4350, -0.6323],\n",
            "        [ 0.6582, -0.9854],\n",
            "        [ 0.9347, -1.1174],\n",
            "        [ 0.7414, -0.7017],\n",
            "        [-1.5185,  1.2814],\n",
            "        [-1.2530,  1.0479],\n",
            "        [-0.3813,  0.4674],\n",
            "        [ 0.5452, -0.5469]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1035, -1.3004],\n",
            "        [ 1.0921, -0.9427],\n",
            "        [ 0.8122, -0.8376],\n",
            "        [ 0.7970, -1.1344],\n",
            "        [-1.4371,  1.5232],\n",
            "        [ 1.1895, -1.1572],\n",
            "        [ 0.9411, -1.1824],\n",
            "        [-0.3398,  0.0493],\n",
            "        [ 0.6019, -1.0424],\n",
            "        [-0.4457,  0.2453],\n",
            "        [ 0.3452, -0.2368],\n",
            "        [ 1.0378, -1.3791],\n",
            "        [ 0.9169, -1.0986],\n",
            "        [ 1.2158, -1.4176],\n",
            "        [-1.0180,  1.0041],\n",
            "        [ 1.1041, -1.2540],\n",
            "        [-1.1317,  0.8396],\n",
            "        [-1.0543,  1.1358],\n",
            "        [-1.4953,  1.7276],\n",
            "        [ 0.3273, -0.1001],\n",
            "        [-1.3771,  1.4117],\n",
            "        [ 0.8651, -0.8633],\n",
            "        [ 0.4695, -0.7907],\n",
            "        [-1.4185,  1.5152],\n",
            "        [ 1.2146, -1.2795],\n",
            "        [ 0.6945, -0.8686],\n",
            "        [ 0.9903, -1.1385],\n",
            "        [ 0.8373, -0.9678],\n",
            "        [-1.4636,  1.5441],\n",
            "        [ 0.8410, -1.0267],\n",
            "        [ 0.1815, -0.4457],\n",
            "        [ 1.2546, -1.3129]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9277, -1.2562],\n",
            "        [-1.5500,  1.4995],\n",
            "        [ 0.7040, -1.1428],\n",
            "        [-1.3827,  1.7443],\n",
            "        [ 0.8638, -1.3604],\n",
            "        [-0.9607,  1.3522],\n",
            "        [-1.4805,  1.5385],\n",
            "        [-1.4991,  1.5367],\n",
            "        [-1.1255,  1.2574],\n",
            "        [ 0.9269, -1.0339],\n",
            "        [ 0.9927, -1.0545],\n",
            "        [ 1.0170, -0.9269],\n",
            "        [ 1.2175, -1.2831],\n",
            "        [ 0.6953, -0.9066],\n",
            "        [ 0.9444, -1.1352],\n",
            "        [ 0.5644, -0.6188],\n",
            "        [-0.4807,  0.4354],\n",
            "        [-1.2036,  1.3210],\n",
            "        [ 1.1090, -1.3193],\n",
            "        [ 0.9148, -1.1432],\n",
            "        [ 1.1227, -1.1400],\n",
            "        [ 0.8652, -1.1330],\n",
            "        [-1.2802,  1.0671],\n",
            "        [ 0.7179, -0.9976],\n",
            "        [-1.2310,  1.3099],\n",
            "        [ 0.4134, -0.8144],\n",
            "        [ 0.8958, -0.9726],\n",
            "        [ 0.7421, -0.9986],\n",
            "        [-0.9336,  1.1727],\n",
            "        [ 0.7518, -0.8486],\n",
            "        [-1.5049,  1.7663],\n",
            "        [-0.8958,  1.2390]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3433, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3012, -1.1525],\n",
            "        [-0.0488, -0.2198],\n",
            "        [ 0.8460, -1.2623],\n",
            "        [-1.6098,  1.4842],\n",
            "        [ 0.8892, -0.9062],\n",
            "        [-1.5130,  1.2624],\n",
            "        [-1.6330,  1.7331],\n",
            "        [-0.4835,  0.4403],\n",
            "        [ 0.5922, -0.7959],\n",
            "        [ 0.7429, -0.9124],\n",
            "        [ 0.9279, -0.8188],\n",
            "        [-1.3657,  1.8041],\n",
            "        [ 0.6696, -0.9962],\n",
            "        [-1.4362,  1.6922],\n",
            "        [ 1.0125, -1.1496],\n",
            "        [ 1.2300, -1.2019],\n",
            "        [ 0.4642, -0.4899],\n",
            "        [ 0.6342, -1.1468],\n",
            "        [-0.6056,  0.9121],\n",
            "        [ 1.1074, -1.3367],\n",
            "        [ 0.5894, -1.1462],\n",
            "        [ 0.9818, -1.1804],\n",
            "        [ 0.7731, -0.8823],\n",
            "        [ 1.0083, -1.2414],\n",
            "        [ 0.6502, -1.0840],\n",
            "        [ 0.8591, -1.1138],\n",
            "        [ 0.7055, -0.7503],\n",
            "        [-0.4389,  0.2586],\n",
            "        [-1.1281,  1.1054],\n",
            "        [ 0.1423, -0.4437],\n",
            "        [ 0.7206, -1.0453],\n",
            "        [-1.4373,  1.4526]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2993, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2840,  1.4634],\n",
            "        [ 0.5558, -0.7627],\n",
            "        [-1.6136,  1.6616],\n",
            "        [-1.2583,  1.4117],\n",
            "        [ 0.8971, -1.1547],\n",
            "        [ 1.0924, -1.0019],\n",
            "        [-1.4804,  1.5592],\n",
            "        [ 0.8633, -1.0441],\n",
            "        [ 0.6594, -1.0050],\n",
            "        [-1.5101,  1.5963],\n",
            "        [ 0.9779, -1.2326],\n",
            "        [-1.2797,  1.4428],\n",
            "        [ 0.8048, -1.1274],\n",
            "        [ 1.0032, -1.0152],\n",
            "        [-0.6119,  0.6287],\n",
            "        [ 0.8767, -0.9813],\n",
            "        [ 0.6975, -0.8857],\n",
            "        [-0.6374,  0.4907],\n",
            "        [ 1.0490, -0.9038],\n",
            "        [ 0.9283, -0.9509],\n",
            "        [-0.1700,  0.2068],\n",
            "        [ 1.0263, -1.1554],\n",
            "        [ 0.3898, -0.8733],\n",
            "        [ 0.9007, -1.2249],\n",
            "        [ 1.0126, -1.2547],\n",
            "        [-1.4751,  1.6051],\n",
            "        [-1.3566,  1.4113],\n",
            "        [ 1.0185, -1.3508],\n",
            "        [-0.4004,  0.2068],\n",
            "        [ 0.9442, -1.3095],\n",
            "        [ 0.6137, -1.0923],\n",
            "        [ 0.6355, -1.2759]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1440,  1.4758],\n",
            "        [ 0.5664, -0.6001],\n",
            "        [ 0.2472, -0.5310],\n",
            "        [-1.5676,  1.6867],\n",
            "        [ 1.1238, -1.3245],\n",
            "        [-1.4566,  1.4274],\n",
            "        [ 0.0174, -0.1538],\n",
            "        [-1.4963,  1.4174],\n",
            "        [ 0.6873, -0.7263],\n",
            "        [-1.6963,  1.3912],\n",
            "        [ 0.4990, -0.6798],\n",
            "        [ 0.8885, -1.2693],\n",
            "        [-1.1440,  0.6999],\n",
            "        [ 0.4411, -0.4923],\n",
            "        [ 0.2780, -0.7881],\n",
            "        [ 0.8011, -0.8994],\n",
            "        [-1.4450,  1.6016],\n",
            "        [ 1.0520, -1.0143],\n",
            "        [-0.1948, -0.1404],\n",
            "        [-1.6766,  1.5136],\n",
            "        [ 0.7182, -1.0207],\n",
            "        [ 0.5798, -0.8554],\n",
            "        [ 1.0701, -1.1237],\n",
            "        [-1.5515,  1.5316],\n",
            "        [-0.6762,  0.9973],\n",
            "        [ 0.7493, -1.0377],\n",
            "        [-1.2750,  1.2307],\n",
            "        [-0.8704,  0.9493],\n",
            "        [ 0.8701, -1.1281],\n",
            "        [ 0.6871, -1.1518],\n",
            "        [ 0.2723, -0.7001],\n",
            "        [ 1.0880, -1.1986]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7597,  1.1012],\n",
            "        [ 0.9955, -1.2754],\n",
            "        [ 0.8310, -0.8922],\n",
            "        [-0.5628,  0.6290],\n",
            "        [ 0.8883, -1.2862],\n",
            "        [-0.1097, -0.2240],\n",
            "        [ 0.6935, -1.0565],\n",
            "        [ 0.8823, -1.0097],\n",
            "        [-1.1443,  1.3262],\n",
            "        [ 0.7634, -1.0351],\n",
            "        [-1.4787,  1.6195],\n",
            "        [-1.3232,  1.6538],\n",
            "        [ 0.7235, -1.0048],\n",
            "        [ 0.9853, -1.1004],\n",
            "        [ 1.0137, -1.3119],\n",
            "        [ 0.6747, -1.0324],\n",
            "        [ 0.8941, -1.1431],\n",
            "        [-1.0105,  0.6403],\n",
            "        [ 0.9369, -1.2246],\n",
            "        [ 0.6042, -0.9254],\n",
            "        [ 0.9364, -0.8993],\n",
            "        [-1.5058,  1.8132],\n",
            "        [ 1.1640, -1.1625],\n",
            "        [ 0.4202, -0.4686],\n",
            "        [ 0.2703, -0.4240],\n",
            "        [-1.5543,  1.7138],\n",
            "        [-0.0685, -0.0469],\n",
            "        [-1.4713,  1.5842],\n",
            "        [-0.0338, -0.0842],\n",
            "        [ 0.8326, -1.2157],\n",
            "        [-1.4096,  1.3196],\n",
            "        [ 0.8919, -1.1697]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4633, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5273,  1.4903],\n",
            "        [ 0.8076, -1.0059],\n",
            "        [-0.2085,  0.2399],\n",
            "        [ 1.0937, -1.4906],\n",
            "        [ 0.7936, -0.8720],\n",
            "        [ 1.0789, -1.3404],\n",
            "        [ 0.8912, -1.0555],\n",
            "        [-1.7976,  1.5707],\n",
            "        [ 1.0335, -1.1625],\n",
            "        [-0.2800,  0.1460],\n",
            "        [ 0.1765, -0.7980],\n",
            "        [ 0.8224, -0.9160],\n",
            "        [ 0.9180, -0.9866],\n",
            "        [ 0.6829, -0.9493],\n",
            "        [-0.6547,  0.7913],\n",
            "        [-0.3441,  0.0220],\n",
            "        [-1.4120,  1.6728],\n",
            "        [ 0.6766, -1.0879],\n",
            "        [ 1.1537, -1.1258],\n",
            "        [-1.3399,  1.6834],\n",
            "        [ 1.0196, -1.4065],\n",
            "        [ 0.9420, -1.0563],\n",
            "        [-0.5659,  0.4807],\n",
            "        [-1.4948,  1.5477],\n",
            "        [ 0.8588, -1.1703],\n",
            "        [ 0.7772, -0.9905],\n",
            "        [ 0.8256, -0.8978],\n",
            "        [ 1.0808, -1.0038],\n",
            "        [ 0.0703, -0.0478],\n",
            "        [ 0.7479, -0.8643],\n",
            "        [-1.6549,  1.4621],\n",
            "        [-0.0957, -0.1537]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2838, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9158, -0.9296],\n",
            "        [ 1.2447, -1.1469],\n",
            "        [ 0.0979, -0.8166],\n",
            "        [ 1.0394, -1.2524],\n",
            "        [ 1.1170, -1.0997],\n",
            "        [ 0.6470, -1.1449],\n",
            "        [ 0.9069, -1.3629],\n",
            "        [ 0.9990, -1.2963],\n",
            "        [ 0.0133,  0.2719],\n",
            "        [ 0.8237, -1.2454],\n",
            "        [ 0.9261, -0.9410],\n",
            "        [ 0.5284, -0.7227],\n",
            "        [ 0.8772, -1.1085],\n",
            "        [ 1.1048, -1.2371],\n",
            "        [ 0.0484, -0.5032],\n",
            "        [ 1.0107, -1.0521],\n",
            "        [-1.5301,  1.8312],\n",
            "        [ 0.2172, -0.3894],\n",
            "        [ 1.3125, -1.2021],\n",
            "        [ 0.9641, -0.9974],\n",
            "        [ 1.1891, -1.1066],\n",
            "        [ 0.5987, -0.7513],\n",
            "        [ 0.9703, -1.2507],\n",
            "        [ 1.0544, -1.2787],\n",
            "        [ 0.9267, -1.0822],\n",
            "        [-1.3387,  1.6791],\n",
            "        [ 1.2937, -1.3981],\n",
            "        [-1.5703,  1.5562],\n",
            "        [ 0.8032, -1.0197],\n",
            "        [ 1.0133, -1.0591],\n",
            "        [ 1.1888, -1.3873],\n",
            "        [ 0.8631, -1.2744]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2003,  1.4526],\n",
            "        [-1.6157,  1.6748],\n",
            "        [ 0.8080, -1.1591],\n",
            "        [ 1.1553, -1.2478],\n",
            "        [ 1.1089, -0.9112],\n",
            "        [ 0.9321, -1.1960],\n",
            "        [-1.3392,  1.5000],\n",
            "        [ 0.8671, -0.8610],\n",
            "        [-1.5103,  1.4588],\n",
            "        [-1.5535,  1.5565],\n",
            "        [-1.4249,  1.4958],\n",
            "        [ 0.7577, -0.8202],\n",
            "        [ 0.5713, -0.9665],\n",
            "        [ 1.0803, -1.3761],\n",
            "        [ 0.9092, -1.0735],\n",
            "        [-1.3893,  1.4962],\n",
            "        [ 0.3583, -0.5832],\n",
            "        [ 0.3802, -0.6506],\n",
            "        [ 0.8364, -0.9309],\n",
            "        [ 0.6820, -1.1720],\n",
            "        [ 0.3730, -0.6493],\n",
            "        [ 0.6898, -1.2635],\n",
            "        [-0.8009,  0.8701],\n",
            "        [ 1.3145, -1.4208],\n",
            "        [-1.5437,  1.5600],\n",
            "        [ 0.6279, -0.4878],\n",
            "        [ 0.7059, -0.6958],\n",
            "        [ 0.4773, -0.7742],\n",
            "        [ 0.9145, -1.2402],\n",
            "        [ 1.0078, -1.0299],\n",
            "        [-0.7555,  0.8791],\n",
            "        [ 0.0824,  0.1319]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3277,  0.3181],\n",
            "        [-1.3839,  1.4459],\n",
            "        [ 0.9004, -1.2398],\n",
            "        [ 0.9594, -1.1020],\n",
            "        [ 0.5651, -0.8830],\n",
            "        [ 0.9917, -1.0263],\n",
            "        [ 1.0174, -1.1773],\n",
            "        [-1.4690,  1.5481],\n",
            "        [ 0.8630, -1.1287],\n",
            "        [ 0.0230, -0.1521],\n",
            "        [-1.5003,  1.7344],\n",
            "        [-1.4842,  1.6319],\n",
            "        [ 0.5491, -0.8041],\n",
            "        [-1.4922,  1.5989],\n",
            "        [ 0.9505, -1.0495],\n",
            "        [ 0.9427, -1.1678],\n",
            "        [ 1.0595, -1.3237],\n",
            "        [ 0.4714, -1.0520],\n",
            "        [ 0.7727, -1.0360],\n",
            "        [-0.2839,  0.4378],\n",
            "        [-1.3004,  1.2499],\n",
            "        [-0.2801,  0.4516],\n",
            "        [-1.2900,  1.4402],\n",
            "        [ 0.5131, -0.7658],\n",
            "        [ 0.9617, -1.1148],\n",
            "        [ 1.0147, -0.9984],\n",
            "        [-1.4978,  1.4984],\n",
            "        [ 1.0282, -1.2313],\n",
            "        [-1.5552,  1.3930],\n",
            "        [ 0.7228, -0.7255],\n",
            "        [-1.2341,  1.5240],\n",
            "        [-1.0245,  0.7825]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8570, -0.7546],\n",
            "        [ 0.9806, -1.1590],\n",
            "        [-0.2526,  0.4316],\n",
            "        [ 0.4724, -0.8335],\n",
            "        [-1.3658,  1.4985],\n",
            "        [-1.3760,  1.4798],\n",
            "        [-1.2669,  1.4993],\n",
            "        [ 0.7814, -1.2862],\n",
            "        [ 0.9425, -1.4467],\n",
            "        [ 0.8854, -1.1460],\n",
            "        [ 0.8147, -1.1171],\n",
            "        [-0.5749,  0.2233],\n",
            "        [-0.4162,  0.3331],\n",
            "        [ 0.7172, -0.8759],\n",
            "        [ 0.6956, -1.0678],\n",
            "        [ 0.9709, -0.8060],\n",
            "        [ 0.0664, -0.3934],\n",
            "        [ 0.8240, -1.0183],\n",
            "        [-1.1713,  1.4083],\n",
            "        [-0.4983,  0.2928],\n",
            "        [ 0.5207, -0.9508],\n",
            "        [ 0.3307, -1.0054],\n",
            "        [-1.2369,  1.5237],\n",
            "        [ 1.0217, -1.0666],\n",
            "        [-0.9884,  0.9531],\n",
            "        [-0.0492, -0.1305],\n",
            "        [-0.7559,  0.7832],\n",
            "        [-0.0764, -0.0986],\n",
            "        [ 0.8298, -1.2202],\n",
            "        [ 0.9554, -0.9343],\n",
            "        [ 1.1689, -1.2400],\n",
            "        [-1.3880,  1.5181]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7429,  0.7788],\n",
            "        [ 1.0641, -1.1671],\n",
            "        [-1.1807,  1.7296],\n",
            "        [ 0.2949, -0.7817],\n",
            "        [-1.3304,  1.3694],\n",
            "        [ 0.1850, -0.4092],\n",
            "        [ 0.7534, -0.9196],\n",
            "        [-1.0329,  0.9710],\n",
            "        [-1.5096,  1.3276],\n",
            "        [-1.2659,  1.2018],\n",
            "        [-1.5780,  1.3817],\n",
            "        [ 0.0598, -0.3711],\n",
            "        [ 0.4205, -0.7004],\n",
            "        [ 0.6347, -0.8745],\n",
            "        [ 0.8121, -0.7255],\n",
            "        [ 1.0995, -0.9562],\n",
            "        [-1.0028,  0.9863],\n",
            "        [-1.4591,  1.4303],\n",
            "        [ 1.0036, -1.1647],\n",
            "        [ 0.8292, -1.2991],\n",
            "        [ 0.7670, -1.2966],\n",
            "        [-0.8611,  0.7920],\n",
            "        [ 0.9403, -1.1199],\n",
            "        [-1.2758,  1.1800],\n",
            "        [ 0.8395, -0.9491],\n",
            "        [ 0.1157, -0.2013],\n",
            "        [-1.5801,  1.7066],\n",
            "        [ 0.7893, -1.0753],\n",
            "        [ 0.6558, -1.0235],\n",
            "        [-0.9730,  1.4145],\n",
            "        [ 0.8138, -1.2975],\n",
            "        [-1.4258,  1.6299]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3434, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5643,  1.4963],\n",
            "        [ 0.9402, -1.0127],\n",
            "        [ 1.0107, -1.3073],\n",
            "        [-1.8142,  1.7000],\n",
            "        [ 0.5439, -0.7277],\n",
            "        [-0.9905,  1.2149],\n",
            "        [ 0.7897, -1.0981],\n",
            "        [ 0.4926, -0.9511],\n",
            "        [-1.5481,  1.7101],\n",
            "        [ 0.7895, -1.3059],\n",
            "        [ 0.6398, -1.0790],\n",
            "        [-1.7166,  1.5123],\n",
            "        [-0.1145, -0.0802],\n",
            "        [ 0.9346, -0.8731],\n",
            "        [ 0.5000, -0.5624],\n",
            "        [-1.3569,  1.3800],\n",
            "        [-1.2508,  1.5119],\n",
            "        [ 0.8985, -1.0503],\n",
            "        [ 0.2323, -0.2131],\n",
            "        [ 0.8032, -1.1193],\n",
            "        [ 1.0683, -1.1508],\n",
            "        [-1.5882,  1.7533],\n",
            "        [ 0.3529, -0.7875],\n",
            "        [-0.4741,  0.4185],\n",
            "        [-1.2391,  1.6213],\n",
            "        [ 0.1139,  0.1390],\n",
            "        [-1.1609,  1.2302],\n",
            "        [-1.3943,  1.3242],\n",
            "        [ 0.8589, -1.0882],\n",
            "        [ 0.1843, -0.0627],\n",
            "        [ 0.9526, -0.9621],\n",
            "        [ 0.8491, -1.0059]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8237, -1.1448],\n",
            "        [-1.4794,  1.6451],\n",
            "        [-1.3476,  1.6828],\n",
            "        [ 0.9653, -1.3370],\n",
            "        [ 1.0680, -1.2369],\n",
            "        [-1.4443,  1.0564],\n",
            "        [-1.4449,  1.7047],\n",
            "        [-0.9467,  1.4105],\n",
            "        [ 0.8975, -0.7996],\n",
            "        [ 0.7593, -0.9711],\n",
            "        [-1.2744,  1.2458],\n",
            "        [ 0.2029, -0.3575],\n",
            "        [ 0.5670, -1.1331],\n",
            "        [-0.0427,  0.2066],\n",
            "        [ 1.2437, -1.0930],\n",
            "        [ 0.7476, -0.8220],\n",
            "        [-1.4655,  1.6722],\n",
            "        [ 1.0824, -1.0733],\n",
            "        [ 0.7437, -1.1148],\n",
            "        [ 1.0230, -1.1543],\n",
            "        [ 0.1403, -0.3347],\n",
            "        [ 0.8087, -1.1188],\n",
            "        [-0.9934,  1.1770],\n",
            "        [ 1.0555, -1.3389],\n",
            "        [ 1.0466, -1.1922],\n",
            "        [-1.1983,  1.1160],\n",
            "        [ 0.8596, -1.2167],\n",
            "        [ 0.9646, -1.2248],\n",
            "        [ 0.6470, -0.9974],\n",
            "        [ 0.6574, -0.8759],\n",
            "        [ 0.3921, -0.6497],\n",
            "        [-1.4117,  1.7136]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5994, -1.0100],\n",
            "        [ 1.0744, -1.5387],\n",
            "        [ 0.1875, -0.7206],\n",
            "        [ 0.5008, -0.8135],\n",
            "        [ 1.2239, -1.3319],\n",
            "        [-1.5064,  1.5122],\n",
            "        [-0.4382,  0.5063],\n",
            "        [ 0.8630, -0.7116],\n",
            "        [-0.7272,  0.8914],\n",
            "        [ 0.6389, -0.9966],\n",
            "        [-1.2824,  1.4305],\n",
            "        [-0.7236,  0.8799],\n",
            "        [ 1.0317, -1.2886],\n",
            "        [ 1.1687, -1.2469],\n",
            "        [-1.5854,  1.4166],\n",
            "        [ 0.8984, -1.1176],\n",
            "        [-0.9550,  0.9483],\n",
            "        [-0.1105,  0.0076],\n",
            "        [-1.1909,  0.7871],\n",
            "        [-1.3940,  1.3704],\n",
            "        [-1.4448,  1.6699],\n",
            "        [-1.3165,  1.7075],\n",
            "        [ 0.5931, -0.8704],\n",
            "        [ 0.8326, -0.8869],\n",
            "        [-1.0920,  1.0551],\n",
            "        [ 0.8778, -0.9156],\n",
            "        [ 1.1011, -1.1856],\n",
            "        [-0.4074,  0.0824],\n",
            "        [-0.5789,  0.4160],\n",
            "        [ 0.6120, -0.7768],\n",
            "        [ 0.7199, -0.8434],\n",
            "        [-1.4503,  1.6888]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4678, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3548,  0.4213],\n",
            "        [-1.3959,  1.6833],\n",
            "        [ 0.8872, -0.9947],\n",
            "        [ 0.9529, -0.6466],\n",
            "        [ 0.3290, -0.2429],\n",
            "        [ 0.0399, -0.0142],\n",
            "        [ 0.1055, -0.0149],\n",
            "        [ 0.9159, -1.3342],\n",
            "        [-1.5301,  1.4543],\n",
            "        [ 0.6892, -0.9503],\n",
            "        [-1.3992,  1.4743],\n",
            "        [ 0.8515, -0.7989],\n",
            "        [ 0.9462, -1.2384],\n",
            "        [ 1.0858, -0.9211],\n",
            "        [ 0.1701,  0.1048],\n",
            "        [ 1.0085, -1.2687],\n",
            "        [ 0.9409, -1.1334],\n",
            "        [ 1.0501, -1.5297],\n",
            "        [ 1.1689, -1.0992],\n",
            "        [ 0.4063, -0.7712],\n",
            "        [ 0.8007, -1.1754],\n",
            "        [-1.4161,  1.6097],\n",
            "        [-0.6754,  0.6404],\n",
            "        [ 0.9532, -1.1032],\n",
            "        [ 0.3556, -0.3148],\n",
            "        [-1.5343,  1.6291],\n",
            "        [-0.9880,  1.0115],\n",
            "        [-1.5843,  1.4155],\n",
            "        [-0.2631, -0.1521],\n",
            "        [-1.1536,  1.0462],\n",
            "        [ 1.2258, -1.1411],\n",
            "        [-1.3900,  1.5879]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4114,  1.6433],\n",
            "        [ 0.8073, -0.9289],\n",
            "        [-1.5425,  1.5082],\n",
            "        [ 0.3527,  0.0760],\n",
            "        [ 0.7371, -1.1938],\n",
            "        [-1.1121,  1.2672],\n",
            "        [-1.2098,  1.6189],\n",
            "        [-1.3534,  1.3437],\n",
            "        [-0.8233,  0.6458],\n",
            "        [-0.2112, -0.1485],\n",
            "        [-0.2568,  0.1426],\n",
            "        [-0.9693,  1.2278],\n",
            "        [ 0.8768, -0.8986],\n",
            "        [ 0.8879, -1.0310],\n",
            "        [ 0.9278, -1.2478],\n",
            "        [ 0.4495, -1.1683],\n",
            "        [ 0.4215, -0.5574],\n",
            "        [ 0.8749, -1.1697],\n",
            "        [-0.8840,  0.9598],\n",
            "        [ 0.8566, -0.8736],\n",
            "        [-0.3052,  0.7778],\n",
            "        [-1.2153,  1.2106],\n",
            "        [-1.3813,  1.5513],\n",
            "        [-1.2049,  1.5693],\n",
            "        [ 1.0294, -1.2363],\n",
            "        [ 0.9627, -0.7941],\n",
            "        [-1.5694,  1.6932],\n",
            "        [ 0.9229, -0.9882],\n",
            "        [-1.3302,  1.3341],\n",
            "        [ 0.8089, -0.7273],\n",
            "        [ 0.8626, -0.8910],\n",
            "        [ 1.0456, -0.8773]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4779, -0.8742],\n",
            "        [ 1.0179, -0.9747],\n",
            "        [ 0.5441, -0.7729],\n",
            "        [ 0.9373, -1.0736],\n",
            "        [ 0.6182, -0.8263],\n",
            "        [ 0.9269, -0.9189],\n",
            "        [ 0.2077, -0.4902],\n",
            "        [-1.2824,  1.3598],\n",
            "        [ 1.0041, -0.9616],\n",
            "        [-0.9123,  1.0415],\n",
            "        [-0.5351,  0.4878],\n",
            "        [ 0.9647, -0.8948],\n",
            "        [ 0.9242, -1.0871],\n",
            "        [ 0.8889, -1.1411],\n",
            "        [ 0.4791, -0.8844],\n",
            "        [-1.6507,  1.7078],\n",
            "        [-0.9136,  0.8413],\n",
            "        [ 0.8898, -0.9431],\n",
            "        [-0.1610,  0.1851],\n",
            "        [-0.8923,  1.1702],\n",
            "        [ 0.5484, -0.7640],\n",
            "        [ 0.2501, -0.9552],\n",
            "        [ 0.9333, -1.3260],\n",
            "        [-0.9449,  1.0605],\n",
            "        [ 0.5170, -1.0009],\n",
            "        [ 0.3117, -0.3866],\n",
            "        [-1.4257,  1.5353],\n",
            "        [-0.8588,  0.9703],\n",
            "        [ 0.0587, -0.2186],\n",
            "        [-0.7578,  0.4717],\n",
            "        [-1.5151,  1.7531],\n",
            "        [ 0.7328, -1.0143]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1033, -1.1446],\n",
            "        [-1.1862,  1.2752],\n",
            "        [ 0.6548, -0.8756],\n",
            "        [ 0.6679, -1.1270],\n",
            "        [ 0.3669, -0.6122],\n",
            "        [-1.4141,  1.5911],\n",
            "        [-1.0834,  1.2161],\n",
            "        [ 0.9168, -1.0654],\n",
            "        [ 0.4209, -0.1610],\n",
            "        [-1.5572,  1.4645],\n",
            "        [-1.1923,  1.1612],\n",
            "        [ 0.2082, -0.3461],\n",
            "        [ 1.0444, -1.2504],\n",
            "        [-0.7043,  0.5037],\n",
            "        [-1.4976,  1.6990],\n",
            "        [-1.4608,  1.5466],\n",
            "        [ 1.0340, -1.1430],\n",
            "        [-0.5180,  0.2939],\n",
            "        [ 0.8432, -1.1813],\n",
            "        [ 0.8077, -1.0690],\n",
            "        [-1.5159,  1.4863],\n",
            "        [-1.6072,  1.4554],\n",
            "        [ 0.8363, -1.0409],\n",
            "        [ 0.6909, -0.7739],\n",
            "        [-1.4900,  1.4180],\n",
            "        [-0.1696,  0.2352],\n",
            "        [-1.3275,  1.1982],\n",
            "        [ 1.1691, -1.5084],\n",
            "        [ 0.1322, -0.3170],\n",
            "        [ 0.8843, -0.8531],\n",
            "        [-0.3498,  0.3797],\n",
            "        [ 0.6610, -0.9932]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3805, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9181, -1.2228],\n",
            "        [ 0.7517, -0.9204],\n",
            "        [ 0.9607, -1.2902],\n",
            "        [ 0.7357, -1.0009],\n",
            "        [-1.2661,  1.2866],\n",
            "        [ 0.6312, -1.1284],\n",
            "        [ 0.8364, -1.1158],\n",
            "        [-0.8557,  1.1818],\n",
            "        [ 0.3636, -0.6908],\n",
            "        [-0.2889,  0.3270],\n",
            "        [ 0.7372, -0.8610],\n",
            "        [-1.1151,  1.5168],\n",
            "        [ 0.9177, -0.9719],\n",
            "        [-1.4835,  1.7485],\n",
            "        [-0.7678,  0.8116],\n",
            "        [-1.4567,  1.6743],\n",
            "        [-0.9238,  1.3724],\n",
            "        [ 0.8082, -0.9074],\n",
            "        [ 1.1164, -1.0911],\n",
            "        [ 0.7006, -0.6601],\n",
            "        [-0.3701,  0.1239],\n",
            "        [-0.5905,  0.4900],\n",
            "        [ 0.6925, -0.9123],\n",
            "        [-0.7614,  0.8081],\n",
            "        [ 0.8643, -1.1499],\n",
            "        [-1.5021,  1.6243],\n",
            "        [ 0.3103, -0.6206],\n",
            "        [-1.6587,  1.5673],\n",
            "        [ 0.7520, -1.1529],\n",
            "        [ 0.7357, -1.1475],\n",
            "        [ 0.8713, -1.0380],\n",
            "        [ 0.9793, -0.9130]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch    50  of    191.    Elapsed: 0:01:09.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 9.8565e-01, -9.2911e-01],\n",
            "        [-1.0449e-01, -7.4434e-03],\n",
            "        [ 5.6469e-01, -1.0586e+00],\n",
            "        [ 6.2223e-01, -1.0389e+00],\n",
            "        [-7.4178e-02, -5.7313e-02],\n",
            "        [ 8.8982e-01, -1.1350e+00],\n",
            "        [-9.3771e-01,  9.1295e-01],\n",
            "        [ 1.8370e-01, -1.8469e-01],\n",
            "        [ 1.0236e+00, -1.0218e+00],\n",
            "        [ 9.3208e-01, -1.1680e+00],\n",
            "        [-1.5929e+00,  1.5979e+00],\n",
            "        [-1.6734e+00,  1.6833e+00],\n",
            "        [-1.2518e+00,  1.1914e+00],\n",
            "        [ 9.0340e-04, -3.3573e-01],\n",
            "        [ 6.0200e-01, -1.0217e+00],\n",
            "        [-2.2699e-01,  2.0656e-01],\n",
            "        [-1.4874e+00,  1.8418e+00],\n",
            "        [-5.8848e-01,  6.9017e-01],\n",
            "        [-1.3884e+00,  1.3017e+00],\n",
            "        [ 1.3771e+00, -1.2395e+00],\n",
            "        [-1.6843e+00,  1.7260e+00],\n",
            "        [-1.5495e+00,  1.4241e+00],\n",
            "        [ 7.5188e-01, -1.0732e+00],\n",
            "        [ 3.1870e-01, -6.1334e-01],\n",
            "        [-6.1045e-01,  5.2597e-01],\n",
            "        [-8.7927e-01,  8.7819e-01],\n",
            "        [-1.3331e-01, -3.5195e-02],\n",
            "        [-1.5895e+00,  1.4312e+00],\n",
            "        [-9.1097e-01,  7.5811e-01],\n",
            "        [ 8.4015e-01, -1.1386e+00],\n",
            "        [ 6.6502e-01, -1.0327e+00],\n",
            "        [ 6.9979e-01, -9.9931e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4973, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4167, -1.0803],\n",
            "        [-1.5633,  1.4051],\n",
            "        [ 0.9652, -1.0027],\n",
            "        [ 0.6566, -0.8259],\n",
            "        [-1.2817,  1.2819],\n",
            "        [ 0.0943,  0.0364],\n",
            "        [ 0.2501, -0.5917],\n",
            "        [ 1.0302, -1.2264],\n",
            "        [ 0.6774, -1.2825],\n",
            "        [-1.7933,  1.6812],\n",
            "        [ 0.9905, -1.2471],\n",
            "        [ 0.8778, -0.9902],\n",
            "        [-1.5425,  1.4710],\n",
            "        [-1.2038,  1.3015],\n",
            "        [-1.5265,  1.4994],\n",
            "        [ 0.7150, -0.8832],\n",
            "        [ 0.9946, -1.3765],\n",
            "        [ 0.2260,  0.2241],\n",
            "        [ 0.6061, -0.7689],\n",
            "        [ 0.3869, -0.7166],\n",
            "        [ 1.0347, -1.1192],\n",
            "        [ 0.6156, -0.9761],\n",
            "        [ 0.9863, -1.1547],\n",
            "        [ 0.9125, -1.1542],\n",
            "        [ 0.8753, -1.1190],\n",
            "        [ 0.5124, -0.5536],\n",
            "        [ 0.8870, -0.7854],\n",
            "        [ 0.9315, -1.2345],\n",
            "        [ 0.0922, -0.0067],\n",
            "        [ 1.1745, -1.1810],\n",
            "        [ 0.9851, -1.0022],\n",
            "        [-1.5629,  1.6851]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.6507,  1.3823],\n",
            "        [-0.4120,  0.3864],\n",
            "        [ 1.1383, -0.9845],\n",
            "        [ 0.6295, -0.5791],\n",
            "        [ 0.6586, -1.1675],\n",
            "        [-0.7703,  0.7418],\n",
            "        [-1.3998,  1.5579],\n",
            "        [ 0.8065, -1.1692],\n",
            "        [-1.3842,  1.3674],\n",
            "        [-1.3466,  1.5381],\n",
            "        [ 1.0891, -1.2488],\n",
            "        [ 0.8119, -1.1605],\n",
            "        [ 0.5563, -0.6270],\n",
            "        [ 1.0414, -1.5287],\n",
            "        [-1.3190,  1.7016],\n",
            "        [ 0.4340, -0.4495],\n",
            "        [ 1.0317, -1.2679],\n",
            "        [ 0.9608, -1.0558],\n",
            "        [-0.0958,  0.2055],\n",
            "        [ 0.1645, -0.2365],\n",
            "        [-0.6512,  0.5214],\n",
            "        [-1.3721,  1.4606],\n",
            "        [-1.1968,  1.5293],\n",
            "        [ 0.9512, -0.9542],\n",
            "        [ 0.6630, -0.9224],\n",
            "        [ 0.7676, -1.0473],\n",
            "        [ 1.1848, -1.1294],\n",
            "        [ 0.6793, -1.1789],\n",
            "        [ 1.0588, -1.0805],\n",
            "        [ 1.1345, -1.1108],\n",
            "        [-1.6130,  1.6050],\n",
            "        [-1.4856,  1.7544]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0486, -0.8961],\n",
            "        [ 0.9693, -1.3907],\n",
            "        [ 0.1512, -0.2805],\n",
            "        [ 0.5860, -1.0269],\n",
            "        [-1.1177,  1.3719],\n",
            "        [-0.9004,  1.0584],\n",
            "        [-1.2384,  1.6667],\n",
            "        [ 0.5045, -0.8838],\n",
            "        [ 0.7366, -1.0315],\n",
            "        [ 0.9531, -1.4083],\n",
            "        [ 0.4865, -0.3913],\n",
            "        [ 0.0035, -0.2145],\n",
            "        [-1.4987,  1.4012],\n",
            "        [ 0.9184, -1.3662],\n",
            "        [-1.5993,  1.7465],\n",
            "        [ 0.8236, -0.9120],\n",
            "        [ 0.7822, -0.9656],\n",
            "        [ 1.2656, -1.3057],\n",
            "        [-1.4648,  1.3667],\n",
            "        [-1.5108,  1.5317],\n",
            "        [ 0.3464, -0.7216],\n",
            "        [ 0.7591, -1.2985],\n",
            "        [ 0.8262, -1.0310],\n",
            "        [ 0.3678, -0.8125],\n",
            "        [ 0.8190, -0.9902],\n",
            "        [ 0.5658, -1.0533],\n",
            "        [ 0.4280, -0.5614],\n",
            "        [ 0.9809, -0.9790],\n",
            "        [-1.0801,  0.8524],\n",
            "        [-0.1763,  0.1253],\n",
            "        [-1.4492,  1.5030],\n",
            "        [ 1.1539, -1.2060]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3525, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5257, -0.7242],\n",
            "        [ 0.7633, -1.2563],\n",
            "        [-1.2953,  1.2651],\n",
            "        [ 0.8603, -0.8321],\n",
            "        [-1.3798,  1.2402],\n",
            "        [ 0.9831, -1.3065],\n",
            "        [ 0.8871, -0.8380],\n",
            "        [ 0.7079, -0.9495],\n",
            "        [ 0.6700, -0.9619],\n",
            "        [ 1.0102, -1.1517],\n",
            "        [-0.8467,  0.9039],\n",
            "        [ 0.3384, -0.8319],\n",
            "        [-1.6497,  1.5683],\n",
            "        [-1.9018,  1.5410],\n",
            "        [ 0.1051,  0.2309],\n",
            "        [-1.2530,  1.5951],\n",
            "        [ 0.5762, -0.7440],\n",
            "        [ 0.4265, -1.0334],\n",
            "        [-1.0518,  1.0269],\n",
            "        [-1.2390,  1.6500],\n",
            "        [ 0.7668, -1.0536],\n",
            "        [ 0.9463, -1.2178],\n",
            "        [-0.9095,  1.1533],\n",
            "        [ 0.6482, -0.9172],\n",
            "        [ 0.1910,  0.0623],\n",
            "        [ 0.8453, -1.1312],\n",
            "        [-0.0172, -0.2967],\n",
            "        [ 0.9250, -0.9505],\n",
            "        [-0.1474,  0.0383],\n",
            "        [ 0.9203, -1.1638],\n",
            "        [ 0.9505, -1.0712],\n",
            "        [ 0.5263, -1.0169]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2111,  1.1564],\n",
            "        [-1.5395,  1.8124],\n",
            "        [-1.3619,  1.4326],\n",
            "        [-0.9837,  1.1314],\n",
            "        [ 0.5741, -0.9062],\n",
            "        [ 0.7044, -1.0273],\n",
            "        [ 0.8810, -1.1978],\n",
            "        [-0.3678,  0.4493],\n",
            "        [ 1.0542, -1.2145],\n",
            "        [-1.1852,  1.4166],\n",
            "        [ 1.2293, -1.0084],\n",
            "        [-1.4355,  1.4810],\n",
            "        [ 0.8353, -1.0186],\n",
            "        [-0.1224, -0.1769],\n",
            "        [-1.6088,  1.3945],\n",
            "        [ 0.9792, -1.0423],\n",
            "        [ 0.8568, -1.0526],\n",
            "        [ 0.9080, -1.2168],\n",
            "        [ 1.0330, -1.0432],\n",
            "        [ 0.7089, -0.6898],\n",
            "        [ 1.3901, -1.3909],\n",
            "        [ 0.7439, -1.1912],\n",
            "        [-1.4667,  1.5883],\n",
            "        [ 1.0568, -1.3098],\n",
            "        [-1.5459,  1.8935],\n",
            "        [ 0.1526, -0.5535],\n",
            "        [ 0.4713, -0.4024],\n",
            "        [ 0.8301, -0.8777],\n",
            "        [ 0.7879, -1.0299],\n",
            "        [-0.9034,  0.6876],\n",
            "        [-1.3445,  1.7130],\n",
            "        [ 1.0521, -1.2132]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5820,  1.4744],\n",
            "        [-1.1192,  1.2104],\n",
            "        [ 0.0523, -0.0550],\n",
            "        [ 0.3920, -0.8393],\n",
            "        [-1.1451,  1.2634],\n",
            "        [-0.9397,  0.6589],\n",
            "        [-0.0710,  0.0861],\n",
            "        [ 0.8682, -1.1039],\n",
            "        [ 1.0238, -1.2985],\n",
            "        [-0.0797,  0.0598],\n",
            "        [-1.4478,  1.5994],\n",
            "        [ 0.6465, -1.1246],\n",
            "        [ 0.3733, -0.5995],\n",
            "        [ 1.0821, -1.2496],\n",
            "        [ 0.8898, -1.1907],\n",
            "        [ 0.8711, -1.5103],\n",
            "        [-0.7900,  0.9857],\n",
            "        [-1.1519,  1.3840],\n",
            "        [-1.4497,  1.7087],\n",
            "        [ 0.3884, -0.7222],\n",
            "        [-1.6074,  1.6146],\n",
            "        [ 0.9102, -0.7979],\n",
            "        [-0.8276,  0.8651],\n",
            "        [-1.3940,  1.4165],\n",
            "        [-0.1900,  0.5072],\n",
            "        [ 1.0611, -1.1084],\n",
            "        [ 0.9710, -1.3586],\n",
            "        [-1.3850,  1.4571],\n",
            "        [ 1.0420, -1.3696],\n",
            "        [ 1.0174, -1.3025],\n",
            "        [ 0.6082, -1.1035],\n",
            "        [-1.7057,  1.6662]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.6124,  1.6356],\n",
            "        [ 0.7854, -1.3522],\n",
            "        [-1.1245,  1.5795],\n",
            "        [ 0.6470, -0.9960],\n",
            "        [-0.1621, -0.1700],\n",
            "        [-0.5971,  0.4043],\n",
            "        [ 0.7067, -0.9707],\n",
            "        [ 1.1232, -1.0049],\n",
            "        [-1.5001,  1.4047],\n",
            "        [ 0.5064, -0.6309],\n",
            "        [ 0.9523, -1.1023],\n",
            "        [ 0.7250, -0.9268],\n",
            "        [ 0.4771, -0.9849],\n",
            "        [ 0.9476, -1.0964],\n",
            "        [-0.5331,  0.3141],\n",
            "        [ 1.3245, -1.3196],\n",
            "        [ 0.0543, -0.4781],\n",
            "        [-0.9849,  1.0359],\n",
            "        [-1.1515,  1.4158],\n",
            "        [ 0.3080, -0.8009],\n",
            "        [-1.4967,  1.3796],\n",
            "        [ 0.7946, -0.9436],\n",
            "        [ 0.9318, -1.1020],\n",
            "        [ 0.7160, -1.0690],\n",
            "        [ 0.1860, -0.1171],\n",
            "        [-1.6811,  1.5927],\n",
            "        [ 1.1623, -1.2315],\n",
            "        [ 1.1033, -1.3180],\n",
            "        [ 0.8428, -1.2685],\n",
            "        [-1.6175,  1.4541],\n",
            "        [ 0.9087, -0.9568],\n",
            "        [ 0.2365, -0.4021]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8696, -1.0231],\n",
            "        [-1.1257,  1.1799],\n",
            "        [-1.2047,  1.5501],\n",
            "        [ 0.6832, -0.7691],\n",
            "        [ 1.2276, -1.1645],\n",
            "        [-0.2001, -0.1649],\n",
            "        [-0.9945,  1.1808],\n",
            "        [-1.4042,  1.4336],\n",
            "        [-1.4303,  1.5285],\n",
            "        [ 0.8681, -1.0354],\n",
            "        [-1.5141,  1.4956],\n",
            "        [ 0.8822, -1.0881],\n",
            "        [ 1.0493, -1.0113],\n",
            "        [ 0.1672, -0.0574],\n",
            "        [ 0.6915, -0.9836],\n",
            "        [ 0.5434, -0.8898],\n",
            "        [ 1.2761, -1.0990],\n",
            "        [-0.5441,  0.3678],\n",
            "        [ 0.8497, -1.0647],\n",
            "        [-0.5698,  0.8563],\n",
            "        [ 0.9703, -0.9179],\n",
            "        [-1.5438,  1.7147],\n",
            "        [-1.5423,  1.6739],\n",
            "        [ 0.3630, -0.7261],\n",
            "        [-1.3090,  1.5801],\n",
            "        [-0.4316,  0.5206],\n",
            "        [ 0.4998, -0.6589],\n",
            "        [-1.2978,  1.6363],\n",
            "        [-1.4684,  1.3740],\n",
            "        [-0.7883,  0.7697],\n",
            "        [-1.4087,  1.4255],\n",
            "        [ 0.7367, -1.0875]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5642,  1.5766],\n",
            "        [ 0.7250, -0.9466],\n",
            "        [ 0.9732, -1.1640],\n",
            "        [ 0.9528, -1.0589],\n",
            "        [-1.5015,  0.9621],\n",
            "        [-0.4868,  0.1699],\n",
            "        [ 0.7596, -1.0291],\n",
            "        [-1.4402,  1.3499],\n",
            "        [-1.4573,  1.6082],\n",
            "        [ 1.0676, -1.2072],\n",
            "        [ 0.5998, -0.6081],\n",
            "        [ 0.3444, -0.5535],\n",
            "        [ 0.8751, -0.9884],\n",
            "        [-1.0023,  1.3636],\n",
            "        [-1.2684,  1.3736],\n",
            "        [-1.1272,  1.3010],\n",
            "        [-0.4226,  0.3897],\n",
            "        [ 0.6823, -1.0736],\n",
            "        [-1.5453,  1.7384],\n",
            "        [ 1.0666, -1.3557],\n",
            "        [-0.4600,  0.4553],\n",
            "        [ 0.1121, -0.0891],\n",
            "        [ 0.5061, -0.5997],\n",
            "        [ 0.9875, -1.2244],\n",
            "        [-0.1627, -0.0180],\n",
            "        [-1.4168,  1.6412],\n",
            "        [-1.3710,  1.3099],\n",
            "        [ 0.7630, -1.3215],\n",
            "        [ 0.6140, -0.6548],\n",
            "        [-1.0926,  1.1478],\n",
            "        [-0.5198,  0.6711],\n",
            "        [-0.7476,  0.9199]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3751, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3658,  1.5217],\n",
            "        [ 0.9765, -0.8683],\n",
            "        [-0.0528,  0.1860],\n",
            "        [ 0.7572, -1.0054],\n",
            "        [ 1.0895, -1.2721],\n",
            "        [ 0.9565, -1.0748],\n",
            "        [ 0.7630, -1.1971],\n",
            "        [ 0.9842, -1.1354],\n",
            "        [-1.5053,  1.4694],\n",
            "        [-1.5364,  1.7872],\n",
            "        [-1.4698,  1.6661],\n",
            "        [-1.2669,  1.8031],\n",
            "        [-0.3872,  0.3660],\n",
            "        [ 0.7573, -1.1924],\n",
            "        [-0.5238,  0.4914],\n",
            "        [ 0.9192, -1.1430],\n",
            "        [ 0.8650, -1.1804],\n",
            "        [ 0.4566, -0.2088],\n",
            "        [ 0.5273, -0.9794],\n",
            "        [ 0.5842, -0.7548],\n",
            "        [-0.0805,  0.1394],\n",
            "        [ 1.1246, -1.2062],\n",
            "        [-0.2173,  0.1819],\n",
            "        [ 0.1693, -0.4689],\n",
            "        [-1.5657,  1.5534],\n",
            "        [ 0.7351, -0.8671],\n",
            "        [-1.2491,  1.6166],\n",
            "        [ 0.4919, -0.4850],\n",
            "        [-1.5810,  1.5001],\n",
            "        [-1.4317,  1.3333],\n",
            "        [ 0.8954, -1.1071],\n",
            "        [ 1.2509, -1.0743]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6350, -1.0116],\n",
            "        [-1.2036,  1.6400],\n",
            "        [ 0.8593, -0.8903],\n",
            "        [-1.2377,  1.3074],\n",
            "        [-0.1806,  0.2890],\n",
            "        [-1.4524,  1.4851],\n",
            "        [-1.2810,  1.3205],\n",
            "        [ 0.5851, -0.9181],\n",
            "        [-1.1098,  1.5204],\n",
            "        [ 0.6277, -0.6369],\n",
            "        [-0.8039,  1.3272],\n",
            "        [-1.4962,  1.6429],\n",
            "        [ 1.2422, -0.9116],\n",
            "        [ 1.1551, -0.9295],\n",
            "        [ 0.5622, -0.7377],\n",
            "        [-1.4072,  1.4941],\n",
            "        [ 0.5782, -1.0567],\n",
            "        [ 0.6389, -1.1688],\n",
            "        [-1.2180,  1.4648],\n",
            "        [ 0.7633, -0.9748],\n",
            "        [-1.3663,  1.6350],\n",
            "        [ 0.8762, -1.4352],\n",
            "        [-0.0050, -0.1164],\n",
            "        [ 0.4768, -0.5642],\n",
            "        [-0.8851,  1.0424],\n",
            "        [-1.3577,  1.5621],\n",
            "        [ 0.6445, -0.8685],\n",
            "        [ 0.8376, -0.9593],\n",
            "        [-1.2244,  1.5349],\n",
            "        [ 0.0651, -0.2378],\n",
            "        [-0.9661,  0.6752],\n",
            "        [-1.5481,  1.2957]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5656, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7527, -0.9955],\n",
            "        [ 0.7409, -0.9464],\n",
            "        [ 1.0570, -0.9288],\n",
            "        [-1.3720,  1.6718],\n",
            "        [ 0.3621, -0.8501],\n",
            "        [ 0.2015, -0.5342],\n",
            "        [-1.7205,  1.4630],\n",
            "        [-1.2802,  1.4544],\n",
            "        [-0.9479,  1.0061],\n",
            "        [ 0.6480, -0.6267],\n",
            "        [ 0.8812, -1.0889],\n",
            "        [-1.3344,  1.7555],\n",
            "        [-1.5089,  1.5826],\n",
            "        [ 0.8132, -1.4458],\n",
            "        [ 0.8367, -0.7717],\n",
            "        [ 0.7722, -1.0937],\n",
            "        [-1.3288,  1.5464],\n",
            "        [ 1.0977, -1.0671],\n",
            "        [ 0.6948, -0.8171],\n",
            "        [ 0.8833, -1.0447],\n",
            "        [-1.1800,  1.2667],\n",
            "        [ 0.7766, -1.0497],\n",
            "        [-1.1707,  1.3799],\n",
            "        [ 0.8823, -1.0406],\n",
            "        [ 0.3667, -0.4802],\n",
            "        [-0.8972,  1.1781],\n",
            "        [-1.5755,  1.6180],\n",
            "        [ 0.0925,  0.1041],\n",
            "        [ 0.8252, -1.1062],\n",
            "        [ 0.8367, -1.2480],\n",
            "        [ 0.8953, -1.2868],\n",
            "        [ 0.7044, -0.9564]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8977, -0.9646],\n",
            "        [ 1.0739, -1.0403],\n",
            "        [-0.2889,  0.3269],\n",
            "        [ 0.9127, -1.2468],\n",
            "        [ 0.8398, -1.1300],\n",
            "        [ 1.2058, -1.1402],\n",
            "        [ 1.1958, -1.1944],\n",
            "        [ 1.0738, -1.0509],\n",
            "        [-1.5004,  1.4469],\n",
            "        [ 0.6542, -0.8620],\n",
            "        [-1.6634,  1.6585],\n",
            "        [ 0.6750, -0.8562],\n",
            "        [ 0.0921, -0.4726],\n",
            "        [-0.1749, -0.2052],\n",
            "        [ 0.8650, -1.0448],\n",
            "        [ 0.9054, -1.0352],\n",
            "        [ 0.2756, -0.7185],\n",
            "        [ 0.7772, -1.1928],\n",
            "        [ 0.4629, -0.1933],\n",
            "        [-1.5539,  1.5864],\n",
            "        [ 1.0218, -1.1949],\n",
            "        [-1.5357,  1.7785],\n",
            "        [ 0.6470, -0.9895],\n",
            "        [ 1.0881, -1.1230],\n",
            "        [ 0.7088, -0.7442],\n",
            "        [-1.6658,  1.6640],\n",
            "        [ 1.0518, -1.3535],\n",
            "        [ 0.7380, -0.7507],\n",
            "        [ 0.9846, -1.1015],\n",
            "        [-0.1821,  0.2946],\n",
            "        [ 0.8747, -1.4379],\n",
            "        [-1.2561,  1.3220]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1008, -1.1598],\n",
            "        [ 0.9058, -1.3219],\n",
            "        [-1.1893,  1.1053],\n",
            "        [-1.4989,  1.2630],\n",
            "        [-0.4536,  0.4639],\n",
            "        [-0.8718,  0.9757],\n",
            "        [ 0.4149, -0.8417],\n",
            "        [-1.5120,  1.5493],\n",
            "        [-0.5794,  0.3864],\n",
            "        [-1.3665,  1.2580],\n",
            "        [ 0.7056, -1.0767],\n",
            "        [ 0.8479, -0.9112],\n",
            "        [-0.7919,  1.0535],\n",
            "        [ 1.0669, -1.3633],\n",
            "        [ 0.9912, -0.7283],\n",
            "        [ 0.4702, -0.6722],\n",
            "        [-1.3467,  1.7840],\n",
            "        [ 0.7850, -0.9217],\n",
            "        [-0.4306,  0.6325],\n",
            "        [ 1.2467, -1.1711],\n",
            "        [ 0.9535, -1.2338],\n",
            "        [-0.9409,  0.7870],\n",
            "        [ 0.7689, -0.6892],\n",
            "        [ 0.9114, -1.3472],\n",
            "        [ 0.7883, -1.0087],\n",
            "        [ 1.1081, -1.1895],\n",
            "        [-1.4322,  1.2985],\n",
            "        [ 0.8178, -0.9361],\n",
            "        [ 0.4739, -0.8702],\n",
            "        [ 0.6670, -0.9069],\n",
            "        [ 0.6741, -0.7126],\n",
            "        [-1.2571,  1.1055]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3161, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0960, -1.1594],\n",
            "        [ 1.0107, -1.2665],\n",
            "        [ 0.9379, -1.2539],\n",
            "        [-1.5362,  1.7286],\n",
            "        [ 0.9359, -1.1771],\n",
            "        [ 0.4214, -0.3540],\n",
            "        [ 0.5815, -0.9218],\n",
            "        [-1.5000,  1.7855],\n",
            "        [ 0.4470, -0.7622],\n",
            "        [ 0.1850,  0.0898],\n",
            "        [-1.6118,  1.4836],\n",
            "        [ 1.1725, -1.4441],\n",
            "        [ 1.0601, -0.6139],\n",
            "        [ 0.1111, -0.4561],\n",
            "        [-1.2196,  1.2917],\n",
            "        [-0.6820,  0.3529],\n",
            "        [-1.4444,  1.7428],\n",
            "        [-1.4393,  1.3098],\n",
            "        [ 1.0163, -1.2219],\n",
            "        [ 1.0086, -1.1924],\n",
            "        [ 1.2173, -1.3525],\n",
            "        [ 0.5642, -1.1446],\n",
            "        [ 0.6451, -0.8859],\n",
            "        [ 0.3099, -0.7482],\n",
            "        [-1.4457,  1.4998],\n",
            "        [-0.8761,  0.9195],\n",
            "        [-0.0874,  0.2384],\n",
            "        [ 1.0616, -1.2832],\n",
            "        [ 1.1458, -1.1806],\n",
            "        [-1.1954,  1.7207],\n",
            "        [-1.4915,  1.3688],\n",
            "        [ 0.5226, -0.8350]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4162,  1.6075],\n",
            "        [ 0.1436,  0.1759],\n",
            "        [-0.3870,  0.4003],\n",
            "        [-1.3740,  1.5912],\n",
            "        [ 1.1404, -1.1960],\n",
            "        [ 1.0038, -1.2060],\n",
            "        [ 1.1875, -1.1920],\n",
            "        [ 0.5225, -0.7102],\n",
            "        [ 0.9602, -0.9135],\n",
            "        [ 0.1891, -0.4406],\n",
            "        [ 0.5671, -0.6820],\n",
            "        [ 0.8004, -0.6567],\n",
            "        [-1.0745,  1.2107],\n",
            "        [ 0.9883, -1.0178],\n",
            "        [-1.3666,  1.3942],\n",
            "        [ 0.9908, -1.1229],\n",
            "        [ 0.7229, -1.0800],\n",
            "        [-0.1860,  0.5821],\n",
            "        [ 0.2286, -0.5284],\n",
            "        [-1.5069,  1.7254],\n",
            "        [ 0.7557, -1.3571],\n",
            "        [-1.4972,  1.1274],\n",
            "        [ 0.7935, -1.1851],\n",
            "        [ 0.8098, -0.9632],\n",
            "        [ 1.0263, -1.2302],\n",
            "        [ 0.8733, -0.6037],\n",
            "        [-0.0120,  0.3397],\n",
            "        [-1.5651,  1.5421],\n",
            "        [-0.0993,  0.1267],\n",
            "        [ 0.8204, -1.2677],\n",
            "        [ 1.1510, -1.3487],\n",
            "        [ 0.8870, -1.1923]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3186, -0.0074],\n",
            "        [-0.0606, -0.2390],\n",
            "        [-1.1629,  1.4571],\n",
            "        [ 0.8383, -1.1189],\n",
            "        [ 0.6420, -1.0321],\n",
            "        [ 0.9048, -0.9918],\n",
            "        [ 0.9715, -0.9962],\n",
            "        [ 0.6255, -1.2409],\n",
            "        [-1.3752,  1.6847],\n",
            "        [ 1.2392, -1.1220],\n",
            "        [-1.1754,  1.3333],\n",
            "        [ 0.0087, -0.4568],\n",
            "        [ 0.9991, -0.9034],\n",
            "        [ 0.8184, -1.2805],\n",
            "        [ 0.8909, -1.1528],\n",
            "        [-1.1355,  1.4820],\n",
            "        [ 0.8860, -0.8998],\n",
            "        [-1.4221,  1.2188],\n",
            "        [-1.4555,  1.5848],\n",
            "        [ 0.9117, -0.9297],\n",
            "        [ 0.7012, -1.0802],\n",
            "        [ 0.4978, -0.6994],\n",
            "        [ 1.0391, -1.3080],\n",
            "        [-1.0159,  0.9331],\n",
            "        [ 1.0344, -1.1784],\n",
            "        [ 0.6493, -0.9311],\n",
            "        [-0.9026,  0.9014],\n",
            "        [ 0.9472, -1.4019],\n",
            "        [-1.7344,  1.2845],\n",
            "        [-1.2661,  1.1645],\n",
            "        [ 0.6383, -0.9469],\n",
            "        [-1.5512,  1.7752]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9222,  1.2585],\n",
            "        [-1.6022,  1.5219],\n",
            "        [-0.0542, -0.2567],\n",
            "        [ 0.8755, -0.9764],\n",
            "        [ 0.3605, -0.7479],\n",
            "        [-1.4013,  1.4824],\n",
            "        [-0.9504,  1.0909],\n",
            "        [ 0.5389, -0.7079],\n",
            "        [ 0.2895, -0.5981],\n",
            "        [ 1.2362, -1.2251],\n",
            "        [ 0.2952, -0.6298],\n",
            "        [-1.2400,  1.5644],\n",
            "        [ 1.0025, -1.0146],\n",
            "        [-1.2689,  1.4103],\n",
            "        [ 0.7988, -1.2890],\n",
            "        [-1.4372,  1.7669],\n",
            "        [ 1.1728, -1.2274],\n",
            "        [ 0.8528, -1.1100],\n",
            "        [ 0.4589, -0.7146],\n",
            "        [-0.6224,  0.7064],\n",
            "        [-1.1925,  1.2760],\n",
            "        [-0.0507,  0.0183],\n",
            "        [ 0.0901, -0.4608],\n",
            "        [-1.4145,  1.6884],\n",
            "        [-1.3361,  1.4069],\n",
            "        [ 0.5947, -0.9894],\n",
            "        [-0.4711,  0.3364],\n",
            "        [ 1.1009, -1.2239],\n",
            "        [ 0.9211, -1.1073],\n",
            "        [ 0.9341, -1.1233],\n",
            "        [ 0.9501, -1.3246],\n",
            "        [ 0.8482, -1.2393]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1517, -1.5184],\n",
            "        [ 0.9619, -1.3315],\n",
            "        [-0.9869,  0.9794],\n",
            "        [-1.2642,  1.6356],\n",
            "        [ 0.0859, -0.3803],\n",
            "        [ 0.6538, -0.9811],\n",
            "        [ 1.0034, -1.3952],\n",
            "        [-0.6252,  0.9221],\n",
            "        [ 0.9050, -1.3503],\n",
            "        [ 0.8977, -1.2760],\n",
            "        [ 0.7182, -1.1118],\n",
            "        [-1.1309,  1.4597],\n",
            "        [-1.0064,  0.8676],\n",
            "        [-0.5832,  0.7213],\n",
            "        [ 0.9390, -1.1305],\n",
            "        [ 0.1204, -0.4102],\n",
            "        [-0.6078,  0.9286],\n",
            "        [-0.5168,  0.5240],\n",
            "        [ 0.4440, -1.1052],\n",
            "        [ 1.1688, -1.2896],\n",
            "        [ 0.9551, -1.0914],\n",
            "        [ 0.9700, -1.0751],\n",
            "        [ 0.0829,  0.1649],\n",
            "        [-1.6552,  1.8429],\n",
            "        [-0.2093,  0.2267],\n",
            "        [ 0.7775, -0.8813],\n",
            "        [ 0.8875, -1.0761],\n",
            "        [-1.5855,  1.3705],\n",
            "        [ 0.8581, -1.0850],\n",
            "        [ 0.0202, -0.0032],\n",
            "        [ 0.8800, -0.7909],\n",
            "        [-0.1292, -0.2564]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6713, -1.0282],\n",
            "        [-0.9207,  0.9595],\n",
            "        [ 0.6335, -1.0100],\n",
            "        [ 0.5048, -0.7626],\n",
            "        [ 0.8845, -1.3049],\n",
            "        [ 0.9899, -1.0641],\n",
            "        [-1.5413,  1.5522],\n",
            "        [ 0.4069, -0.3942],\n",
            "        [ 0.7838, -0.8816],\n",
            "        [ 0.7036, -0.9619],\n",
            "        [ 0.3067, -0.9583],\n",
            "        [-1.3335,  1.6258],\n",
            "        [ 0.6044, -0.9233],\n",
            "        [ 0.9430, -1.2567],\n",
            "        [ 0.8947, -0.9750],\n",
            "        [ 1.0907, -0.9851],\n",
            "        [ 0.4297, -0.9416],\n",
            "        [-1.2952,  1.6818],\n",
            "        [ 0.9319, -1.3117],\n",
            "        [-1.6077,  1.2268],\n",
            "        [ 0.9739, -1.1873],\n",
            "        [ 0.6512, -0.6546],\n",
            "        [-1.2707,  1.2662],\n",
            "        [ 0.8008, -1.0717],\n",
            "        [ 1.0343, -1.2243],\n",
            "        [ 1.1329, -1.3272],\n",
            "        [-0.7957,  0.8910],\n",
            "        [-1.4503,  1.5498],\n",
            "        [ 0.3800, -0.7661],\n",
            "        [-1.7104,  1.4238],\n",
            "        [ 0.8485, -1.1938],\n",
            "        [ 1.1232, -1.3581]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4929,  1.5853],\n",
            "        [-1.6717,  1.6810],\n",
            "        [-1.5649,  1.7395],\n",
            "        [ 0.7319, -0.6488],\n",
            "        [-1.4807,  1.6541],\n",
            "        [ 0.2163, -0.4666],\n",
            "        [ 0.8681, -1.0402],\n",
            "        [ 1.1311, -1.4744],\n",
            "        [ 0.9208, -1.2776],\n",
            "        [ 1.1615, -1.1914],\n",
            "        [-0.4006,  0.2821],\n",
            "        [ 0.6945, -0.8551],\n",
            "        [ 0.9676, -1.1554],\n",
            "        [ 1.1898, -1.1686],\n",
            "        [ 0.9227, -1.0119],\n",
            "        [ 0.8055, -1.0786],\n",
            "        [-1.5047,  1.4870],\n",
            "        [ 0.6591, -1.1052],\n",
            "        [-1.7668,  1.6919],\n",
            "        [ 0.8314, -1.2121],\n",
            "        [ 1.0674, -1.2295],\n",
            "        [ 1.0592, -1.2351],\n",
            "        [-0.8917,  1.0635],\n",
            "        [-1.4461,  1.3573],\n",
            "        [ 0.4228, -0.7192],\n",
            "        [ 0.2748, -0.7473],\n",
            "        [ 1.1803, -1.3236],\n",
            "        [-1.4192,  1.5964],\n",
            "        [ 0.9047, -1.1541],\n",
            "        [ 0.9131, -1.0487],\n",
            "        [ 0.8057, -1.1318],\n",
            "        [-0.1677,  0.0080]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2809, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2666, -0.4535],\n",
            "        [ 0.9447, -1.2761],\n",
            "        [-1.4997,  1.7747],\n",
            "        [ 0.2975, -0.4676],\n",
            "        [ 0.8962, -1.2816],\n",
            "        [ 0.7060, -0.9159],\n",
            "        [ 1.0670, -1.1466],\n",
            "        [ 0.9782, -1.1775],\n",
            "        [ 0.8694, -1.3340],\n",
            "        [ 0.8880, -0.9639],\n",
            "        [ 0.8373, -1.3521],\n",
            "        [ 0.0422, -0.5003],\n",
            "        [-0.2365,  0.1932],\n",
            "        [ 0.4513, -0.8144],\n",
            "        [-0.8493,  1.1824],\n",
            "        [ 0.4822, -0.8773],\n",
            "        [ 0.9295, -1.0798],\n",
            "        [ 0.5033, -0.7256],\n",
            "        [ 0.8838, -1.0976],\n",
            "        [ 1.1832, -1.4364],\n",
            "        [ 0.5858, -0.9379],\n",
            "        [ 0.8068, -0.7877],\n",
            "        [ 1.0272, -1.2367],\n",
            "        [ 0.3202, -0.4693],\n",
            "        [ 1.1190, -1.5330],\n",
            "        [ 0.7357, -0.9191],\n",
            "        [-0.7871,  0.8659],\n",
            "        [-0.0323,  0.0799],\n",
            "        [-1.7212,  1.6061],\n",
            "        [-0.3354,  0.1213],\n",
            "        [-1.4608,  1.5543],\n",
            "        [ 0.6984, -0.8188]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4407,  1.4460],\n",
            "        [ 0.8553, -1.2760],\n",
            "        [-0.9733,  0.9467],\n",
            "        [ 0.5735, -0.7644],\n",
            "        [-0.9876,  0.8833],\n",
            "        [ 0.6692, -0.5356],\n",
            "        [ 0.6487, -0.7545],\n",
            "        [ 1.2680, -1.4885],\n",
            "        [-1.2773,  1.4079],\n",
            "        [-1.0233,  0.9121],\n",
            "        [ 0.8247, -0.9815],\n",
            "        [ 0.9405, -1.2983],\n",
            "        [-1.0250,  1.2245],\n",
            "        [-0.3479,  0.3041],\n",
            "        [ 0.8486, -0.3818],\n",
            "        [ 1.0305, -0.9907],\n",
            "        [ 0.9933, -0.9473],\n",
            "        [ 0.9292, -0.9823],\n",
            "        [ 0.2348, -0.5732],\n",
            "        [-1.2508,  1.4281],\n",
            "        [-1.4039,  1.4904],\n",
            "        [ 1.2080, -1.0347],\n",
            "        [ 1.1573, -1.4848],\n",
            "        [-0.4001,  0.4728],\n",
            "        [-1.2297,  1.4264],\n",
            "        [ 0.3247, -0.3159],\n",
            "        [-1.4166,  1.4881],\n",
            "        [ 0.7740, -1.0262],\n",
            "        [ 0.3385, -0.8257],\n",
            "        [ 0.8812, -0.9258],\n",
            "        [ 0.8091, -0.7420],\n",
            "        [ 0.6790, -0.9547]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1214, -0.2353],\n",
            "        [ 0.7674, -0.8386],\n",
            "        [-1.0750,  1.2761],\n",
            "        [ 0.9120, -1.2221],\n",
            "        [ 0.9093, -1.1555],\n",
            "        [ 1.0332, -1.2878],\n",
            "        [-1.5484,  1.7207],\n",
            "        [ 0.8491, -1.1133],\n",
            "        [-1.6492,  1.6860],\n",
            "        [-0.4322,  0.4130],\n",
            "        [-1.2994,  1.4424],\n",
            "        [ 0.7816, -0.8936],\n",
            "        [-1.7630,  1.3763],\n",
            "        [ 1.0518, -1.0529],\n",
            "        [ 0.9155, -1.2195],\n",
            "        [ 0.6204, -1.4089],\n",
            "        [ 0.5001, -0.5352],\n",
            "        [ 1.0989, -0.9744],\n",
            "        [ 0.9892, -1.0948],\n",
            "        [-0.8362,  0.8757],\n",
            "        [ 1.1040, -1.2992],\n",
            "        [-0.6353,  0.9773],\n",
            "        [ 0.6949, -0.7566],\n",
            "        [-1.0562,  0.9543],\n",
            "        [-0.8452,  0.7466],\n",
            "        [ 1.2213, -1.2422],\n",
            "        [ 1.1171, -1.2323],\n",
            "        [ 0.9783, -1.1211],\n",
            "        [ 0.6176, -1.1702],\n",
            "        [ 0.9419, -1.0047],\n",
            "        [ 1.1559, -1.4876],\n",
            "        [ 0.2867, -0.3033]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9112, -1.0999],\n",
            "        [-0.3269,  0.3171],\n",
            "        [-1.4051,  1.5136],\n",
            "        [ 0.8812, -1.2251],\n",
            "        [ 1.0176, -1.1614],\n",
            "        [ 0.6761, -1.0313],\n",
            "        [-1.5129,  1.4987],\n",
            "        [ 1.1615, -1.3333],\n",
            "        [-0.7290,  0.7778],\n",
            "        [-1.3425,  1.3225],\n",
            "        [ 0.9360, -1.0949],\n",
            "        [ 0.3594, -0.5179],\n",
            "        [ 0.8542, -1.1276],\n",
            "        [-0.3847,  0.4101],\n",
            "        [ 0.9511, -1.0742],\n",
            "        [-1.4444,  1.3990],\n",
            "        [-0.3815,  0.6065],\n",
            "        [-0.5015,  0.5140],\n",
            "        [ 0.9861, -1.0177],\n",
            "        [ 0.9738, -1.2591],\n",
            "        [-1.5145,  1.4742],\n",
            "        [ 0.7296, -0.9204],\n",
            "        [ 1.1161, -1.3652],\n",
            "        [-1.0846,  1.1282],\n",
            "        [ 0.7411, -1.4789],\n",
            "        [ 0.3197, -0.4359],\n",
            "        [-1.4747,  1.6110],\n",
            "        [ 0.8377, -1.3221],\n",
            "        [ 0.8623, -1.0539],\n",
            "        [-0.0382,  0.3123],\n",
            "        [ 1.0353, -1.1661],\n",
            "        [ 0.9678, -1.0785]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3557,  1.6645],\n",
            "        [ 1.0780, -0.9951],\n",
            "        [ 0.4277, -0.7327],\n",
            "        [-0.3910,  0.4240],\n",
            "        [-0.0633, -0.3776],\n",
            "        [ 0.5447, -0.6836],\n",
            "        [-1.4138,  1.4719],\n",
            "        [ 0.9430, -0.8854],\n",
            "        [ 0.6814, -1.0434],\n",
            "        [-0.9839,  0.8366],\n",
            "        [ 1.0584, -1.4973],\n",
            "        [ 0.7393, -0.7852],\n",
            "        [-1.3121,  1.5035],\n",
            "        [ 0.8978, -1.0597],\n",
            "        [ 0.9987, -1.2699],\n",
            "        [ 0.8262, -1.0240],\n",
            "        [-0.7416,  0.8432],\n",
            "        [ 0.5797, -0.8519],\n",
            "        [ 0.9918, -1.2878],\n",
            "        [-0.8847,  1.0208],\n",
            "        [ 1.1129, -1.0712],\n",
            "        [-1.6763,  1.5762],\n",
            "        [-1.4108,  1.1615],\n",
            "        [ 0.9625, -1.1371],\n",
            "        [ 0.5308, -0.8487],\n",
            "        [-1.5993,  1.8406],\n",
            "        [ 0.2212, -0.5679],\n",
            "        [-1.7372,  1.8479],\n",
            "        [ 1.1143, -1.2110],\n",
            "        [ 1.0725, -1.1085],\n",
            "        [ 0.4829, -0.6979],\n",
            "        [ 0.8768, -1.0840]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2133,  1.6047],\n",
            "        [ 0.5510, -0.6587],\n",
            "        [-1.3880,  1.6035],\n",
            "        [-0.9585,  0.9846],\n",
            "        [-0.8264,  1.3888],\n",
            "        [ 0.4762, -1.1304],\n",
            "        [ 0.7252, -0.9950],\n",
            "        [-1.2706,  1.4133],\n",
            "        [ 0.5936, -1.0337],\n",
            "        [-0.6592,  0.7891],\n",
            "        [-1.3299,  1.2008],\n",
            "        [-0.6181,  0.5861],\n",
            "        [ 0.6109, -0.8350],\n",
            "        [-0.2030,  0.2137],\n",
            "        [ 0.7226, -0.9494],\n",
            "        [ 0.6168, -0.9449],\n",
            "        [ 1.0850, -1.4058],\n",
            "        [-1.7409,  1.6126],\n",
            "        [-1.2528,  1.3452],\n",
            "        [-1.4530,  1.7727],\n",
            "        [-1.1941,  1.1115],\n",
            "        [ 1.0612, -0.9020],\n",
            "        [-0.6969,  0.3068],\n",
            "        [ 0.2513, -0.3857],\n",
            "        [ 0.7972, -0.9377],\n",
            "        [ 0.8213, -1.2396],\n",
            "        [-0.5530,  0.4697],\n",
            "        [ 0.5857, -0.9332],\n",
            "        [-1.3941,  1.3762],\n",
            "        [-1.3439,  1.3952],\n",
            "        [ 0.7389, -1.3737],\n",
            "        [ 1.0396, -1.1014]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1076, -1.2455],\n",
            "        [ 0.9715, -0.9788],\n",
            "        [-1.2601,  1.2552],\n",
            "        [ 0.7969, -0.9889],\n",
            "        [ 1.3328, -1.3421],\n",
            "        [-0.9071,  0.8208],\n",
            "        [-1.0280,  1.2452],\n",
            "        [-0.6147,  0.6744],\n",
            "        [ 0.9833, -1.1708],\n",
            "        [ 0.8463, -1.0327],\n",
            "        [ 1.0840, -1.3392],\n",
            "        [ 0.9338, -1.3097],\n",
            "        [ 1.1716, -1.2055],\n",
            "        [-1.7807,  1.5515],\n",
            "        [ 0.7019, -1.0585],\n",
            "        [ 0.5432, -0.9206],\n",
            "        [ 0.9114, -1.3286],\n",
            "        [ 0.9905, -0.8084],\n",
            "        [ 0.0858, -0.0993],\n",
            "        [ 1.0012, -1.5485],\n",
            "        [-1.7070,  1.7798],\n",
            "        [ 0.8359, -1.0625],\n",
            "        [ 1.0105, -1.1461],\n",
            "        [ 0.9715, -1.1882],\n",
            "        [-1.5151,  1.4870],\n",
            "        [ 0.8816, -1.0334],\n",
            "        [ 0.6148, -0.9656],\n",
            "        [ 1.1466, -1.1739],\n",
            "        [-1.3963,  1.7281],\n",
            "        [ 0.6912, -1.0415],\n",
            "        [ 0.4547, -0.7820],\n",
            "        [-0.7093,  0.9813]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1984,  1.2792],\n",
            "        [ 0.0975, -0.0047],\n",
            "        [-0.8048,  0.9458],\n",
            "        [-1.6708,  1.5124],\n",
            "        [-0.3339,  0.4399],\n",
            "        [ 1.0379, -0.8513],\n",
            "        [ 0.6298, -1.2041],\n",
            "        [ 0.9586, -1.0603],\n",
            "        [ 0.7180, -1.0532],\n",
            "        [-1.4893,  1.4374],\n",
            "        [ 0.7393, -1.2459],\n",
            "        [-1.3427,  1.5728],\n",
            "        [ 0.4751, -1.0129],\n",
            "        [ 0.9961, -1.0720],\n",
            "        [ 0.5642, -1.0793],\n",
            "        [ 0.8511, -1.0721],\n",
            "        [-1.6490,  1.6167],\n",
            "        [-1.4675,  1.6579],\n",
            "        [ 0.6656, -1.3416],\n",
            "        [ 1.0425, -1.1930],\n",
            "        [ 1.2380, -1.1903],\n",
            "        [-1.1208,  1.4526],\n",
            "        [ 0.9249, -1.3513],\n",
            "        [ 0.2811, -0.4615],\n",
            "        [ 0.2968, -0.5520],\n",
            "        [ 0.5131, -0.6958],\n",
            "        [ 1.0765, -1.2627],\n",
            "        [ 0.6304, -1.0004],\n",
            "        [-1.6740,  1.5889],\n",
            "        [-1.4609,  1.8710],\n",
            "        [-0.8391,  0.8779],\n",
            "        [ 1.1768, -1.1667]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8992, -1.1538],\n",
            "        [ 0.7778, -0.8768],\n",
            "        [ 0.8889, -1.0582],\n",
            "        [ 0.1009, -0.3945],\n",
            "        [ 1.1032, -1.1017],\n",
            "        [ 0.7241, -1.0904],\n",
            "        [ 0.9845, -1.0744],\n",
            "        [-1.5950,  1.7450],\n",
            "        [-0.7473,  0.7925],\n",
            "        [-1.4354,  1.5540],\n",
            "        [ 0.8948, -1.0735],\n",
            "        [ 0.7735, -0.7740],\n",
            "        [ 0.6772, -0.9170],\n",
            "        [ 0.7004, -0.7055],\n",
            "        [-1.2469,  1.1228],\n",
            "        [ 0.7855, -0.8892],\n",
            "        [ 1.0881, -0.8172],\n",
            "        [-1.4530,  1.5215],\n",
            "        [ 0.9654, -1.0845],\n",
            "        [ 0.8484, -1.0004],\n",
            "        [ 1.0497, -1.3202],\n",
            "        [-0.3196,  0.4260],\n",
            "        [ 0.6080, -0.6917],\n",
            "        [ 1.0561, -1.1749],\n",
            "        [-0.0595, -0.0258],\n",
            "        [-0.1396, -0.2901],\n",
            "        [-0.6950,  0.7334],\n",
            "        [-1.4081,  1.2857],\n",
            "        [ 1.0268, -1.3197],\n",
            "        [ 0.9826, -1.1197],\n",
            "        [ 0.9840, -1.2319],\n",
            "        [-1.3774,  1.5613]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3046, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6620, -1.2234],\n",
            "        [ 0.5032, -1.1294],\n",
            "        [-1.1675,  1.2070],\n",
            "        [-1.4671,  1.6434],\n",
            "        [ 0.7829, -0.6921],\n",
            "        [-0.9215,  0.8988],\n",
            "        [-1.4503,  1.4606],\n",
            "        [ 0.9714, -0.8790],\n",
            "        [ 0.7873, -0.9380],\n",
            "        [-1.5237,  1.5463],\n",
            "        [ 0.7558, -1.1125],\n",
            "        [ 0.5494, -1.0188],\n",
            "        [ 1.1352, -1.2457],\n",
            "        [-0.9455,  1.1561],\n",
            "        [ 1.2414, -1.0950],\n",
            "        [-1.1797,  1.4177],\n",
            "        [-1.0062,  0.9377],\n",
            "        [-1.0296,  1.3936],\n",
            "        [-1.0944,  1.2388],\n",
            "        [ 1.1564, -1.3003],\n",
            "        [-0.0244,  0.3280],\n",
            "        [-0.0226, -0.2078],\n",
            "        [-1.3244,  1.3859],\n",
            "        [-1.2983,  1.6658],\n",
            "        [ 0.8946, -1.1351],\n",
            "        [ 0.7999, -1.0271],\n",
            "        [ 0.6669, -1.1538],\n",
            "        [ 0.8400, -1.2425],\n",
            "        [ 1.0647, -1.4573],\n",
            "        [ 0.6700, -0.8674],\n",
            "        [-1.4497,  1.7634],\n",
            "        [ 1.2560, -1.2585]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3187,  1.3930],\n",
            "        [ 0.9285, -1.1000],\n",
            "        [ 1.1373, -1.3965],\n",
            "        [ 1.0484, -1.1253],\n",
            "        [ 1.2147, -1.3448],\n",
            "        [-1.2165,  1.1152],\n",
            "        [ 1.2488, -0.9267],\n",
            "        [-1.6138,  1.6269],\n",
            "        [-0.5252,  0.5619],\n",
            "        [ 0.6781, -0.7628],\n",
            "        [ 0.8427, -0.8826],\n",
            "        [-0.7854,  0.9217],\n",
            "        [ 0.7226, -0.9259],\n",
            "        [ 1.2457, -1.4854],\n",
            "        [ 0.6615, -1.2365],\n",
            "        [-0.2100,  0.4772],\n",
            "        [-1.5515,  1.3053],\n",
            "        [-1.2453,  1.6861],\n",
            "        [-1.0203,  1.1666],\n",
            "        [ 0.6568, -0.7248],\n",
            "        [-0.5993,  0.9553],\n",
            "        [-1.5166,  1.6106],\n",
            "        [-0.8456,  0.4622],\n",
            "        [-0.4298,  0.5581],\n",
            "        [ 1.0986, -1.1359],\n",
            "        [ 0.1554, -0.1012],\n",
            "        [-1.3262,  1.3928],\n",
            "        [-1.5111,  1.5580],\n",
            "        [ 1.0625, -1.2428],\n",
            "        [-1.4651,  1.5216],\n",
            "        [ 0.5846, -1.0443],\n",
            "        [ 1.0803, -1.1125]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6809, -0.8945],\n",
            "        [ 1.0544, -1.0735],\n",
            "        [ 0.3817, -0.9191],\n",
            "        [-0.7537,  0.7696],\n",
            "        [ 1.0632, -1.3544],\n",
            "        [-0.6104,  0.1834],\n",
            "        [ 1.0815, -1.3265],\n",
            "        [ 0.7191, -0.7467],\n",
            "        [ 1.3115, -1.1826],\n",
            "        [-0.1363,  0.1809],\n",
            "        [ 0.9079, -1.0071],\n",
            "        [ 0.7709, -0.9166],\n",
            "        [-1.2916,  1.4407],\n",
            "        [ 1.0079, -0.9694],\n",
            "        [ 1.1582, -1.3323],\n",
            "        [ 1.2431, -1.0388],\n",
            "        [ 0.8900, -0.8689],\n",
            "        [-1.2023,  1.4604],\n",
            "        [ 0.9239, -1.1893],\n",
            "        [ 1.0533, -1.3192],\n",
            "        [-1.3525,  1.3403],\n",
            "        [ 1.0620, -1.2798],\n",
            "        [ 1.1385, -0.9981],\n",
            "        [ 1.1631, -1.3500],\n",
            "        [ 0.6124, -0.9257],\n",
            "        [ 0.8968, -0.9941],\n",
            "        [ 1.1974, -1.1181],\n",
            "        [ 0.7497, -0.8215],\n",
            "        [-1.6476,  1.7656],\n",
            "        [-1.5390,  1.5789],\n",
            "        [ 1.1073, -1.5347],\n",
            "        [ 0.1504, -0.0475]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8940, -0.8189],\n",
            "        [ 1.0921, -1.2625],\n",
            "        [ 0.2089, -0.1684],\n",
            "        [ 1.0953, -1.2504],\n",
            "        [ 0.4994, -0.5117],\n",
            "        [-1.1563,  1.1863],\n",
            "        [-1.2641,  1.3248],\n",
            "        [-0.8874,  1.1423],\n",
            "        [ 1.0204, -0.9123],\n",
            "        [ 0.2963, -0.5455],\n",
            "        [ 0.4646, -0.8299],\n",
            "        [ 0.8877, -1.2730],\n",
            "        [-1.5204,  1.4664],\n",
            "        [ 0.8791, -0.9794],\n",
            "        [-0.0579, -0.3889],\n",
            "        [-0.6494,  0.6165],\n",
            "        [ 1.4259, -1.3553],\n",
            "        [-1.3514,  1.5510],\n",
            "        [ 0.7383, -1.2307],\n",
            "        [ 0.6417, -1.0771],\n",
            "        [ 1.1392, -1.3172],\n",
            "        [ 0.9395, -1.0144],\n",
            "        [ 1.0512, -1.1531],\n",
            "        [-1.4521,  1.4544],\n",
            "        [-0.2626, -0.2660],\n",
            "        [ 1.3529, -1.2365],\n",
            "        [-0.5306,  0.4703],\n",
            "        [-1.3599,  1.4074],\n",
            "        [-1.1367,  1.3838],\n",
            "        [-0.7390,  0.9414],\n",
            "        [-1.3935,  1.4617],\n",
            "        [ 1.1743, -1.2131]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8839, -0.9464],\n",
            "        [-1.6470,  1.4655],\n",
            "        [ 1.0977, -1.4259],\n",
            "        [-1.5302,  1.6598],\n",
            "        [ 0.9265, -1.2331],\n",
            "        [ 0.9096, -1.3913],\n",
            "        [ 1.0194, -1.1692],\n",
            "        [-1.3378,  1.5072],\n",
            "        [ 0.9753, -0.9828],\n",
            "        [ 0.5738, -0.5252],\n",
            "        [ 0.9692, -1.1108],\n",
            "        [-1.2797,  1.4545],\n",
            "        [-0.9814,  1.3518],\n",
            "        [-1.1040,  1.4872],\n",
            "        [-0.3998,  0.0545],\n",
            "        [-1.6539,  1.4037],\n",
            "        [ 1.0072, -1.2314],\n",
            "        [-1.5950,  1.8525],\n",
            "        [ 0.6790, -0.6880],\n",
            "        [ 0.4915, -0.9622],\n",
            "        [ 0.7363, -1.0779],\n",
            "        [ 0.6312, -0.8920],\n",
            "        [ 1.1219, -1.1108],\n",
            "        [ 1.0587, -1.3643],\n",
            "        [-1.3835,  1.4306],\n",
            "        [-1.5803,  1.5574],\n",
            "        [ 0.7528, -0.9985],\n",
            "        [ 0.5347, -0.8868],\n",
            "        [ 1.0521, -1.0611],\n",
            "        [ 0.8424, -0.9392],\n",
            "        [ 0.9472, -1.2045],\n",
            "        [ 0.2239, -0.1038]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4172, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9320, -0.9695],\n",
            "        [-1.3709,  1.5226],\n",
            "        [-1.0078,  1.1630],\n",
            "        [ 0.7832, -1.2067],\n",
            "        [ 0.7976, -0.9088],\n",
            "        [-0.6452,  0.5960],\n",
            "        [ 0.4183, -0.5933],\n",
            "        [ 1.0936, -1.1237],\n",
            "        [-0.9121,  1.2316],\n",
            "        [ 0.3957, -0.6263],\n",
            "        [ 0.9020, -1.0798],\n",
            "        [ 0.1483, -0.5072],\n",
            "        [-1.4781,  1.4782],\n",
            "        [-0.0055,  0.0612],\n",
            "        [ 1.2103, -0.9221],\n",
            "        [ 1.0109, -1.2896],\n",
            "        [ 0.5573, -1.1148],\n",
            "        [ 0.9085, -1.0600],\n",
            "        [-1.4332,  1.5872],\n",
            "        [ 0.8834, -0.6713],\n",
            "        [ 0.7926, -1.2544],\n",
            "        [-1.6565,  1.6366],\n",
            "        [ 0.3894, -0.5489],\n",
            "        [-0.4100,  0.3536],\n",
            "        [ 0.0999, -0.0505],\n",
            "        [ 1.0414, -1.1339],\n",
            "        [ 0.8579, -1.1078],\n",
            "        [ 0.3709, -0.4960],\n",
            "        [ 1.0441, -1.0126],\n",
            "        [ 0.2778, -0.3201],\n",
            "        [-0.0028, -0.2100],\n",
            "        [-0.4723,  0.2357]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7554,  0.6987],\n",
            "        [ 0.6739, -0.9316],\n",
            "        [-1.0921,  1.2602],\n",
            "        [-0.7928,  0.8525],\n",
            "        [ 0.8128, -1.1074],\n",
            "        [ 0.9970, -1.2235],\n",
            "        [ 0.4602, -0.8197],\n",
            "        [-0.8193,  1.0128],\n",
            "        [-1.4392,  1.8114],\n",
            "        [ 0.9307, -0.9695],\n",
            "        [-1.0609,  1.3276],\n",
            "        [ 0.3760, -0.5539],\n",
            "        [ 0.5971, -0.5912],\n",
            "        [ 0.5659, -0.8143],\n",
            "        [ 0.9801, -0.9367],\n",
            "        [-1.6552,  1.4642],\n",
            "        [ 0.6130, -0.7154],\n",
            "        [ 1.0077, -1.0529],\n",
            "        [-1.3853,  1.5182],\n",
            "        [-1.5779,  1.7994],\n",
            "        [ 1.0485, -1.3216],\n",
            "        [ 0.6151, -1.0011],\n",
            "        [-1.3773,  1.4969],\n",
            "        [-1.2999,  1.5819],\n",
            "        [ 0.8196, -1.0834],\n",
            "        [ 0.9361, -1.2242],\n",
            "        [ 1.1739, -1.3043],\n",
            "        [-1.0918,  1.5693],\n",
            "        [-1.5141,  1.6552],\n",
            "        [-0.3929,  0.4757],\n",
            "        [ 0.5884, -1.1481],\n",
            "        [-0.4809,  0.6431]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4382, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0651, -1.0711],\n",
            "        [ 1.1289, -1.2531],\n",
            "        [ 0.5664, -0.6500],\n",
            "        [-0.4805,  0.3708],\n",
            "        [ 0.9351, -1.2376],\n",
            "        [ 0.7585, -0.9205],\n",
            "        [ 0.7128, -1.1064],\n",
            "        [ 1.0874, -1.1046],\n",
            "        [ 0.8139, -1.1885],\n",
            "        [ 0.6735, -0.9125],\n",
            "        [ 0.6219, -0.7517],\n",
            "        [ 0.5017, -0.6519],\n",
            "        [-0.9939,  1.1804],\n",
            "        [ 0.8684, -0.9367],\n",
            "        [ 1.2508, -1.4799],\n",
            "        [ 0.9396, -0.7949],\n",
            "        [ 0.6035, -1.1501],\n",
            "        [ 0.5085, -0.3169],\n",
            "        [ 1.0847, -1.1867],\n",
            "        [-1.0656,  1.3062],\n",
            "        [-0.7896,  0.8015],\n",
            "        [-0.5877,  0.5029],\n",
            "        [ 0.4655, -0.6237],\n",
            "        [-0.6942,  0.9957],\n",
            "        [-1.6508,  1.6515],\n",
            "        [ 1.2362, -1.0688],\n",
            "        [-0.5084,  0.6585],\n",
            "        [-1.0440,  1.1821],\n",
            "        [ 0.9672, -1.0499],\n",
            "        [ 0.7302, -0.9432],\n",
            "        [-1.3610,  1.3179],\n",
            "        [ 0.7896, -0.8065]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5860, -0.5265],\n",
            "        [-0.3755,  0.4244],\n",
            "        [-0.9677,  0.7442],\n",
            "        [ 0.6929, -0.7517],\n",
            "        [-1.1540,  1.5317],\n",
            "        [-1.5072,  1.6302],\n",
            "        [-1.4205,  1.4627],\n",
            "        [-1.3953,  1.4507],\n",
            "        [ 0.8821, -0.8928],\n",
            "        [-0.3087,  0.4509],\n",
            "        [ 0.3306, -0.6016],\n",
            "        [-1.5000,  1.5493],\n",
            "        [ 0.0106,  0.1330],\n",
            "        [-1.3953,  1.5760],\n",
            "        [-1.6269,  1.7240],\n",
            "        [ 0.7085, -0.7972],\n",
            "        [-0.1463,  0.1647],\n",
            "        [ 0.8323, -0.9063],\n",
            "        [ 0.8976, -1.2621],\n",
            "        [ 1.0845, -1.3917],\n",
            "        [ 0.9225, -1.2268],\n",
            "        [ 0.5626, -0.4829],\n",
            "        [ 0.8987, -1.3251],\n",
            "        [-1.5105,  1.5441],\n",
            "        [-1.3491,  1.5345],\n",
            "        [ 0.9932, -1.0715],\n",
            "        [ 0.7260, -0.9715],\n",
            "        [ 1.1719, -1.3819],\n",
            "        [-1.6250,  1.7590],\n",
            "        [-1.3377,  1.3641],\n",
            "        [ 0.6004, -0.8301],\n",
            "        [ 0.3375, -0.4531]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0106, -1.1544],\n",
            "        [ 1.0448, -0.9828],\n",
            "        [ 1.1413, -1.1640],\n",
            "        [ 1.1777, -1.2158],\n",
            "        [ 0.7334, -0.8676],\n",
            "        [ 1.0394, -1.2718],\n",
            "        [-1.1113,  1.2276],\n",
            "        [ 1.1194, -1.3934],\n",
            "        [ 0.9733, -1.2618],\n",
            "        [ 0.4111, -0.4725],\n",
            "        [ 0.7484, -0.9216],\n",
            "        [ 0.9373, -0.9925],\n",
            "        [ 1.1845, -1.2941],\n",
            "        [-0.3676,  0.3054],\n",
            "        [ 0.9334, -1.2826],\n",
            "        [-1.5204,  1.5188],\n",
            "        [ 1.0948, -1.1894],\n",
            "        [ 0.8917, -1.0035],\n",
            "        [ 0.7188, -1.1451],\n",
            "        [ 0.7588, -1.2821],\n",
            "        [-0.7438,  1.0071],\n",
            "        [ 1.0782, -1.1566],\n",
            "        [ 1.0450, -1.2065],\n",
            "        [-0.4625,  0.7235],\n",
            "        [ 0.4629, -1.0715],\n",
            "        [ 1.0465, -0.8957],\n",
            "        [ 0.2353, -0.5260],\n",
            "        [-0.8699,  0.7309],\n",
            "        [ 0.2322, -0.4417],\n",
            "        [ 0.7533, -1.0119],\n",
            "        [ 0.7116, -1.0309],\n",
            "        [ 0.7709, -1.1422]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.2680,  1.2650],\n",
            "        [ 1.0964, -1.1109],\n",
            "        [ 0.2730, -0.4052],\n",
            "        [ 0.9301, -1.2006],\n",
            "        [-1.0559,  1.1335],\n",
            "        [ 0.9704, -1.2029],\n",
            "        [-1.5517,  1.6853],\n",
            "        [-1.3632,  1.4069],\n",
            "        [ 0.5836, -1.0647],\n",
            "        [-1.2121,  1.2178],\n",
            "        [ 1.2169, -1.6370],\n",
            "        [ 0.9312, -1.1483],\n",
            "        [-0.9629,  0.9416],\n",
            "        [-1.4606,  1.4747],\n",
            "        [ 0.9861, -1.0821],\n",
            "        [ 0.9378, -1.2483],\n",
            "        [ 0.9731, -1.1904],\n",
            "        [-1.4035,  1.6385],\n",
            "        [ 0.7606, -1.1090],\n",
            "        [ 1.0591, -1.0484],\n",
            "        [ 0.4808, -1.0292],\n",
            "        [ 1.2289, -1.3893],\n",
            "        [ 1.0008, -1.0063],\n",
            "        [ 0.9049, -0.8090],\n",
            "        [-0.1683, -0.0826],\n",
            "        [-1.3220,  1.5203],\n",
            "        [-0.9715,  1.1551],\n",
            "        [-1.1685,  1.3385],\n",
            "        [ 1.2201, -1.2836],\n",
            "        [ 0.7359, -1.0999],\n",
            "        [ 0.5835, -1.0387],\n",
            "        [-1.4049,  1.3337]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4854, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8033, -1.2974],\n",
            "        [ 1.0515, -1.4336],\n",
            "        [ 0.6083, -0.8251],\n",
            "        [ 0.2923, -0.4358],\n",
            "        [ 1.1449, -1.4827],\n",
            "        [ 0.5547, -0.9035],\n",
            "        [-1.4885,  1.3505],\n",
            "        [ 1.0465, -1.2183],\n",
            "        [ 0.8936, -1.1988],\n",
            "        [ 0.7878, -1.0409],\n",
            "        [ 0.9796, -1.2811],\n",
            "        [-1.5920,  1.4421],\n",
            "        [-1.3962,  1.5420],\n",
            "        [ 0.4063, -0.4534],\n",
            "        [ 0.2049, -0.1032],\n",
            "        [-1.6831,  1.4950],\n",
            "        [ 0.8832, -1.3992],\n",
            "        [ 1.1857, -1.2962],\n",
            "        [ 0.8424, -1.2192],\n",
            "        [-1.3684,  1.2225],\n",
            "        [ 0.8680, -1.1961],\n",
            "        [-1.7792,  1.8019],\n",
            "        [ 0.6188, -1.1203],\n",
            "        [ 0.8336, -1.2432],\n",
            "        [ 1.1110, -1.3883],\n",
            "        [ 1.0040, -1.2283],\n",
            "        [-0.1605, -0.1820],\n",
            "        [ 0.9450, -0.9569],\n",
            "        [ 1.0880, -1.1359],\n",
            "        [ 0.8685, -1.0908],\n",
            "        [-0.2167,  0.1102],\n",
            "        [ 0.8424, -1.1841]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8271, -0.9949],\n",
            "        [-1.2985,  1.5067],\n",
            "        [-1.3215,  1.3257],\n",
            "        [-0.1842, -0.2507],\n",
            "        [ 0.5264, -0.8768],\n",
            "        [-1.1876,  1.3728],\n",
            "        [-1.4427,  1.1945],\n",
            "        [-1.6440,  1.4870],\n",
            "        [-1.4274,  1.6781],\n",
            "        [-0.5431,  0.4360],\n",
            "        [ 0.7698, -0.7729],\n",
            "        [-0.8618,  0.5442],\n",
            "        [-1.3905,  1.2946],\n",
            "        [-1.6139,  1.8342],\n",
            "        [ 0.8984, -0.9308],\n",
            "        [-0.8559,  1.1775],\n",
            "        [-0.4250,  0.7082],\n",
            "        [-1.1057,  1.1407],\n",
            "        [ 0.0371, -0.0070],\n",
            "        [ 1.2567, -1.3085],\n",
            "        [-0.7384,  0.6713],\n",
            "        [ 0.9072, -1.2072],\n",
            "        [-0.5345,  0.7434],\n",
            "        [-0.3229, -0.2784],\n",
            "        [ 0.7653, -1.0328],\n",
            "        [ 1.1945, -1.3708],\n",
            "        [ 0.5697, -1.3721],\n",
            "        [-1.1617,  1.2485],\n",
            "        [ 0.7861, -0.9641],\n",
            "        [ 0.6710, -0.8737],\n",
            "        [-1.6845,  1.4305],\n",
            "        [-1.3026,  1.5723]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9607, -1.1482],\n",
            "        [ 0.9702, -1.6180],\n",
            "        [-1.3100,  1.4730],\n",
            "        [ 0.4807,  0.0054],\n",
            "        [-1.5401,  1.7489],\n",
            "        [ 0.9212, -1.1482],\n",
            "        [ 1.0546, -1.1321],\n",
            "        [ 0.9575, -0.9928],\n",
            "        [-0.3666,  0.1917],\n",
            "        [-1.3323,  1.5465],\n",
            "        [-1.4472,  1.3843],\n",
            "        [-1.4499,  1.4451],\n",
            "        [-1.5640,  1.7575],\n",
            "        [-0.6550,  0.6534],\n",
            "        [ 1.1602, -1.1865],\n",
            "        [ 0.1048, -0.0381],\n",
            "        [ 0.9470, -0.9982],\n",
            "        [-0.2996,  0.2581],\n",
            "        [ 0.6684, -0.9698],\n",
            "        [ 1.0611, -1.3260],\n",
            "        [-1.3446,  1.6436],\n",
            "        [ 0.7115, -0.9820],\n",
            "        [ 0.1130, -0.4313],\n",
            "        [ 0.0059, -0.2243],\n",
            "        [-1.4429,  1.2962],\n",
            "        [ 0.2051, -0.0831],\n",
            "        [-1.1179,  1.0912],\n",
            "        [-0.3406,  0.4440],\n",
            "        [ 0.5413, -0.3776],\n",
            "        [ 0.9927, -1.1871],\n",
            "        [-1.4891,  1.9537],\n",
            "        [ 0.7945, -1.0497]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5040, -1.1031],\n",
            "        [-1.3884,  1.6483],\n",
            "        [-1.0356,  1.0759],\n",
            "        [ 0.8487, -0.9043],\n",
            "        [-1.3044,  1.5074],\n",
            "        [ 1.1365, -1.4034],\n",
            "        [ 0.7172, -0.9939],\n",
            "        [-1.3230,  1.3898],\n",
            "        [-0.0710, -0.1611],\n",
            "        [ 0.8518, -1.4883],\n",
            "        [ 0.7831, -1.0086],\n",
            "        [-1.2591,  1.6404],\n",
            "        [ 1.1234, -1.0813],\n",
            "        [ 1.0157, -1.1772],\n",
            "        [-1.7063,  1.8456],\n",
            "        [-0.3159,  0.0235],\n",
            "        [-0.4548,  0.2862],\n",
            "        [ 0.7217, -0.9894],\n",
            "        [ 0.9819, -0.7972],\n",
            "        [ 0.0736, -0.2794],\n",
            "        [-1.3774,  1.6911],\n",
            "        [-1.3625,  1.4983],\n",
            "        [ 0.8384, -1.2113],\n",
            "        [ 1.0431, -1.2919],\n",
            "        [ 0.9116, -0.8834],\n",
            "        [-1.5467,  1.7819],\n",
            "        [ 0.4267, -0.6250],\n",
            "        [ 0.9426, -1.0786],\n",
            "        [ 0.9513, -1.0682],\n",
            "        [ 0.9473, -1.3878],\n",
            "        [ 0.2200, -0.3151],\n",
            "        [ 1.1061, -1.0132]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0082, -0.3785],\n",
            "        [ 0.7150, -0.9647],\n",
            "        [ 0.9396, -1.0718],\n",
            "        [ 0.8223, -1.2591],\n",
            "        [-0.3073,  0.0464],\n",
            "        [ 0.4192, -0.7521],\n",
            "        [ 0.7194, -1.1570],\n",
            "        [-1.3681,  1.2889],\n",
            "        [-0.4160,  0.4437],\n",
            "        [ 1.1156, -0.9667],\n",
            "        [ 0.3593, -0.6948],\n",
            "        [-1.3527,  1.4164],\n",
            "        [ 0.9355, -1.0698],\n",
            "        [ 1.2721, -1.2150],\n",
            "        [-1.2741,  1.5477],\n",
            "        [-0.8891,  0.9939],\n",
            "        [-1.4781,  1.6149],\n",
            "        [ 0.9393, -1.0038],\n",
            "        [-1.4310,  1.5015],\n",
            "        [ 0.3925, -0.4575],\n",
            "        [ 1.1471, -0.9185],\n",
            "        [-0.3387,  0.6237],\n",
            "        [ 0.9411, -0.9700],\n",
            "        [-1.0992,  1.5382],\n",
            "        [ 1.0334, -1.1597],\n",
            "        [ 0.4525, -0.6229],\n",
            "        [-0.2273, -0.2440],\n",
            "        [ 0.8945, -1.2600],\n",
            "        [ 0.8484, -0.9521],\n",
            "        [-1.7573,  1.8123],\n",
            "        [ 0.8745, -1.0790],\n",
            "        [-1.1237,  1.7390]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7431, -0.7158],\n",
            "        [-0.3846,  0.4797],\n",
            "        [ 1.0321, -1.0608],\n",
            "        [ 0.6267, -0.8336],\n",
            "        [ 0.9756, -1.1018],\n",
            "        [ 0.5405, -0.8673],\n",
            "        [-1.1903,  1.2009],\n",
            "        [ 1.0438, -1.3197],\n",
            "        [ 0.9195, -0.7015],\n",
            "        [-1.4813,  1.4336],\n",
            "        [-1.4613,  1.3174],\n",
            "        [ 1.0399, -0.9843],\n",
            "        [-0.8656,  0.5387],\n",
            "        [-1.6804,  1.6310],\n",
            "        [ 0.9709, -1.0599],\n",
            "        [-1.2652,  1.5487],\n",
            "        [-1.4544,  1.3670],\n",
            "        [ 1.1338, -1.2528],\n",
            "        [-1.2840,  1.5475],\n",
            "        [ 1.3897, -1.4053],\n",
            "        [ 0.9312, -1.1755],\n",
            "        [ 0.9217, -1.3648],\n",
            "        [-0.4679,  0.4382],\n",
            "        [ 0.4042, -0.8217],\n",
            "        [ 1.1473, -1.3764],\n",
            "        [-1.4963,  1.4244],\n",
            "        [-1.0002,  1.0791],\n",
            "        [ 0.8878, -0.9503],\n",
            "        [-0.0252,  0.1118],\n",
            "        [ 1.0663, -1.2080],\n",
            "        [ 0.6914, -0.9395],\n",
            "        [-1.5703,  1.6663]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7102, -1.0467],\n",
            "        [ 1.1783, -1.0153],\n",
            "        [ 0.9250, -1.0844],\n",
            "        [ 0.8442, -0.9339],\n",
            "        [-0.9442,  0.9659],\n",
            "        [-1.5734,  1.5710],\n",
            "        [ 1.0528, -1.2929],\n",
            "        [-0.7313,  1.0669],\n",
            "        [-0.7649,  1.0431],\n",
            "        [-0.3956, -0.1909],\n",
            "        [ 1.1507, -1.4991],\n",
            "        [ 0.5665, -0.9453],\n",
            "        [ 0.9258, -1.0148],\n",
            "        [ 0.9847, -1.0046],\n",
            "        [-0.7665,  0.9565],\n",
            "        [-1.5368,  1.4697],\n",
            "        [ 0.4253, -0.7991],\n",
            "        [-0.7850,  1.0133],\n",
            "        [ 0.1811, -0.2275],\n",
            "        [ 0.2721, -0.4526],\n",
            "        [ 0.7181, -1.0399],\n",
            "        [ 0.9985, -1.3564],\n",
            "        [ 0.7173, -1.1952],\n",
            "        [ 0.8542, -1.1995],\n",
            "        [ 0.4355, -1.0217],\n",
            "        [ 0.1899, -0.3793],\n",
            "        [ 0.7127, -1.0944],\n",
            "        [-1.4472,  1.5525],\n",
            "        [-1.2550,  1.6876],\n",
            "        [ 0.5968, -0.7334],\n",
            "        [-1.1007,  1.1690],\n",
            "        [-0.9036,  0.9136]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9065, -1.2716],\n",
            "        [-1.1980,  1.1160],\n",
            "        [ 0.9651, -1.1780],\n",
            "        [ 0.9520, -1.0523],\n",
            "        [ 1.0503, -1.1955],\n",
            "        [ 0.3686, -0.6703],\n",
            "        [ 1.0026, -1.1259],\n",
            "        [ 0.8225, -1.1431],\n",
            "        [ 0.9417, -1.0765],\n",
            "        [ 0.3174, -0.4970],\n",
            "        [-1.6025,  1.4511],\n",
            "        [ 1.1651, -1.2816],\n",
            "        [ 0.9843, -1.3487],\n",
            "        [ 0.2224, -0.4582],\n",
            "        [ 0.3935, -0.8485],\n",
            "        [ 1.1185, -1.3306],\n",
            "        [-1.5315,  1.5895],\n",
            "        [ 0.8483, -0.9407],\n",
            "        [ 0.2300, -0.6899],\n",
            "        [ 1.1663, -1.1379],\n",
            "        [ 1.0856, -1.1477],\n",
            "        [-1.4982,  1.5331],\n",
            "        [ 0.8863, -1.2168],\n",
            "        [ 1.1314, -1.2389],\n",
            "        [-0.2526,  0.3051],\n",
            "        [ 1.0871, -1.0220],\n",
            "        [ 0.9374, -0.8357],\n",
            "        [ 0.6413, -0.9654],\n",
            "        [-0.5812,  0.5816],\n",
            "        [-1.3085,  1.5223],\n",
            "        [ 1.2812, -1.1671],\n",
            "        [-1.5198,  1.4554]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch   100  of    191.    Elapsed: 0:02:18.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4672, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4199,  1.2968],\n",
            "        [ 0.2288, -0.3815],\n",
            "        [-1.4242,  1.9004],\n",
            "        [ 0.9121, -0.8557],\n",
            "        [-1.6122,  1.4445],\n",
            "        [ 0.7006, -1.2225],\n",
            "        [-1.6287,  1.7048],\n",
            "        [-0.6066,  0.8354],\n",
            "        [-1.3757,  1.6817],\n",
            "        [ 1.0922, -1.1977],\n",
            "        [ 0.6993, -1.2258],\n",
            "        [-1.3934,  1.3978],\n",
            "        [ 0.9960, -1.0135],\n",
            "        [ 0.9305, -1.0217],\n",
            "        [-1.6489,  1.6441],\n",
            "        [-1.4747,  1.4967],\n",
            "        [ 1.1264, -1.0785],\n",
            "        [-1.6449,  1.5305],\n",
            "        [ 0.1545, -0.1981],\n",
            "        [-1.1632,  1.7258],\n",
            "        [-1.5518,  1.5416],\n",
            "        [ 1.2350, -1.2659],\n",
            "        [ 0.9152, -1.1475],\n",
            "        [ 0.3208, -0.7541],\n",
            "        [ 1.0180, -1.4764],\n",
            "        [-1.4008,  1.4913],\n",
            "        [ 0.8202, -1.0202],\n",
            "        [ 0.8477, -1.1585],\n",
            "        [ 0.6175, -0.9234],\n",
            "        [ 0.5856, -1.0939],\n",
            "        [ 0.7130, -0.7540],\n",
            "        [-1.3211,  0.9927]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4931, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3998e+00,  1.3784e+00],\n",
            "        [-1.3698e+00,  1.3481e+00],\n",
            "        [ 1.1324e+00, -1.3212e+00],\n",
            "        [-1.4481e+00,  1.5587e+00],\n",
            "        [ 5.8350e-01, -1.1320e+00],\n",
            "        [ 5.9691e-01, -1.1312e+00],\n",
            "        [ 8.7923e-01, -9.3934e-01],\n",
            "        [ 1.0070e+00, -9.4227e-01],\n",
            "        [ 6.8419e-01, -7.9204e-01],\n",
            "        [ 1.0605e+00, -1.2055e+00],\n",
            "        [ 1.3376e-01, -1.1205e-01],\n",
            "        [ 1.7458e-01, -3.8692e-01],\n",
            "        [ 1.0748e+00, -9.9564e-01],\n",
            "        [-1.1400e-03,  4.0672e-02],\n",
            "        [ 9.1791e-01, -1.1916e+00],\n",
            "        [ 9.6440e-01, -1.1422e+00],\n",
            "        [ 9.0288e-01, -1.2321e+00],\n",
            "        [ 1.0178e+00, -1.1790e+00],\n",
            "        [ 1.1325e+00, -1.1045e+00],\n",
            "        [ 8.3097e-01, -1.0820e+00],\n",
            "        [ 5.8677e-01, -9.3349e-01],\n",
            "        [ 8.9768e-01, -9.1346e-01],\n",
            "        [ 1.1048e+00, -1.1597e+00],\n",
            "        [-7.5851e-01,  1.0492e+00],\n",
            "        [ 7.9983e-01, -1.0529e+00],\n",
            "        [ 9.3970e-01, -1.2162e+00],\n",
            "        [ 9.0127e-01, -8.5580e-01],\n",
            "        [ 8.6800e-01, -1.2396e+00],\n",
            "        [-1.3376e+00,  1.4293e+00],\n",
            "        [ 7.6912e-01, -7.1214e-01],\n",
            "        [ 7.5552e-01, -1.1050e+00],\n",
            "        [-1.3681e+00,  1.5854e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4834, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2347, -1.4922],\n",
            "        [-1.3741,  1.7629],\n",
            "        [-1.2748,  1.0752],\n",
            "        [ 0.8796, -0.8653],\n",
            "        [-1.4279,  1.5563],\n",
            "        [-0.9041,  0.8462],\n",
            "        [ 0.9036, -1.1289],\n",
            "        [ 1.1777, -1.0501],\n",
            "        [ 0.8861, -0.9964],\n",
            "        [ 0.5661, -0.8956],\n",
            "        [-0.8690,  0.6901],\n",
            "        [ 0.9031, -0.9016],\n",
            "        [ 0.7287, -0.8151],\n",
            "        [ 0.9642, -0.9495],\n",
            "        [-1.6154,  1.5071],\n",
            "        [ 0.1533, -0.6245],\n",
            "        [ 0.6200, -0.8121],\n",
            "        [-0.9191,  1.0231],\n",
            "        [ 0.8112, -1.2155],\n",
            "        [-1.3906,  1.3742],\n",
            "        [ 0.9846, -1.1490],\n",
            "        [-1.3012,  1.5103],\n",
            "        [ 0.4345, -0.9949],\n",
            "        [-1.3751,  1.2332],\n",
            "        [ 0.8231, -1.2864],\n",
            "        [ 0.8964, -0.8197],\n",
            "        [ 0.7679, -0.8142],\n",
            "        [ 0.7383, -0.9689],\n",
            "        [ 1.0005, -1.4150],\n",
            "        [ 0.8606, -1.1626],\n",
            "        [-1.4144,  1.2911],\n",
            "        [ 0.8593, -1.4020]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1462e-03,  2.0221e-01],\n",
            "        [-1.4100e+00,  1.7616e+00],\n",
            "        [ 8.4933e-01, -1.1058e+00],\n",
            "        [ 2.0178e-01, -2.1297e-01],\n",
            "        [ 3.5729e-01, -6.5822e-01],\n",
            "        [ 8.0187e-01, -1.2687e+00],\n",
            "        [ 8.4675e-01, -8.6142e-01],\n",
            "        [ 9.2286e-01, -8.5173e-01],\n",
            "        [ 3.9378e-01, -1.5421e-01],\n",
            "        [-3.9030e-01,  3.0396e-01],\n",
            "        [ 3.6166e-01, -2.2563e-01],\n",
            "        [-1.2532e+00,  1.4928e+00],\n",
            "        [-1.0180e+00,  9.5920e-01],\n",
            "        [-1.1954e+00,  1.4673e+00],\n",
            "        [ 4.3293e-01, -5.3069e-01],\n",
            "        [ 8.3434e-02, -1.2424e-01],\n",
            "        [-7.5926e-01,  5.7083e-01],\n",
            "        [ 6.3648e-01, -8.4636e-01],\n",
            "        [ 9.6345e-01, -1.3358e+00],\n",
            "        [ 1.0284e+00, -1.2068e+00],\n",
            "        [-8.5387e-01,  9.4625e-01],\n",
            "        [-1.2162e+00,  1.3591e+00],\n",
            "        [-8.3327e-01,  9.6143e-01],\n",
            "        [ 8.5983e-01, -1.0076e+00],\n",
            "        [ 1.0141e+00, -8.0889e-01],\n",
            "        [-1.2454e+00,  1.4736e+00],\n",
            "        [-1.3336e+00,  1.5636e+00],\n",
            "        [ 3.4621e-01, -3.7712e-01],\n",
            "        [-1.5551e+00,  1.5351e+00],\n",
            "        [ 6.8975e-01, -9.3177e-01],\n",
            "        [-1.2108e+00,  1.6001e+00],\n",
            "        [ 1.0641e+00, -1.2025e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3459, -1.2104],\n",
            "        [-1.4028,  1.4442],\n",
            "        [-1.7056,  1.7648],\n",
            "        [ 0.8579, -0.8839],\n",
            "        [-0.7829,  0.5262],\n",
            "        [ 0.3325, -0.2643],\n",
            "        [ 0.3885, -0.7952],\n",
            "        [-1.6916,  1.6782],\n",
            "        [ 0.9431, -1.0190],\n",
            "        [-1.5239,  1.8118],\n",
            "        [ 0.3379, -0.7714],\n",
            "        [ 0.2980, -0.2136],\n",
            "        [-0.0321,  0.2485],\n",
            "        [ 0.8604, -1.0453],\n",
            "        [ 1.0248, -1.0952],\n",
            "        [ 0.5004, -0.8131],\n",
            "        [-0.3131,  0.2386],\n",
            "        [ 0.3403, -0.5789],\n",
            "        [-0.8698,  1.0743],\n",
            "        [-1.6534,  1.5324],\n",
            "        [-1.3402,  1.5889],\n",
            "        [ 0.2268, -0.5172],\n",
            "        [ 0.5487, -0.9684],\n",
            "        [-1.0490,  1.1865],\n",
            "        [ 0.6086, -1.0997],\n",
            "        [ 0.5662, -0.8320],\n",
            "        [ 0.5576, -0.9527],\n",
            "        [ 1.0951, -1.2408],\n",
            "        [-1.4504,  1.4780],\n",
            "        [-0.8959,  0.9539],\n",
            "        [ 0.7577, -1.1747],\n",
            "        [ 1.1409, -1.0200]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4346,  1.5926],\n",
            "        [ 0.4495, -0.5034],\n",
            "        [ 0.8723, -1.1920],\n",
            "        [-1.6402,  1.5333],\n",
            "        [ 0.6662, -0.9171],\n",
            "        [ 0.9728, -1.2856],\n",
            "        [ 0.6572, -1.0591],\n",
            "        [-0.7422,  0.4097],\n",
            "        [ 0.1997, -0.4487],\n",
            "        [-0.9450,  0.8924],\n",
            "        [-1.1716,  1.5290],\n",
            "        [ 0.0743, -0.0277],\n",
            "        [ 0.7416, -0.8013],\n",
            "        [ 1.0172, -1.2896],\n",
            "        [ 0.7800, -1.0516],\n",
            "        [ 0.0850, -0.3610],\n",
            "        [-1.1695,  1.0118],\n",
            "        [-0.6358,  0.8326],\n",
            "        [ 0.7048, -1.0208],\n",
            "        [ 1.2052, -1.2054],\n",
            "        [ 0.5652, -0.9664],\n",
            "        [ 0.3248, -0.4394],\n",
            "        [ 0.7741, -0.7125],\n",
            "        [ 0.5206, -1.0029],\n",
            "        [ 1.1701, -1.3414],\n",
            "        [-1.5170,  1.7849],\n",
            "        [ 0.8323, -1.2481],\n",
            "        [ 0.7292, -0.9163],\n",
            "        [-1.1896,  1.5033],\n",
            "        [ 1.0757, -1.1454],\n",
            "        [ 0.7868, -0.8399],\n",
            "        [-0.8524,  1.0561]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6879, -1.2654],\n",
            "        [-1.5380,  1.4000],\n",
            "        [ 0.7340, -1.0022],\n",
            "        [-1.6505,  1.6255],\n",
            "        [ 0.8419, -0.6920],\n",
            "        [ 1.0574, -1.3592],\n",
            "        [ 1.0893, -1.1023],\n",
            "        [ 0.6668, -0.9128],\n",
            "        [-1.6978,  1.6825],\n",
            "        [ 0.7168, -1.3309],\n",
            "        [-0.6698,  0.8378],\n",
            "        [-1.2478,  1.1338],\n",
            "        [-0.5550,  0.5310],\n",
            "        [-1.2988,  1.2186],\n",
            "        [ 0.2719, -0.5250],\n",
            "        [-1.1421,  1.2915],\n",
            "        [ 1.0810, -1.0661],\n",
            "        [-1.3956,  1.2852],\n",
            "        [ 0.2780, -0.4055],\n",
            "        [-1.4093,  1.5501],\n",
            "        [ 0.9542, -1.1610],\n",
            "        [ 0.2760, -0.7977],\n",
            "        [ 0.8927, -1.2047],\n",
            "        [ 0.5592, -0.9221],\n",
            "        [ 0.9927, -1.3009],\n",
            "        [ 0.9222, -1.1306],\n",
            "        [ 1.0290, -1.0155],\n",
            "        [-1.3951,  1.6028],\n",
            "        [ 0.5271, -0.9977],\n",
            "        [ 0.8084, -1.1417],\n",
            "        [ 0.9771, -1.4103],\n",
            "        [-1.3115,  1.5380]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.7344,  1.5183],\n",
            "        [-0.9157,  0.9788],\n",
            "        [-1.4870,  1.3819],\n",
            "        [ 0.9437, -1.4147],\n",
            "        [ 0.5077, -0.7210],\n",
            "        [ 0.8589, -1.2635],\n",
            "        [-0.9568,  1.1026],\n",
            "        [-0.0598, -0.2486],\n",
            "        [ 0.9971, -1.1459],\n",
            "        [-0.4181,  0.2666],\n",
            "        [-1.6240,  1.5001],\n",
            "        [-1.3907,  1.4993],\n",
            "        [ 1.2464, -1.1189],\n",
            "        [-1.2312,  1.4670],\n",
            "        [ 0.8651, -1.4744],\n",
            "        [ 0.4394, -1.2260],\n",
            "        [-0.0500, -0.1002],\n",
            "        [ 0.6251, -0.9184],\n",
            "        [ 1.0347, -1.3186],\n",
            "        [ 0.5930, -0.8260],\n",
            "        [ 1.0072, -1.4106],\n",
            "        [-1.5279,  1.5887],\n",
            "        [-1.3488,  1.6359],\n",
            "        [ 0.7698, -1.3821],\n",
            "        [ 1.1790, -1.3466],\n",
            "        [ 0.7568, -1.2485],\n",
            "        [-0.0682, -0.1201],\n",
            "        [-0.2383,  0.3453],\n",
            "        [-0.7298,  0.6168],\n",
            "        [-1.2975,  1.5098],\n",
            "        [ 0.6513, -0.8717],\n",
            "        [-0.5947,  0.5638]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2850, -0.1502],\n",
            "        [-1.5087,  1.4128],\n",
            "        [ 1.0035, -1.1308],\n",
            "        [ 1.0134, -1.2273],\n",
            "        [-1.6202,  1.5873],\n",
            "        [-0.8478,  0.7556],\n",
            "        [-0.2573,  0.3763],\n",
            "        [ 0.6253, -0.9274],\n",
            "        [ 0.8477, -0.9201],\n",
            "        [-0.2083, -0.0646],\n",
            "        [ 0.7262, -1.1072],\n",
            "        [ 0.6602, -1.0181],\n",
            "        [ 1.1930, -1.1951],\n",
            "        [-0.4450,  0.4330],\n",
            "        [ 0.8625, -1.1642],\n",
            "        [ 0.6045, -0.9610],\n",
            "        [ 1.0432, -1.2517],\n",
            "        [-1.4193,  1.6481],\n",
            "        [ 0.8644, -1.3068],\n",
            "        [ 0.4299, -0.5554],\n",
            "        [-1.5360,  1.3325],\n",
            "        [ 0.6794, -1.3879],\n",
            "        [-0.7943,  0.6125],\n",
            "        [ 0.9757, -1.0636],\n",
            "        [-1.0016,  1.3012],\n",
            "        [ 0.7479, -0.7613],\n",
            "        [ 1.0691, -1.1033],\n",
            "        [-1.1687,  1.5896],\n",
            "        [ 0.0292, -0.7159],\n",
            "        [-0.9880,  1.0590],\n",
            "        [-0.0575, -0.3342],\n",
            "        [-1.4399,  1.4859]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6544, -0.6374],\n",
            "        [ 0.6153, -1.1186],\n",
            "        [ 0.0582, -0.2204],\n",
            "        [-1.5344,  1.5041],\n",
            "        [ 1.1050, -1.0958],\n",
            "        [ 1.0158, -1.3106],\n",
            "        [ 0.7899, -1.1241],\n",
            "        [ 0.9800, -1.1384],\n",
            "        [-0.3798, -0.0535],\n",
            "        [ 0.8354, -1.1756],\n",
            "        [ 0.3766, -0.5683],\n",
            "        [ 0.3229, -0.5656],\n",
            "        [-0.1586, -0.3925],\n",
            "        [ 1.0111, -0.9895],\n",
            "        [ 0.9860, -1.3768],\n",
            "        [ 0.9901, -1.1624],\n",
            "        [-0.4580,  0.7039],\n",
            "        [ 1.1495, -1.1889],\n",
            "        [ 0.6517, -0.8920],\n",
            "        [-1.4111,  1.5265],\n",
            "        [ 0.6237, -0.9414],\n",
            "        [ 0.8115, -1.0180],\n",
            "        [ 0.8749, -1.2681],\n",
            "        [-1.5130,  1.5386],\n",
            "        [-0.2491, -0.1132],\n",
            "        [ 1.2139, -1.3017],\n",
            "        [-1.7447,  1.5792],\n",
            "        [-1.3297,  1.2857],\n",
            "        [-1.6409,  1.5223],\n",
            "        [ 0.9195, -1.0814],\n",
            "        [-1.4826,  1.6792],\n",
            "        [-1.5616,  1.5314]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2913, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1743, -1.3438],\n",
            "        [-1.0050,  1.1368],\n",
            "        [ 0.6452, -1.0442],\n",
            "        [-1.0214,  1.0201],\n",
            "        [ 0.5229, -0.7091],\n",
            "        [ 0.6885, -1.3437],\n",
            "        [-1.4973,  1.7248],\n",
            "        [ 0.8663, -0.9437],\n",
            "        [ 1.1515, -1.0729],\n",
            "        [ 0.7516, -0.6024],\n",
            "        [-0.1176, -0.2132],\n",
            "        [ 1.0664, -1.0116],\n",
            "        [ 1.0975, -1.2918],\n",
            "        [ 0.7321, -1.1374],\n",
            "        [ 1.1200, -1.0278],\n",
            "        [ 1.0246, -1.1454],\n",
            "        [-1.2223,  1.1855],\n",
            "        [-1.6266,  1.5446],\n",
            "        [ 0.9796, -1.1952],\n",
            "        [ 0.5953, -0.6357],\n",
            "        [-1.2978,  1.4984],\n",
            "        [ 0.7241, -0.8291],\n",
            "        [ 0.9660, -1.3579],\n",
            "        [-1.4744,  1.3922],\n",
            "        [-1.3466,  1.4877],\n",
            "        [ 0.6772, -1.2048],\n",
            "        [ 0.7143, -1.3889],\n",
            "        [-1.5595,  1.7444],\n",
            "        [ 0.9452, -1.2970],\n",
            "        [ 0.8037, -1.4079],\n",
            "        [ 0.8188, -0.9964],\n",
            "        [-1.5089,  1.6725]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9398, -1.0171],\n",
            "        [-1.5372,  1.7519],\n",
            "        [-0.1453,  0.1548],\n",
            "        [-1.5525,  1.5322],\n",
            "        [-1.3261,  1.6760],\n",
            "        [ 0.9501, -1.1356],\n",
            "        [-1.2134,  1.4319],\n",
            "        [-1.2316,  1.1726],\n",
            "        [-1.2635,  1.2709],\n",
            "        [-1.0958,  1.6628],\n",
            "        [ 0.9086, -1.6164],\n",
            "        [-1.2619,  1.5648],\n",
            "        [ 0.0982, -0.1519],\n",
            "        [-1.4507,  1.3950],\n",
            "        [ 0.3289, -0.7444],\n",
            "        [-1.2289,  1.1677],\n",
            "        [ 0.5891, -1.0663],\n",
            "        [ 0.1239, -0.6442],\n",
            "        [-1.5551,  1.7245],\n",
            "        [-1.6363,  1.7602],\n",
            "        [ 0.5415, -1.0397],\n",
            "        [-0.5909,  0.5662],\n",
            "        [-1.0513,  1.1002],\n",
            "        [ 0.9648, -1.2821],\n",
            "        [ 1.0720, -1.0859],\n",
            "        [ 0.3392, -1.0655],\n",
            "        [-0.0933,  0.2587],\n",
            "        [-1.4242,  1.7393],\n",
            "        [ 0.2798, -0.6220],\n",
            "        [ 0.7551, -1.4091],\n",
            "        [ 1.0152, -0.8564],\n",
            "        [-1.3841,  1.3610]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5346,  1.5369],\n",
            "        [-1.4432,  1.5832],\n",
            "        [ 0.9341, -0.9184],\n",
            "        [ 0.8745, -0.8982],\n",
            "        [-1.4598,  1.4433],\n",
            "        [-0.5953,  0.7174],\n",
            "        [ 0.8229, -0.9949],\n",
            "        [ 0.9448, -1.0274],\n",
            "        [ 0.4145, -0.8044],\n",
            "        [ 1.0849, -1.0909],\n",
            "        [ 0.9473, -1.3904],\n",
            "        [-1.5063,  1.4545],\n",
            "        [-0.1219,  0.3956],\n",
            "        [-1.4917,  1.5202],\n",
            "        [ 0.4782, -0.7542],\n",
            "        [ 0.8928, -0.9634],\n",
            "        [ 0.7910, -1.3580],\n",
            "        [ 1.3279, -1.4707],\n",
            "        [ 1.2164, -1.1446],\n",
            "        [ 0.9528, -1.3976],\n",
            "        [-1.2397,  1.2821],\n",
            "        [ 0.5099, -0.8559],\n",
            "        [ 0.8511, -0.9137],\n",
            "        [ 0.7036, -1.0900],\n",
            "        [ 0.9958, -1.1703],\n",
            "        [-1.2806,  1.3943],\n",
            "        [-1.4294,  1.5268],\n",
            "        [-0.6012,  0.4481],\n",
            "        [ 0.2163, -0.3561],\n",
            "        [ 1.0394, -1.2150],\n",
            "        [ 0.8550, -0.9604],\n",
            "        [ 0.5093, -0.7462]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8100, -1.2185],\n",
            "        [ 0.8523, -1.4060],\n",
            "        [-1.2615,  1.2877],\n",
            "        [-1.5401,  1.3634],\n",
            "        [-1.6788,  1.6084],\n",
            "        [ 0.9797, -1.0409],\n",
            "        [ 0.8301, -1.0093],\n",
            "        [ 0.6457, -0.6252],\n",
            "        [ 0.6465, -0.8965],\n",
            "        [ 0.3832, -0.5271],\n",
            "        [-0.6310,  0.6121],\n",
            "        [ 1.2188, -1.1334],\n",
            "        [ 1.2167, -1.4185],\n",
            "        [ 0.7347, -1.2086],\n",
            "        [ 1.0211, -1.2993],\n",
            "        [-1.5597,  1.6853],\n",
            "        [-1.5854,  1.5043],\n",
            "        [-0.4524,  0.7067],\n",
            "        [-1.8316,  1.7524],\n",
            "        [ 0.3990, -0.8273],\n",
            "        [-0.8931,  0.8532],\n",
            "        [ 0.0464, -0.3035],\n",
            "        [ 0.8791, -0.9769],\n",
            "        [ 0.9163, -1.0842],\n",
            "        [ 0.8431, -1.1067],\n",
            "        [ 0.9491, -1.2626],\n",
            "        [ 1.0460, -1.0926],\n",
            "        [-1.4398,  1.3580],\n",
            "        [-0.6435,  0.7860],\n",
            "        [ 0.8597, -1.0602],\n",
            "        [ 1.0832, -1.1237],\n",
            "        [-1.2734,  1.4834]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3230, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9876, -1.2665],\n",
            "        [ 0.9556, -1.3542],\n",
            "        [-1.2582,  1.3003],\n",
            "        [ 0.6772, -0.9768],\n",
            "        [-1.0974,  1.0683],\n",
            "        [ 0.8070, -0.8540],\n",
            "        [ 0.8746, -0.9459],\n",
            "        [-0.9641,  1.3424],\n",
            "        [ 0.7001, -1.1963],\n",
            "        [ 1.0873, -1.4507],\n",
            "        [ 1.0350, -1.2979],\n",
            "        [-1.5879,  1.5503],\n",
            "        [-1.5694,  1.7690],\n",
            "        [-0.2193,  0.2049],\n",
            "        [ 0.4088, -0.6453],\n",
            "        [ 0.4192, -0.5520],\n",
            "        [-1.5476,  1.5486],\n",
            "        [ 0.3447, -0.4431],\n",
            "        [ 0.8105, -1.0777],\n",
            "        [-0.5901,  0.3449],\n",
            "        [ 0.9220, -0.9546],\n",
            "        [-0.4721,  0.5780],\n",
            "        [-0.7164,  1.0257],\n",
            "        [ 0.8454, -0.8398],\n",
            "        [-1.3029,  1.4601],\n",
            "        [ 0.7110, -0.4447],\n",
            "        [ 0.8671, -0.7733],\n",
            "        [ 0.7951, -1.0733],\n",
            "        [ 0.3099, -0.6316],\n",
            "        [-0.2492, -0.0214],\n",
            "        [ 0.6959, -0.9337],\n",
            "        [ 0.7402, -1.1791]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6207, -0.8205],\n",
            "        [ 1.1765, -1.1485],\n",
            "        [ 0.6651, -0.7711],\n",
            "        [ 1.0738, -1.3629],\n",
            "        [ 0.7138, -1.3714],\n",
            "        [ 0.4216, -0.6181],\n",
            "        [-1.1659,  0.8508],\n",
            "        [-1.5916,  1.8406],\n",
            "        [-0.4697,  0.6334],\n",
            "        [-0.4426,  0.2846],\n",
            "        [-1.6596,  1.8016],\n",
            "        [ 0.8107, -0.9336],\n",
            "        [ 1.1221, -1.4227],\n",
            "        [-0.3807,  0.1389],\n",
            "        [ 0.8186, -1.2267],\n",
            "        [ 0.7492, -0.6514],\n",
            "        [ 0.6318, -0.9319],\n",
            "        [ 1.1234, -1.3564],\n",
            "        [ 0.3609, -0.0985],\n",
            "        [-1.3813,  1.7041],\n",
            "        [ 0.5918, -0.7639],\n",
            "        [ 0.1217, -0.6105],\n",
            "        [ 0.6833, -1.0330],\n",
            "        [ 0.9829, -1.0430],\n",
            "        [ 0.8873, -0.8753],\n",
            "        [ 1.1138, -1.2068],\n",
            "        [-1.5317,  1.8876],\n",
            "        [ 0.5252, -0.8264],\n",
            "        [ 0.1912, -0.5058],\n",
            "        [-1.3040,  1.3771],\n",
            "        [-1.1441,  1.3040],\n",
            "        [-1.2294,  1.5305]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1060, -0.1255],\n",
            "        [ 0.6407, -1.0303],\n",
            "        [ 0.9956, -1.1490],\n",
            "        [-1.6280,  1.5782],\n",
            "        [-0.1566,  0.2388],\n",
            "        [-1.2066,  1.0307],\n",
            "        [ 0.7834, -1.0533],\n",
            "        [ 0.8523, -1.3947],\n",
            "        [-0.2312,  0.0422],\n",
            "        [ 1.0869, -1.1875],\n",
            "        [ 0.5868, -1.1555],\n",
            "        [ 0.6566, -0.9669],\n",
            "        [-1.3253,  1.0647],\n",
            "        [ 0.4184, -0.6346],\n",
            "        [ 0.2396, -0.7346],\n",
            "        [-1.3770,  1.8118],\n",
            "        [-1.3166,  1.5923],\n",
            "        [-1.3679,  1.2831],\n",
            "        [ 0.8029, -1.0088],\n",
            "        [ 0.4701, -0.3003],\n",
            "        [-1.1976,  1.5665],\n",
            "        [ 0.9395, -1.1633],\n",
            "        [ 0.3238, -0.3958],\n",
            "        [-1.2996,  1.0953],\n",
            "        [-1.0433,  1.0817],\n",
            "        [ 0.9097, -0.7581],\n",
            "        [ 1.0114, -0.8712],\n",
            "        [-1.4289,  1.5255],\n",
            "        [ 0.7529, -1.0410],\n",
            "        [ 1.1696, -1.1242],\n",
            "        [-1.5139,  1.7516],\n",
            "        [ 1.1018, -1.3038]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4960, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0966,  1.3122],\n",
            "        [ 0.5195, -0.8179],\n",
            "        [ 0.4491, -0.7890],\n",
            "        [ 0.9377, -1.4027],\n",
            "        [ 0.8075, -1.2361],\n",
            "        [ 0.8607, -0.9857],\n",
            "        [ 0.8091, -1.0213],\n",
            "        [-0.5915,  0.4540],\n",
            "        [ 0.8401, -0.9077],\n",
            "        [-1.1196,  1.0726],\n",
            "        [ 0.5977, -0.5915],\n",
            "        [-1.2944,  1.3460],\n",
            "        [ 0.8468, -0.9302],\n",
            "        [ 0.2438, -0.7518],\n",
            "        [-1.6025,  1.5729],\n",
            "        [-1.4103,  1.0945],\n",
            "        [ 0.1805, -0.6276],\n",
            "        [ 0.0359, -0.3782],\n",
            "        [ 0.8300, -1.2354],\n",
            "        [-0.1302, -0.3808],\n",
            "        [ 0.9495, -0.9911],\n",
            "        [ 0.5692, -0.5584],\n",
            "        [ 0.7483, -0.8143],\n",
            "        [ 0.9002, -0.9875],\n",
            "        [-1.5398,  1.4691],\n",
            "        [-1.5327,  1.7562],\n",
            "        [ 0.8777, -1.3843],\n",
            "        [-0.1784,  0.1154],\n",
            "        [ 0.8395, -0.9380],\n",
            "        [ 1.2260, -1.1480],\n",
            "        [ 0.5121, -0.5103],\n",
            "        [ 0.0804,  0.0486]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6340,  0.5793],\n",
            "        [-0.4130,  0.2420],\n",
            "        [-0.7989,  0.6181],\n",
            "        [ 0.9025, -1.0310],\n",
            "        [-1.3367,  1.7476],\n",
            "        [ 0.8574, -1.1285],\n",
            "        [ 1.1624, -1.0834],\n",
            "        [-1.4436,  1.6676],\n",
            "        [-1.3201,  1.3254],\n",
            "        [ 0.5916, -1.0337],\n",
            "        [-1.4090,  1.4374],\n",
            "        [ 1.2033, -1.0412],\n",
            "        [-0.4159,  0.2673],\n",
            "        [-1.6584,  1.5444],\n",
            "        [ 0.2620, -0.7521],\n",
            "        [-0.6923,  0.7675],\n",
            "        [ 1.0476, -1.2662],\n",
            "        [ 0.6761, -1.1166],\n",
            "        [-1.5725,  1.7089],\n",
            "        [ 1.1094, -1.3006],\n",
            "        [ 1.0964, -1.3418],\n",
            "        [ 0.9434, -1.2975],\n",
            "        [ 1.1074, -1.3639],\n",
            "        [ 1.4314, -1.0049],\n",
            "        [ 0.3135, -0.3980],\n",
            "        [-1.3295,  1.6471],\n",
            "        [-0.3946,  0.4269],\n",
            "        [ 0.9348, -0.8269],\n",
            "        [ 0.9005, -1.0449],\n",
            "        [ 0.1132,  0.1343],\n",
            "        [ 0.8757, -1.3074],\n",
            "        [ 0.6960, -0.9051]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9902, -1.0495],\n",
            "        [ 1.1311, -1.1906],\n",
            "        [ 0.4428, -0.6744],\n",
            "        [ 0.7784, -1.1319],\n",
            "        [ 0.8713, -1.0503],\n",
            "        [ 0.7691, -1.2898],\n",
            "        [ 1.1382, -1.2466],\n",
            "        [-1.6950,  1.4309],\n",
            "        [-0.1318, -0.2940],\n",
            "        [-1.3891,  1.5422],\n",
            "        [ 1.0497, -1.1678],\n",
            "        [ 0.3778, -0.7276],\n",
            "        [ 0.9795, -1.1071],\n",
            "        [ 0.7529, -1.0234],\n",
            "        [ 0.8466, -1.3005],\n",
            "        [ 1.1379, -0.9614],\n",
            "        [ 1.0846, -0.9241],\n",
            "        [-0.7467,  0.6410],\n",
            "        [-0.9010,  1.0901],\n",
            "        [-1.4762,  1.7727],\n",
            "        [ 0.5975, -0.8362],\n",
            "        [ 0.8950, -0.7524],\n",
            "        [-1.2046,  0.9940],\n",
            "        [ 0.9954, -1.2503],\n",
            "        [ 0.7944, -1.0358],\n",
            "        [ 1.1199, -1.2703],\n",
            "        [-0.4972,  0.3822],\n",
            "        [-1.6704,  1.7160],\n",
            "        [ 0.8898, -0.9296],\n",
            "        [-1.6356,  1.7074],\n",
            "        [ 0.9208, -1.0732],\n",
            "        [ 0.7481, -1.1587]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5755, -0.6433],\n",
            "        [ 0.7642, -1.0437],\n",
            "        [-1.5840,  1.6118],\n",
            "        [ 0.7131, -1.1399],\n",
            "        [ 0.8865, -1.1319],\n",
            "        [ 0.5010, -1.0297],\n",
            "        [ 0.9395, -0.9464],\n",
            "        [-0.1904, -0.0908],\n",
            "        [ 1.1407, -1.1150],\n",
            "        [ 0.9223, -1.0474],\n",
            "        [ 0.8594, -0.8416],\n",
            "        [ 1.0483, -1.2754],\n",
            "        [ 0.9243, -1.1186],\n",
            "        [ 0.9260, -1.1176],\n",
            "        [ 1.1207, -1.1276],\n",
            "        [ 0.0616, -0.4602],\n",
            "        [-0.1566,  0.0431],\n",
            "        [-1.5833,  1.4562],\n",
            "        [ 0.9398, -1.2656],\n",
            "        [ 0.0101,  0.1439],\n",
            "        [-1.5159,  1.7263],\n",
            "        [ 0.9554, -1.2001],\n",
            "        [-0.6691,  0.7794],\n",
            "        [ 0.4342, -0.8298],\n",
            "        [ 0.6611, -0.9234],\n",
            "        [ 0.5514, -0.8392],\n",
            "        [-0.4952,  0.3307],\n",
            "        [ 1.0174, -1.3264],\n",
            "        [ 1.0962, -1.0106],\n",
            "        [-1.0501,  1.1993],\n",
            "        [ 0.6441, -1.0195],\n",
            "        [-1.0869,  1.5264]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9258, -0.8733],\n",
            "        [ 0.8910, -1.0764],\n",
            "        [ 1.0047, -0.8199],\n",
            "        [-0.3468,  0.2823],\n",
            "        [-1.3059,  1.3455],\n",
            "        [ 0.3636, -0.5142],\n",
            "        [-1.3045,  1.6272],\n",
            "        [-1.4850,  1.4692],\n",
            "        [ 0.9906, -1.0201],\n",
            "        [ 0.5979, -0.7755],\n",
            "        [ 0.1843, -0.5649],\n",
            "        [ 0.8721, -1.0827],\n",
            "        [-0.4976,  0.6515],\n",
            "        [-1.4634,  1.6099],\n",
            "        [ 1.0790, -0.9262],\n",
            "        [-1.2844,  1.6492],\n",
            "        [-1.4694,  1.7174],\n",
            "        [ 0.9017, -1.1457],\n",
            "        [ 0.5853, -1.0702],\n",
            "        [-0.5975,  0.3982],\n",
            "        [ 0.6169, -0.8319],\n",
            "        [ 0.9529, -1.1235],\n",
            "        [ 0.8401, -1.1033],\n",
            "        [ 0.9326, -1.0285],\n",
            "        [ 0.7565, -0.7954],\n",
            "        [ 1.0009, -1.2324],\n",
            "        [ 0.8485, -0.9881],\n",
            "        [ 0.9279, -1.2294],\n",
            "        [ 0.6102, -1.0947],\n",
            "        [-0.1496,  0.1705],\n",
            "        [ 0.8254, -0.5970],\n",
            "        [-1.2645,  1.4065]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3153,  1.3598],\n",
            "        [ 0.5975, -1.0297],\n",
            "        [ 0.7229, -1.0550],\n",
            "        [-0.6539,  0.9876],\n",
            "        [ 0.8726, -1.1594],\n",
            "        [ 1.0736, -1.2421],\n",
            "        [-1.5006,  1.6462],\n",
            "        [ 0.7194, -0.9635],\n",
            "        [ 0.5264, -0.5341],\n",
            "        [ 0.9778, -1.3482],\n",
            "        [ 0.9305, -1.0448],\n",
            "        [ 1.0866, -1.0768],\n",
            "        [ 0.9073, -1.1159],\n",
            "        [-1.4199,  1.5214],\n",
            "        [ 1.2008, -1.3851],\n",
            "        [ 0.3208, -0.2555],\n",
            "        [ 0.8912, -1.2005],\n",
            "        [-1.4322,  1.2528],\n",
            "        [ 0.8217, -0.6340],\n",
            "        [-1.7483,  1.6458],\n",
            "        [-1.2301,  1.7626],\n",
            "        [ 0.9289, -1.3975],\n",
            "        [-1.4245,  1.6279],\n",
            "        [ 0.9028, -1.2619],\n",
            "        [ 0.1584, -0.6462],\n",
            "        [ 0.7216, -0.8089],\n",
            "        [ 1.0106, -1.3529],\n",
            "        [ 0.7634, -1.0393],\n",
            "        [ 0.9922, -1.1942],\n",
            "        [ 0.3638, -0.5031],\n",
            "        [-1.6006,  1.5966],\n",
            "        [-1.4620,  1.5156]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7224, -1.0378],\n",
            "        [ 1.3578, -1.1881],\n",
            "        [-0.3531,  0.6128],\n",
            "        [-1.4952,  1.4507],\n",
            "        [ 1.1365, -1.0441],\n",
            "        [-1.7954,  1.6561],\n",
            "        [ 0.8066, -0.8622],\n",
            "        [ 0.8889, -1.0087],\n",
            "        [ 1.0061, -1.1235],\n",
            "        [-1.4591,  1.6446],\n",
            "        [ 1.1197, -1.3154],\n",
            "        [ 1.1732, -1.3690],\n",
            "        [ 0.9562, -1.0110],\n",
            "        [-0.3752,  0.3902],\n",
            "        [-0.7112,  0.7305],\n",
            "        [ 0.5347, -0.6705],\n",
            "        [ 0.0158, -0.3573],\n",
            "        [ 0.5332, -1.0356],\n",
            "        [-1.6844,  1.3566],\n",
            "        [-0.6690,  0.7724],\n",
            "        [ 0.9344, -1.1477],\n",
            "        [ 0.6131, -0.7946],\n",
            "        [ 1.3344, -1.2038],\n",
            "        [-1.6553,  1.6170],\n",
            "        [ 0.7520, -0.9036],\n",
            "        [ 0.3837, -0.6796],\n",
            "        [ 0.8660, -0.9129],\n",
            "        [-1.3555,  1.5896],\n",
            "        [ 0.6318, -0.9200],\n",
            "        [ 1.1209, -1.2466],\n",
            "        [ 0.9292, -0.9813],\n",
            "        [-1.4362,  1.7555]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4294, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5299, -0.8371],\n",
            "        [-1.6020,  1.8746],\n",
            "        [ 0.5679, -0.8505],\n",
            "        [-1.4635,  1.5500],\n",
            "        [-1.5313,  1.6960],\n",
            "        [-0.9975,  0.9783],\n",
            "        [ 0.8970, -1.3315],\n",
            "        [ 0.6313, -1.1655],\n",
            "        [-1.2589,  1.5852],\n",
            "        [ 0.8382, -0.9523],\n",
            "        [-1.5251,  1.6013],\n",
            "        [ 0.7505, -1.2530],\n",
            "        [-1.2798,  1.2624],\n",
            "        [-1.5212,  1.4278],\n",
            "        [ 0.9540, -1.0660],\n",
            "        [-1.3105,  1.5570],\n",
            "        [ 1.2768, -1.3678],\n",
            "        [ 0.5951, -0.6086],\n",
            "        [ 1.1087, -1.2844],\n",
            "        [ 0.6327, -0.8720],\n",
            "        [ 0.8062, -1.0557],\n",
            "        [ 0.4614, -1.0597],\n",
            "        [ 0.8873, -1.4197],\n",
            "        [ 0.9701, -1.3457],\n",
            "        [-0.4245,  0.5084],\n",
            "        [ 1.1152, -1.3346],\n",
            "        [-1.4938,  1.3714],\n",
            "        [ 1.0179, -1.1176],\n",
            "        [-1.4854,  1.6807],\n",
            "        [ 0.6762, -0.8902],\n",
            "        [ 0.8777, -1.2200],\n",
            "        [-1.6260,  1.8710]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2915, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7605, -0.9027],\n",
            "        [-1.5990,  1.5952],\n",
            "        [ 0.7905, -1.3372],\n",
            "        [ 0.9637, -1.0803],\n",
            "        [-1.3455,  1.8642],\n",
            "        [ 0.6180, -1.1123],\n",
            "        [ 0.7430, -0.9527],\n",
            "        [-0.9937,  1.2060],\n",
            "        [ 0.9074, -1.0979],\n",
            "        [ 0.8348, -1.1963],\n",
            "        [ 0.4856, -0.8345],\n",
            "        [-0.3585,  0.1388],\n",
            "        [ 0.8791, -0.9669],\n",
            "        [-1.4300,  1.3374],\n",
            "        [ 0.2409, -0.6965],\n",
            "        [-1.4162,  1.4211],\n",
            "        [ 1.1329, -1.3062],\n",
            "        [ 1.1357, -1.3324],\n",
            "        [-0.7182,  0.7239],\n",
            "        [ 0.7264, -1.0095],\n",
            "        [ 0.5910, -0.7859],\n",
            "        [-0.9938,  1.2227],\n",
            "        [ 1.0478, -1.2214],\n",
            "        [-0.7068,  0.7778],\n",
            "        [-0.4294,  0.5218],\n",
            "        [ 0.1790, -0.5900],\n",
            "        [-0.7031,  0.8809],\n",
            "        [ 1.0150, -0.8479],\n",
            "        [ 0.8661, -1.0103],\n",
            "        [ 0.8946, -0.8899],\n",
            "        [-1.1720,  1.0951],\n",
            "        [-1.3831,  1.4108]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8316,  0.8249],\n",
            "        [ 0.0755, -0.3459],\n",
            "        [ 0.8231, -1.0784],\n",
            "        [ 0.9569, -1.3233],\n",
            "        [ 0.2372, -0.0569],\n",
            "        [ 0.9718, -0.9383],\n",
            "        [ 0.5807, -0.9174],\n",
            "        [-1.4013,  1.5680],\n",
            "        [ 0.6140, -0.4100],\n",
            "        [-1.4400,  1.7337],\n",
            "        [-1.3923,  1.6337],\n",
            "        [ 0.9186, -1.3403],\n",
            "        [ 0.8291, -0.9219],\n",
            "        [ 1.1683, -1.0186],\n",
            "        [-1.4935,  1.3809],\n",
            "        [-1.4090,  1.6047],\n",
            "        [ 1.0626, -1.1740],\n",
            "        [ 1.0786, -0.9323],\n",
            "        [ 1.2390, -1.1533],\n",
            "        [-1.0986,  0.9754],\n",
            "        [ 0.8466, -1.0472],\n",
            "        [-1.6225,  1.4491],\n",
            "        [-1.4095,  1.6579],\n",
            "        [-1.3937,  1.3850],\n",
            "        [ 1.3339, -1.5065],\n",
            "        [ 0.8587, -1.1470],\n",
            "        [-0.2671, -0.0704],\n",
            "        [-1.5816,  1.8519],\n",
            "        [ 0.5976, -0.4576],\n",
            "        [ 1.4180, -1.2438],\n",
            "        [ 0.9335, -1.0429],\n",
            "        [ 0.8424, -0.9672]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8908, -1.2101],\n",
            "        [ 0.8686, -1.1726],\n",
            "        [ 0.8231, -1.2802],\n",
            "        [ 0.7237, -1.0188],\n",
            "        [-1.1629,  1.6835],\n",
            "        [-1.1134,  1.1428],\n",
            "        [ 0.8248, -1.3011],\n",
            "        [ 0.4777, -0.8329],\n",
            "        [ 0.9169, -0.9521],\n",
            "        [ 0.6810, -1.2624],\n",
            "        [ 0.4050, -0.5007],\n",
            "        [ 0.9130, -1.0582],\n",
            "        [ 0.8359, -0.8047],\n",
            "        [ 0.2493, -0.1785],\n",
            "        [ 0.9426, -1.1954],\n",
            "        [ 1.0518, -1.2487],\n",
            "        [ 0.8950, -1.0956],\n",
            "        [ 1.1569, -1.1252],\n",
            "        [ 0.7598, -0.9545],\n",
            "        [ 0.7969, -1.3464],\n",
            "        [ 0.0985, -0.1560],\n",
            "        [ 0.8595, -0.9211],\n",
            "        [-0.2029,  0.1806],\n",
            "        [ 0.8407, -1.0850],\n",
            "        [-1.3257,  1.5690],\n",
            "        [-1.3803,  1.3964],\n",
            "        [-1.4574,  1.3435],\n",
            "        [ 0.9809, -0.8902],\n",
            "        [-1.5208,  1.6855],\n",
            "        [-0.7764,  0.7200],\n",
            "        [-1.8190,  1.6388],\n",
            "        [-0.2933,  0.6547]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4055, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9208, -0.7193],\n",
            "        [-1.5472,  1.6531],\n",
            "        [-1.1577,  1.2944],\n",
            "        [-0.3698,  0.5802],\n",
            "        [ 0.8935, -0.8904],\n",
            "        [ 0.9236, -1.2083],\n",
            "        [-0.5604,  0.4361],\n",
            "        [ 1.0589, -1.1329],\n",
            "        [ 0.4977, -1.0821],\n",
            "        [ 1.0170, -1.1034],\n",
            "        [-1.4661,  1.4069],\n",
            "        [ 1.2715, -1.5367],\n",
            "        [ 0.4366, -1.0403],\n",
            "        [-0.5428,  0.4421],\n",
            "        [ 0.5978, -1.0786],\n",
            "        [-1.1138,  1.1344],\n",
            "        [-1.5105,  1.5196],\n",
            "        [ 0.9593, -1.0813],\n",
            "        [ 1.0423, -1.2279],\n",
            "        [-1.5620,  1.4108],\n",
            "        [-1.4977,  1.4980],\n",
            "        [ 0.4434, -0.6906],\n",
            "        [ 1.0078, -0.9898],\n",
            "        [ 0.9766, -0.9367],\n",
            "        [ 1.0393, -1.2015],\n",
            "        [-1.4067,  1.2723],\n",
            "        [-1.1234,  0.8446],\n",
            "        [-0.5921,  0.4907],\n",
            "        [-1.1459,  1.1830],\n",
            "        [ 0.3091, -0.2584],\n",
            "        [ 0.2784, -0.2363],\n",
            "        [ 0.6348, -0.7264]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5358, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3739,  1.4317],\n",
            "        [ 0.4020, -1.0546],\n",
            "        [ 0.7143, -0.8189],\n",
            "        [ 0.9806, -1.3054],\n",
            "        [ 1.0086, -0.9581],\n",
            "        [-1.4958,  1.8326],\n",
            "        [ 0.8930, -1.1839],\n",
            "        [ 0.6373, -1.0245],\n",
            "        [ 0.5041, -0.4634],\n",
            "        [ 1.0823, -1.2540],\n",
            "        [ 1.0818, -0.9833],\n",
            "        [ 1.0261, -1.1692],\n",
            "        [ 0.6429, -0.8969],\n",
            "        [ 1.0540, -1.4944],\n",
            "        [ 0.3561, -0.4783],\n",
            "        [ 1.1640, -1.1939],\n",
            "        [-1.2783,  1.4949],\n",
            "        [-0.2353,  0.3420],\n",
            "        [-1.6111,  1.8004],\n",
            "        [ 0.7723, -0.9144],\n",
            "        [ 0.4884, -0.6699],\n",
            "        [-0.4649,  0.3800],\n",
            "        [-1.6867,  1.4396],\n",
            "        [ 0.8806, -1.4269],\n",
            "        [ 0.8656, -1.2380],\n",
            "        [ 0.9803, -1.2938],\n",
            "        [ 0.8831, -0.7885],\n",
            "        [-1.4942,  1.6317],\n",
            "        [-0.5046,  0.7128],\n",
            "        [-0.6839,  0.6777],\n",
            "        [ 0.7943, -1.2329],\n",
            "        [ 0.6610, -0.6865]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7252,  0.6996],\n",
            "        [ 1.2448, -1.3373],\n",
            "        [ 0.9513, -1.2580],\n",
            "        [ 1.0940, -1.0676],\n",
            "        [ 0.8820, -1.0848],\n",
            "        [ 0.6968, -1.4957],\n",
            "        [-1.4369,  1.5726],\n",
            "        [-1.3385,  1.7164],\n",
            "        [ 0.7530, -1.0418],\n",
            "        [ 0.3700, -0.2216],\n",
            "        [ 1.0086, -1.2720],\n",
            "        [-1.5061,  1.7459],\n",
            "        [ 1.0865, -1.3177],\n",
            "        [ 0.8416, -0.9691],\n",
            "        [-1.4937,  1.9010],\n",
            "        [-1.3574,  1.5854],\n",
            "        [ 1.1230, -1.3756],\n",
            "        [ 0.8635, -1.5996],\n",
            "        [ 0.8588, -0.8040],\n",
            "        [ 0.9025, -0.9879],\n",
            "        [ 0.8873, -1.0717],\n",
            "        [ 1.3910, -1.1814],\n",
            "        [ 0.8298, -1.0792],\n",
            "        [ 1.2180, -1.5168],\n",
            "        [-1.4840,  1.5388],\n",
            "        [ 0.5818, -0.7070],\n",
            "        [-0.0113, -0.3929],\n",
            "        [-1.4949,  1.5188],\n",
            "        [ 1.1024, -1.1616],\n",
            "        [ 0.9636, -1.1752],\n",
            "        [-1.1285,  1.2432],\n",
            "        [-0.7250,  0.7982]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4149e+00,  1.4032e+00],\n",
            "        [-1.3357e+00,  1.3276e+00],\n",
            "        [-4.4822e-01,  7.5460e-01],\n",
            "        [ 1.0869e+00, -1.1623e+00],\n",
            "        [ 9.2660e-01, -1.1876e+00],\n",
            "        [-1.2264e+00,  1.3946e+00],\n",
            "        [ 9.0906e-01, -8.4272e-01],\n",
            "        [ 5.2854e-01, -8.9561e-01],\n",
            "        [-1.3048e+00,  1.5023e+00],\n",
            "        [ 6.3955e-01, -9.4807e-01],\n",
            "        [-1.4886e+00,  1.8266e+00],\n",
            "        [ 9.5929e-01, -9.6549e-01],\n",
            "        [ 1.0227e+00, -1.1126e+00],\n",
            "        [ 1.1911e+00, -9.9965e-01],\n",
            "        [-1.5897e+00,  1.6645e+00],\n",
            "        [-1.2761e+00,  1.9870e+00],\n",
            "        [-1.3027e+00,  1.4474e+00],\n",
            "        [-6.7933e-02,  8.3853e-04],\n",
            "        [ 9.4977e-01, -8.6306e-01],\n",
            "        [ 8.0097e-01, -9.1376e-01],\n",
            "        [-1.5147e+00,  1.5452e+00],\n",
            "        [ 7.5774e-01, -1.1876e+00],\n",
            "        [-1.4545e+00,  1.5639e+00],\n",
            "        [ 1.1179e+00, -1.1582e+00],\n",
            "        [ 1.1040e+00, -1.0587e+00],\n",
            "        [-5.0876e-01,  4.6295e-01],\n",
            "        [ 6.2266e-01, -6.7072e-01],\n",
            "        [-1.6952e+00,  1.8735e+00],\n",
            "        [ 9.6726e-01, -1.2054e+00],\n",
            "        [ 6.5550e-01, -1.0401e+00],\n",
            "        [ 1.1968e+00, -1.2561e+00],\n",
            "        [ 1.1527e+00, -1.0781e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1997,  1.6756],\n",
            "        [-1.3302,  1.6171],\n",
            "        [-1.1066,  1.3770],\n",
            "        [-1.2926,  1.2372],\n",
            "        [ 0.0486, -0.2055],\n",
            "        [ 0.5994, -0.7719],\n",
            "        [ 1.0626, -1.2709],\n",
            "        [-1.8069,  1.4547],\n",
            "        [ 1.2291, -1.3839],\n",
            "        [-1.5767,  1.7359],\n",
            "        [ 1.0201, -1.1946],\n",
            "        [ 1.3093, -1.3886],\n",
            "        [ 0.7819, -1.1319],\n",
            "        [ 1.0184, -1.2963],\n",
            "        [ 0.8314, -1.0998],\n",
            "        [ 0.8605, -1.3071],\n",
            "        [ 0.8189, -1.1481],\n",
            "        [-0.9571,  1.0319],\n",
            "        [ 0.9505, -1.1203],\n",
            "        [-1.0128,  1.2329],\n",
            "        [-1.4235,  1.6829],\n",
            "        [ 1.2164, -1.3106],\n",
            "        [ 0.7518, -1.2344],\n",
            "        [ 0.9174, -0.8704],\n",
            "        [ 0.0856, -0.4588],\n",
            "        [-1.3280,  1.4727],\n",
            "        [ 0.6856, -0.9091],\n",
            "        [ 1.0859, -1.0981],\n",
            "        [-1.7238,  1.5756],\n",
            "        [ 0.9268, -1.1175],\n",
            "        [-1.6079,  1.8539],\n",
            "        [ 0.5648, -0.5395]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5862e+00,  1.6073e+00],\n",
            "        [ 7.7841e-01, -1.2630e+00],\n",
            "        [ 1.0223e+00, -1.1908e+00],\n",
            "        [ 1.1027e+00, -1.2495e+00],\n",
            "        [ 6.7278e-01, -1.1764e+00],\n",
            "        [ 1.3456e+00, -1.3374e+00],\n",
            "        [-3.2151e-01,  5.3200e-01],\n",
            "        [ 8.2784e-01, -8.1371e-01],\n",
            "        [ 7.9962e-01, -1.0351e+00],\n",
            "        [-2.0248e-01, -2.6523e-01],\n",
            "        [-1.7209e+00,  1.5519e+00],\n",
            "        [ 4.7830e-01, -1.9564e-01],\n",
            "        [ 1.6590e-03, -3.7706e-01],\n",
            "        [ 1.1163e+00, -1.4630e+00],\n",
            "        [ 7.8127e-01, -8.4056e-01],\n",
            "        [-1.6749e+00,  1.7014e+00],\n",
            "        [ 8.5670e-01, -9.4532e-01],\n",
            "        [-1.1409e-01,  1.7001e-01],\n",
            "        [ 1.0238e+00, -1.1386e+00],\n",
            "        [-1.6748e+00,  1.4568e+00],\n",
            "        [-1.1313e+00,  1.2163e+00],\n",
            "        [ 7.6653e-01, -1.0456e+00],\n",
            "        [ 4.4474e-01, -1.7536e-01],\n",
            "        [ 1.3439e+00, -1.3342e+00],\n",
            "        [ 1.0653e+00, -1.2747e+00],\n",
            "        [-3.0521e-01,  5.1130e-01],\n",
            "        [-1.1874e+00,  1.4723e+00],\n",
            "        [ 8.8614e-01, -1.1024e+00],\n",
            "        [ 3.8181e-01, -9.1021e-01],\n",
            "        [ 1.1672e+00, -1.1204e+00],\n",
            "        [ 8.0660e-01, -9.9037e-01],\n",
            "        [-6.7597e-01,  5.7528e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5205, -0.9016],\n",
            "        [ 0.1719, -0.1623],\n",
            "        [-1.3790,  1.3346],\n",
            "        [-1.1964,  1.3470],\n",
            "        [ 0.3577, -0.2013],\n",
            "        [ 0.5114, -0.9888],\n",
            "        [-0.0075, -0.0563],\n",
            "        [-0.2180,  0.4127],\n",
            "        [ 1.1879, -1.1144],\n",
            "        [ 1.1668, -1.1801],\n",
            "        [-1.6520,  1.7912],\n",
            "        [ 0.8256, -0.8720],\n",
            "        [-0.8953,  0.9918],\n",
            "        [-1.5744,  1.7630],\n",
            "        [-1.6990,  1.7570],\n",
            "        [-0.7237,  0.4827],\n",
            "        [ 0.5423, -0.4676],\n",
            "        [ 0.7766, -1.3410],\n",
            "        [ 0.4249, -0.9097],\n",
            "        [-1.2386,  1.2463],\n",
            "        [-0.7884,  0.6903],\n",
            "        [-1.0373,  1.3703],\n",
            "        [ 0.2393, -0.3512],\n",
            "        [-1.5128,  1.5058],\n",
            "        [-1.5325,  1.4719],\n",
            "        [-1.1429,  1.6320],\n",
            "        [ 0.5382, -0.6568],\n",
            "        [ 0.5917, -0.9700],\n",
            "        [ 0.6896, -0.7666],\n",
            "        [-1.5112,  1.4686],\n",
            "        [-1.5452,  1.3985],\n",
            "        [-1.4147,  1.6652]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8586, -1.1611],\n",
            "        [-1.4115,  1.7959],\n",
            "        [-1.6897,  1.5808],\n",
            "        [-0.9746,  1.1385],\n",
            "        [-1.4308,  1.6712],\n",
            "        [-1.1098,  1.0675],\n",
            "        [ 0.8056, -1.1586],\n",
            "        [ 0.8258, -1.2563],\n",
            "        [ 0.8527, -1.0717],\n",
            "        [-1.5918,  1.7072],\n",
            "        [-0.5906,  0.7543],\n",
            "        [ 0.9260, -1.2316],\n",
            "        [-1.3593,  1.4664],\n",
            "        [ 0.9530, -1.2625],\n",
            "        [ 0.8075, -0.9566],\n",
            "        [ 0.5900, -0.7201],\n",
            "        [-1.6322,  1.4600],\n",
            "        [ 0.5894, -0.7750],\n",
            "        [ 0.9771, -1.3219],\n",
            "        [-1.4698,  1.5749],\n",
            "        [-1.1498,  1.4225],\n",
            "        [ 1.0427, -1.0168],\n",
            "        [-0.8935,  1.1930],\n",
            "        [-1.8498,  1.7837],\n",
            "        [ 0.8029, -1.4741],\n",
            "        [-1.5100,  1.4827],\n",
            "        [-0.3474,  0.1500],\n",
            "        [-1.7421,  1.3442],\n",
            "        [ 0.7559, -1.1282],\n",
            "        [-0.1947, -0.0336],\n",
            "        [ 0.3122, -0.7166],\n",
            "        [ 0.0843, -0.5198]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4718, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9312, -1.1886],\n",
            "        [ 0.9143, -1.0450],\n",
            "        [ 0.7129, -0.9806],\n",
            "        [ 0.7160, -1.0279],\n",
            "        [ 0.4849, -0.7756],\n",
            "        [ 1.0852, -1.1120],\n",
            "        [ 0.6229, -1.0423],\n",
            "        [ 0.3716, -0.5234],\n",
            "        [-1.6439,  1.6617],\n",
            "        [-0.8162,  0.9326],\n",
            "        [-1.3115,  1.3791],\n",
            "        [-0.9778,  1.2918],\n",
            "        [-1.5412,  1.5763],\n",
            "        [ 1.0750, -1.3598],\n",
            "        [ 0.9020, -1.1089],\n",
            "        [-1.8049,  1.5220],\n",
            "        [-0.8655,  0.9288],\n",
            "        [-0.8468,  1.0569],\n",
            "        [-1.1896,  1.3568],\n",
            "        [ 0.7243, -0.9581],\n",
            "        [ 1.0430, -1.1580],\n",
            "        [-0.2110, -0.1460],\n",
            "        [-1.2609,  1.0751],\n",
            "        [-0.8106,  0.9743],\n",
            "        [ 0.6720, -0.9297],\n",
            "        [ 0.9120, -1.0686],\n",
            "        [ 0.8297, -1.2116],\n",
            "        [-1.5032,  1.4754],\n",
            "        [ 1.1885, -1.4468],\n",
            "        [-1.4180,  1.4528],\n",
            "        [-1.6806,  1.6292],\n",
            "        [ 0.2707, -0.4304]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5341, -0.8372],\n",
            "        [ 0.7037, -1.0006],\n",
            "        [-1.3688,  1.5183],\n",
            "        [ 0.6801, -0.7181],\n",
            "        [ 0.9563, -1.1091],\n",
            "        [ 1.2909, -1.3043],\n",
            "        [ 1.0439, -1.3555],\n",
            "        [-1.2638,  1.2723],\n",
            "        [ 0.3957, -0.6246],\n",
            "        [-0.7970,  0.7805],\n",
            "        [ 0.9360, -1.3383],\n",
            "        [ 1.1407, -1.3420],\n",
            "        [-1.7739,  1.4288],\n",
            "        [-1.2861,  1.3598],\n",
            "        [ 0.9131, -0.9089],\n",
            "        [ 1.0474, -0.9127],\n",
            "        [-1.4418,  1.5698],\n",
            "        [ 0.0421, -0.8375],\n",
            "        [-1.1349,  1.1995],\n",
            "        [-1.4936,  1.8133],\n",
            "        [ 0.9739, -1.2406],\n",
            "        [-0.3037,  0.2875],\n",
            "        [ 1.0507, -1.0591],\n",
            "        [ 0.8094, -0.7713],\n",
            "        [-1.0753,  1.0927],\n",
            "        [-0.6315,  0.5632],\n",
            "        [ 0.9647, -1.2114],\n",
            "        [ 0.9728, -1.1046],\n",
            "        [-0.8908,  0.7741],\n",
            "        [ 1.0199, -1.1573],\n",
            "        [ 1.1816, -1.0401],\n",
            "        [ 0.7752, -1.2153]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0435, -1.2983],\n",
            "        [ 1.1717, -1.3124],\n",
            "        [ 0.1212, -0.3956],\n",
            "        [-1.4675,  1.6861],\n",
            "        [ 0.9443, -1.1704],\n",
            "        [ 1.0946, -1.1672],\n",
            "        [-1.4632,  1.5950],\n",
            "        [ 0.8032, -1.1443],\n",
            "        [-0.5001,  0.5919],\n",
            "        [-1.7140,  1.4853],\n",
            "        [ 0.3730, -0.4838],\n",
            "        [ 0.2960, -0.6438],\n",
            "        [ 1.3518, -1.2384],\n",
            "        [-1.6246,  1.6348],\n",
            "        [ 1.0634, -1.2416],\n",
            "        [-1.4207,  1.5817],\n",
            "        [ 0.7960, -0.8107],\n",
            "        [-1.1145,  0.9660],\n",
            "        [ 0.6215, -0.9970],\n",
            "        [-1.5346,  1.5613],\n",
            "        [ 0.1370, -0.0095],\n",
            "        [-1.5003,  1.2546],\n",
            "        [-1.4327,  1.6054],\n",
            "        [ 1.0310, -1.3008],\n",
            "        [ 1.1093, -1.4134],\n",
            "        [-1.5363,  1.5467],\n",
            "        [ 0.5640, -1.1514],\n",
            "        [ 0.6359, -0.8954],\n",
            "        [ 0.8838, -1.0458],\n",
            "        [ 0.9756, -1.1393],\n",
            "        [-1.0661,  1.1493],\n",
            "        [ 0.4869, -0.5419]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1604, -0.9554],\n",
            "        [-1.3865,  1.4458],\n",
            "        [-1.5434,  1.6570],\n",
            "        [ 0.3348, -0.3493],\n",
            "        [ 0.4254, -0.5176],\n",
            "        [ 0.8945, -1.1739],\n",
            "        [ 0.6259, -0.7270],\n",
            "        [-1.6494,  1.6031],\n",
            "        [-0.8824,  1.2008],\n",
            "        [-1.6443,  1.7262],\n",
            "        [-1.4250,  0.9820],\n",
            "        [-0.4179,  0.3889],\n",
            "        [ 1.0532, -1.1812],\n",
            "        [-0.8700,  1.0577],\n",
            "        [ 0.5520, -0.7640],\n",
            "        [ 0.4235, -0.6955],\n",
            "        [ 1.0665, -0.8831],\n",
            "        [-1.2952,  1.7214],\n",
            "        [-1.1309,  1.2011],\n",
            "        [ 0.9298, -1.1561],\n",
            "        [-1.5246,  1.4859],\n",
            "        [ 0.8570, -0.8015],\n",
            "        [ 1.0601, -1.1296],\n",
            "        [ 0.4565, -0.6628],\n",
            "        [ 0.9099, -1.1211],\n",
            "        [-1.7485,  1.6250],\n",
            "        [ 0.8231, -0.9938],\n",
            "        [ 0.2377,  0.1666],\n",
            "        [ 0.2757, -0.7694],\n",
            "        [ 1.2075, -0.9704],\n",
            "        [ 1.0223, -1.0775],\n",
            "        [ 0.8525, -1.1523]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7465, -0.8894],\n",
            "        [-0.0555, -0.4598],\n",
            "        [ 1.0758, -1.1531],\n",
            "        [ 0.7122, -0.9900],\n",
            "        [ 0.8504, -1.1119],\n",
            "        [ 1.1570, -1.1882],\n",
            "        [ 1.1425, -1.3533],\n",
            "        [ 1.0822, -1.0308],\n",
            "        [-1.2757,  1.4889],\n",
            "        [ 0.9435, -1.1969],\n",
            "        [ 0.6095, -0.7381],\n",
            "        [ 0.7459, -0.8396],\n",
            "        [ 1.1569, -1.3782],\n",
            "        [-1.4538,  1.7208],\n",
            "        [ 0.6734, -1.1183],\n",
            "        [-1.7870,  1.6123],\n",
            "        [ 0.0502, -0.2856],\n",
            "        [-1.6245,  1.6321],\n",
            "        [ 0.4554, -0.6881],\n",
            "        [ 0.8120, -0.9202],\n",
            "        [ 0.8890, -1.1990],\n",
            "        [-1.3049,  1.4867],\n",
            "        [-1.2820,  1.1124],\n",
            "        [ 1.0054, -1.0737],\n",
            "        [ 1.0838, -1.2669],\n",
            "        [-1.1357,  1.1249],\n",
            "        [ 0.4419, -0.1685],\n",
            "        [ 0.1136, -0.0175],\n",
            "        [-1.3724,  1.3159],\n",
            "        [-1.5566,  1.6225],\n",
            "        [-1.2056,  0.9200],\n",
            "        [ 0.8165, -1.0405]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6217,  0.5513],\n",
            "        [ 0.6728, -1.0928],\n",
            "        [ 0.8937, -1.2585],\n",
            "        [-0.5262,  0.3213],\n",
            "        [ 0.7427, -0.8128],\n",
            "        [ 0.6086, -0.7755],\n",
            "        [-1.3224,  1.4270],\n",
            "        [-0.0938, -0.2484],\n",
            "        [ 0.8973, -1.1675],\n",
            "        [ 0.9280, -1.1993],\n",
            "        [ 0.9589, -1.1321],\n",
            "        [-1.3687,  1.3831],\n",
            "        [ 0.8168, -1.3863],\n",
            "        [ 1.0223, -0.8405],\n",
            "        [ 0.9718, -1.3913],\n",
            "        [ 0.7361, -0.7367],\n",
            "        [ 0.1366, -0.0674],\n",
            "        [-1.7090,  1.6927],\n",
            "        [ 0.4745, -0.8962],\n",
            "        [ 0.9432, -0.9146],\n",
            "        [ 0.7288, -1.2207],\n",
            "        [-1.5035,  1.6542],\n",
            "        [ 0.9196, -1.3967],\n",
            "        [ 1.0298, -1.1814],\n",
            "        [ 1.0604, -1.2251],\n",
            "        [ 0.7012, -0.6756],\n",
            "        [-1.4281,  1.4605],\n",
            "        [ 0.6121, -1.1285],\n",
            "        [ 1.1275, -1.4629],\n",
            "        [ 0.3995, -1.0981],\n",
            "        [ 0.8260, -1.2480],\n",
            "        [-0.1206,  0.0870]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5262, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0719,  0.8344],\n",
            "        [ 0.9274, -1.0133],\n",
            "        [ 0.8312, -1.0473],\n",
            "        [ 1.0220, -0.8552],\n",
            "        [-0.6533,  0.4165],\n",
            "        [ 0.9203, -0.9659],\n",
            "        [-1.6815,  1.6449],\n",
            "        [ 0.9155, -1.0154],\n",
            "        [ 0.8111, -0.8905],\n",
            "        [ 0.7499, -0.7678],\n",
            "        [ 0.7836, -1.1472],\n",
            "        [ 0.5402, -0.8646],\n",
            "        [ 1.0627, -1.1643],\n",
            "        [ 1.0335, -1.1856],\n",
            "        [-1.4509,  1.5718],\n",
            "        [-0.2017,  0.0895],\n",
            "        [ 1.0477, -1.3441],\n",
            "        [ 0.9961, -1.2791],\n",
            "        [-1.5640,  1.5028],\n",
            "        [ 0.9984, -1.0411],\n",
            "        [ 0.6319, -0.6016],\n",
            "        [ 0.3286, -0.5808],\n",
            "        [ 1.0898, -1.2868],\n",
            "        [ 0.0642,  0.1171],\n",
            "        [ 0.5175, -1.0998],\n",
            "        [-0.6876,  0.7173],\n",
            "        [ 0.8329, -0.7204],\n",
            "        [ 0.9610, -1.0823],\n",
            "        [-1.4643,  1.4845],\n",
            "        [ 1.0677, -1.1942],\n",
            "        [ 1.0551, -1.1577],\n",
            "        [ 0.5334, -0.9412]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8345, -0.9094],\n",
            "        [ 0.7376, -1.0783],\n",
            "        [-1.5037,  1.2873],\n",
            "        [ 0.5598, -0.7518],\n",
            "        [ 0.4985, -0.6357],\n",
            "        [-1.4856,  1.6958],\n",
            "        [ 0.5161, -0.8549],\n",
            "        [-1.2910,  1.1301],\n",
            "        [ 1.0005, -1.3763],\n",
            "        [ 0.0642, -0.3463],\n",
            "        [-1.4794,  1.4431],\n",
            "        [ 1.1182, -1.1048],\n",
            "        [ 0.7943, -0.9722],\n",
            "        [-1.0783,  1.2689],\n",
            "        [-1.2997,  1.4655],\n",
            "        [ 0.8773, -1.2757],\n",
            "        [ 1.1214, -1.2208],\n",
            "        [ 0.8601, -1.1048],\n",
            "        [ 0.8239, -1.2810],\n",
            "        [ 0.7873, -1.2093],\n",
            "        [ 0.4221, -0.8329],\n",
            "        [-0.5093,  0.7292],\n",
            "        [ 0.8825, -1.0932],\n",
            "        [-1.0026,  1.3721],\n",
            "        [ 0.9217, -0.9112],\n",
            "        [ 0.1816, -0.4482],\n",
            "        [ 0.8920, -1.2559],\n",
            "        [-1.5136,  1.4279],\n",
            "        [ 0.8680, -1.1952],\n",
            "        [ 0.7342, -1.1813],\n",
            "        [ 0.9559, -0.8441],\n",
            "        [ 0.1381, -0.3363]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6936, -1.0045],\n",
            "        [-1.4340,  1.4217],\n",
            "        [ 0.6656, -1.0050],\n",
            "        [-0.7431,  0.9878],\n",
            "        [ 0.7847, -0.9310],\n",
            "        [ 0.3161, -0.4632],\n",
            "        [ 0.9485, -1.0114],\n",
            "        [ 0.9341, -1.1081],\n",
            "        [ 0.3035, -0.4832],\n",
            "        [ 0.4330, -0.8457],\n",
            "        [ 0.8218, -1.0384],\n",
            "        [ 0.9459, -1.0324],\n",
            "        [ 0.4567, -0.7975],\n",
            "        [-1.7304,  1.8244],\n",
            "        [-1.0588,  1.5115],\n",
            "        [-1.3819,  1.3620],\n",
            "        [ 0.7559, -1.3607],\n",
            "        [ 0.1002, -0.7210],\n",
            "        [ 1.0299, -1.0221],\n",
            "        [ 0.2034, -0.3824],\n",
            "        [-0.0065,  0.4526],\n",
            "        [ 0.6862, -1.1470],\n",
            "        [ 0.6986, -1.1013],\n",
            "        [ 0.7329, -1.1669],\n",
            "        [-1.4851,  1.6625],\n",
            "        [ 0.5076, -0.3355],\n",
            "        [ 0.8702, -1.1491],\n",
            "        [-1.2988,  1.4519],\n",
            "        [ 0.6493, -0.9888],\n",
            "        [ 1.1058, -1.6177],\n",
            "        [ 0.7322, -1.0601],\n",
            "        [ 1.0039, -1.3455]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9700, -0.8136],\n",
            "        [-0.2628,  0.1518],\n",
            "        [ 0.7461, -1.2698],\n",
            "        [-1.4640,  1.5687],\n",
            "        [ 0.8762, -0.9124],\n",
            "        [-1.5167,  1.5554],\n",
            "        [ 0.7716, -1.0934],\n",
            "        [-1.3470,  1.7406],\n",
            "        [ 0.5812, -1.2478],\n",
            "        [ 0.6339, -0.3760],\n",
            "        [-0.8337,  1.0155],\n",
            "        [ 0.9770, -1.2329],\n",
            "        [-1.2936,  1.3781],\n",
            "        [ 1.2056, -1.4264],\n",
            "        [-1.5090,  1.6302],\n",
            "        [ 0.7752, -1.1489],\n",
            "        [ 1.3217, -1.1436],\n",
            "        [-0.9977,  1.2137],\n",
            "        [ 1.0394, -1.2076],\n",
            "        [-1.5977,  1.2597],\n",
            "        [ 1.0509, -0.9239],\n",
            "        [ 1.0468, -1.4661],\n",
            "        [-1.7403,  1.5892],\n",
            "        [ 0.2265, -0.3883],\n",
            "        [ 0.8056, -0.8783],\n",
            "        [-0.9927,  0.6345],\n",
            "        [ 0.9595, -1.1574],\n",
            "        [ 0.7535, -1.2368],\n",
            "        [-1.3558,  1.4552],\n",
            "        [-1.4559,  1.4302],\n",
            "        [ 0.9443, -1.1729],\n",
            "        [ 0.1242, -0.0023]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8943, -1.3560],\n",
            "        [ 0.8791, -1.2621],\n",
            "        [ 0.6612, -0.3896],\n",
            "        [-1.5085,  1.4878],\n",
            "        [ 1.3899, -1.5286],\n",
            "        [ 1.1387, -1.0325],\n",
            "        [-1.4936,  1.5629],\n",
            "        [ 0.5859, -1.1710],\n",
            "        [-0.1292, -0.4158],\n",
            "        [-0.3197,  0.3020],\n",
            "        [-1.2871,  1.7066],\n",
            "        [ 1.0404, -0.9024],\n",
            "        [-0.1038,  0.0721],\n",
            "        [ 1.1907, -0.9607],\n",
            "        [ 0.8271, -1.0228],\n",
            "        [ 0.9677, -1.0992],\n",
            "        [ 1.1491, -0.8835],\n",
            "        [ 0.9457, -1.0894],\n",
            "        [-1.4931,  1.5615],\n",
            "        [ 1.1441, -0.9502],\n",
            "        [-1.4998,  1.6552],\n",
            "        [ 0.7880, -1.0115],\n",
            "        [ 0.6927, -0.9317],\n",
            "        [ 0.8431, -0.8355],\n",
            "        [-1.4354,  1.7464],\n",
            "        [-1.6121,  1.5656],\n",
            "        [ 0.9670, -0.7561],\n",
            "        [-1.1006,  1.6890],\n",
            "        [ 0.7822, -0.7830],\n",
            "        [ 0.3328, -0.4029],\n",
            "        [-1.2963,  1.4206],\n",
            "        [ 0.8571, -1.0771]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9408, -1.2416],\n",
            "        [-1.4028,  1.3587],\n",
            "        [ 0.3156, -0.9380],\n",
            "        [ 0.6152, -0.6724],\n",
            "        [ 0.7579, -1.0526],\n",
            "        [ 0.7161, -1.0332],\n",
            "        [-0.5970,  0.9978],\n",
            "        [ 0.9823, -0.8793],\n",
            "        [-1.6346,  1.5881],\n",
            "        [ 0.5496, -0.8795],\n",
            "        [ 0.7382, -1.3126],\n",
            "        [ 0.5080, -0.7933],\n",
            "        [-1.6290,  1.6406],\n",
            "        [ 0.6565, -1.2095],\n",
            "        [ 1.1153, -1.0435],\n",
            "        [ 0.0653, -0.7377],\n",
            "        [-0.5190,  0.5874],\n",
            "        [ 0.8340, -1.0973],\n",
            "        [ 0.6786, -0.8647],\n",
            "        [ 1.0277, -1.2649],\n",
            "        [-0.4190,  0.0686],\n",
            "        [ 0.8767, -0.9513],\n",
            "        [-1.3851,  1.3474],\n",
            "        [-1.7210,  1.6625],\n",
            "        [ 1.2004, -1.4793],\n",
            "        [ 0.8550, -1.1718],\n",
            "        [-0.2646,  0.2814],\n",
            "        [ 0.4401, -0.9023],\n",
            "        [-1.2663,  1.0561],\n",
            "        [ 0.9428, -1.1854],\n",
            "        [ 0.2535, -0.7975],\n",
            "        [ 0.7208, -1.0515]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4938,  1.6973],\n",
            "        [-1.6141,  1.6850],\n",
            "        [-0.4033,  0.3185],\n",
            "        [ 0.1794, -0.3600],\n",
            "        [ 0.7550, -0.9137],\n",
            "        [-1.1490,  1.2318],\n",
            "        [ 1.0272, -1.0702],\n",
            "        [ 0.2995, -0.5773],\n",
            "        [ 0.9285, -1.1222],\n",
            "        [ 0.3165, -0.5298],\n",
            "        [ 0.9965, -1.0064],\n",
            "        [-1.7371,  1.7371],\n",
            "        [ 0.9427, -1.3290],\n",
            "        [ 0.7799, -1.1157],\n",
            "        [-1.3569,  1.3854],\n",
            "        [-1.4070,  1.5504],\n",
            "        [-1.6883,  1.7214],\n",
            "        [ 0.9927, -1.2985],\n",
            "        [-0.6499,  0.7018],\n",
            "        [-0.4446,  0.3415],\n",
            "        [-0.9970,  1.1924],\n",
            "        [ 0.8872, -1.2506],\n",
            "        [-1.1503,  1.1574],\n",
            "        [-0.6384,  0.5385],\n",
            "        [ 1.1780, -1.3698],\n",
            "        [ 0.9501, -1.1519],\n",
            "        [ 0.6714, -0.8269],\n",
            "        [ 0.9628, -1.1797],\n",
            "        [ 1.0080, -0.9653],\n",
            "        [-1.2194,  1.3083],\n",
            "        [ 0.6260, -1.0419],\n",
            "        [ 0.6609, -0.8082]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.1514,  0.9159],\n",
            "        [ 0.9584, -1.0717],\n",
            "        [-1.5258,  1.6664],\n",
            "        [ 0.6910, -1.0485],\n",
            "        [ 0.5586, -0.9850],\n",
            "        [ 0.8575, -1.2181],\n",
            "        [ 0.2237, -0.0654],\n",
            "        [ 0.6425, -0.8194],\n",
            "        [ 0.6307, -0.7204],\n",
            "        [ 0.8148, -1.2354],\n",
            "        [ 0.2594, -0.9066],\n",
            "        [-0.6389,  0.8098],\n",
            "        [-1.4460,  1.6792],\n",
            "        [-0.1204,  0.4252],\n",
            "        [ 0.5219, -0.5483],\n",
            "        [ 1.1506, -1.3238],\n",
            "        [ 0.2881, -0.4794],\n",
            "        [ 0.7436, -0.7079],\n",
            "        [-0.6254,  0.8088],\n",
            "        [-1.5487,  1.7903],\n",
            "        [-0.9833,  0.9882],\n",
            "        [-1.6970,  1.6609],\n",
            "        [-0.9489,  1.5551],\n",
            "        [ 0.9597, -1.0235],\n",
            "        [-1.5781,  1.6778],\n",
            "        [ 0.7050, -1.1305],\n",
            "        [ 0.8817, -0.9550],\n",
            "        [-1.6406,  1.6640],\n",
            "        [ 0.9869, -1.1883],\n",
            "        [ 0.6488, -0.8383],\n",
            "        [ 0.6335, -0.7063],\n",
            "        [ 0.2161, -0.4585]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch   150  of    191.    Elapsed: 0:03:27.\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2077, -1.2671],\n",
            "        [ 0.9710, -1.1065],\n",
            "        [ 1.3562, -1.2180],\n",
            "        [ 0.8862, -0.9865],\n",
            "        [ 0.7539, -0.9945],\n",
            "        [-1.5202,  1.6006],\n",
            "        [ 0.9746, -1.0759],\n",
            "        [ 0.9076, -0.8648],\n",
            "        [ 0.9188, -1.5137],\n",
            "        [-1.4740,  1.3452],\n",
            "        [-0.2579,  0.4153],\n",
            "        [ 0.0070,  0.2582],\n",
            "        [ 0.7730, -0.7434],\n",
            "        [ 0.4500, -0.9077],\n",
            "        [-1.3661,  1.3105],\n",
            "        [-0.8118,  0.6463],\n",
            "        [ 1.1883, -1.1547],\n",
            "        [ 0.9710, -0.9481],\n",
            "        [ 0.3380, -0.6828],\n",
            "        [ 0.7659, -1.2278],\n",
            "        [-0.9056,  1.2137],\n",
            "        [ 0.7003, -0.7577],\n",
            "        [-1.2784,  1.1857],\n",
            "        [ 0.9713, -1.2719],\n",
            "        [-1.4883,  1.7065],\n",
            "        [ 0.9695, -0.9370],\n",
            "        [ 1.0057, -1.1208],\n",
            "        [-0.2637,  0.3484],\n",
            "        [ 0.9291, -1.3435],\n",
            "        [-0.6699,  0.3439],\n",
            "        [ 0.5435, -0.7102],\n",
            "        [ 0.8192, -0.7960]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4638, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.6938,  1.6318],\n",
            "        [-0.9199,  1.1631],\n",
            "        [-1.1730,  1.0552],\n",
            "        [ 0.1101, -0.3300],\n",
            "        [-1.5440,  1.5279],\n",
            "        [ 0.9432, -1.2486],\n",
            "        [ 0.7915, -1.0108],\n",
            "        [-1.4540,  1.6501],\n",
            "        [ 1.0731, -1.4060],\n",
            "        [ 0.9440, -1.1097],\n",
            "        [ 0.8836, -1.1281],\n",
            "        [ 0.5321, -0.6955],\n",
            "        [-0.0534,  0.2457],\n",
            "        [ 0.7877, -0.9654],\n",
            "        [ 0.7816, -1.0429],\n",
            "        [ 0.8424, -1.2245],\n",
            "        [ 0.7762, -0.7491],\n",
            "        [ 0.7341, -0.9940],\n",
            "        [ 0.6079, -0.9206],\n",
            "        [ 0.5755, -1.1295],\n",
            "        [ 0.9132, -1.0297],\n",
            "        [ 0.5075, -0.8286],\n",
            "        [ 0.8327, -0.9852],\n",
            "        [-1.2893,  1.5944],\n",
            "        [ 1.2825, -1.4090],\n",
            "        [ 1.1281, -1.2925],\n",
            "        [ 0.9487, -1.2286],\n",
            "        [-0.3262,  0.1300],\n",
            "        [ 0.6068, -0.7938],\n",
            "        [-1.2098,  1.2926],\n",
            "        [ 0.5388, -0.7668],\n",
            "        [-0.6579,  0.6059]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2674, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5213,  1.5381],\n",
            "        [ 0.8534, -1.1737],\n",
            "        [-1.3400,  1.3811],\n",
            "        [-0.9050,  1.1718],\n",
            "        [-1.6191,  1.6031],\n",
            "        [ 0.6751, -0.8527],\n",
            "        [ 0.5894, -0.8341],\n",
            "        [-1.6103,  1.7077],\n",
            "        [ 1.0245, -1.1562],\n",
            "        [ 1.0397, -1.2670],\n",
            "        [ 0.8506, -1.1723],\n",
            "        [ 0.7921, -0.8628],\n",
            "        [-1.4319,  1.7086],\n",
            "        [ 0.7577, -0.9395],\n",
            "        [ 0.6829, -0.9568],\n",
            "        [ 0.3683, -0.4803],\n",
            "        [-1.4510,  1.9090],\n",
            "        [ 1.0429, -0.9859],\n",
            "        [ 0.9764, -1.3027],\n",
            "        [-1.5806,  1.6247],\n",
            "        [ 0.6959, -0.9488],\n",
            "        [ 0.9916, -1.0951],\n",
            "        [-1.3190,  1.4988],\n",
            "        [ 0.4883, -0.6462],\n",
            "        [ 0.5497, -0.7216],\n",
            "        [-1.4610,  1.7410],\n",
            "        [ 1.0967, -0.8930],\n",
            "        [-0.4563,  0.5616],\n",
            "        [ 0.0108,  0.2241],\n",
            "        [ 0.0625, -0.4197],\n",
            "        [ 0.7627, -1.0809],\n",
            "        [ 0.9559, -1.0982]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9052, -1.0488],\n",
            "        [-1.2036,  1.4820],\n",
            "        [-0.9828,  0.9344],\n",
            "        [ 0.2015, -0.5008],\n",
            "        [ 1.0485, -0.8713],\n",
            "        [ 1.0414, -1.2206],\n",
            "        [ 0.8382, -1.5431],\n",
            "        [ 0.9630, -1.1656],\n",
            "        [ 0.8768, -1.0841],\n",
            "        [-1.1949,  1.2803],\n",
            "        [-0.7721,  1.1208],\n",
            "        [ 0.8824, -1.2127],\n",
            "        [ 0.9011, -1.1199],\n",
            "        [-1.6316,  1.5240],\n",
            "        [ 0.8810, -1.0486],\n",
            "        [-1.2060,  1.1267],\n",
            "        [-1.2594,  1.4912],\n",
            "        [ 0.2521, -0.6185],\n",
            "        [-1.5680,  1.7657],\n",
            "        [ 0.5971, -0.7756],\n",
            "        [ 0.2835, -0.4796],\n",
            "        [ 0.6305, -0.9770],\n",
            "        [-1.3061,  1.4935],\n",
            "        [-1.2615,  1.2617],\n",
            "        [-1.1817,  1.3438],\n",
            "        [ 0.8319, -1.1273],\n",
            "        [-1.5339,  1.6038],\n",
            "        [-1.6634,  1.7623],\n",
            "        [ 0.7945, -0.7746],\n",
            "        [-0.9809,  1.0673],\n",
            "        [ 1.0649, -1.3600],\n",
            "        [-0.2367,  0.1523]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0051,  0.0381],\n",
            "        [-0.4185,  0.2512],\n",
            "        [ 0.6610, -1.0292],\n",
            "        [ 0.0829, -0.4683],\n",
            "        [-1.7787,  1.4220],\n",
            "        [ 0.6922, -1.0699],\n",
            "        [ 0.8676, -1.0573],\n",
            "        [ 0.8182, -1.1285],\n",
            "        [-1.5164,  1.5641],\n",
            "        [ 0.9961, -0.9218],\n",
            "        [ 1.0444, -1.1328],\n",
            "        [-1.6024,  1.8581],\n",
            "        [ 0.8602, -1.1993],\n",
            "        [ 0.8469, -0.9439],\n",
            "        [ 0.9738, -1.1988],\n",
            "        [-0.1474, -0.3120],\n",
            "        [-1.4325,  1.5760],\n",
            "        [ 0.9786, -1.0179],\n",
            "        [-0.9960,  1.0573],\n",
            "        [-1.7254,  1.5992],\n",
            "        [ 0.9892, -1.1691],\n",
            "        [ 0.8030, -1.2701],\n",
            "        [ 0.6995, -1.3391],\n",
            "        [ 0.6500, -0.6777],\n",
            "        [ 0.9475, -1.1544],\n",
            "        [-1.1945,  1.2923],\n",
            "        [ 0.9992, -1.2310],\n",
            "        [-1.4191,  1.3413],\n",
            "        [ 1.1585, -1.0183],\n",
            "        [-1.7095,  1.7724],\n",
            "        [-1.2646,  1.3538],\n",
            "        [ 1.0106, -0.8292]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9868, -1.2520],\n",
            "        [-1.2547,  1.3251],\n",
            "        [ 1.0872, -1.1590],\n",
            "        [ 1.0349, -1.1842],\n",
            "        [-0.3265,  0.6335],\n",
            "        [ 1.0019, -1.3805],\n",
            "        [ 0.9990, -1.5122],\n",
            "        [ 1.1195, -1.4557],\n",
            "        [ 0.4120, -1.0188],\n",
            "        [ 0.8820, -1.1372],\n",
            "        [-1.5558,  1.5762],\n",
            "        [ 0.3143, -0.7123],\n",
            "        [ 0.5675, -0.8575],\n",
            "        [ 0.8973, -1.2744],\n",
            "        [ 0.0374, -0.2853],\n",
            "        [ 0.5020, -0.8710],\n",
            "        [ 0.8566, -1.2228],\n",
            "        [ 0.0481, -1.0018],\n",
            "        [-1.6578,  1.5137],\n",
            "        [ 0.5823, -0.2104],\n",
            "        [ 0.9252, -1.2948],\n",
            "        [ 0.8174, -1.2500],\n",
            "        [-1.6982,  1.8923],\n",
            "        [ 1.0332, -1.3413],\n",
            "        [-0.1326, -0.1274],\n",
            "        [-1.3280,  1.4614],\n",
            "        [ 0.7472, -0.9251],\n",
            "        [ 1.1443, -1.3198],\n",
            "        [ 1.0017, -1.1440],\n",
            "        [ 1.0779, -1.1624],\n",
            "        [ 0.6489, -1.0469],\n",
            "        [ 0.9843, -1.2687]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.6214,  1.6885],\n",
            "        [ 0.6408, -0.9828],\n",
            "        [ 1.1971, -1.2592],\n",
            "        [-0.4862,  0.2746],\n",
            "        [-1.4266,  1.6922],\n",
            "        [-0.8576,  0.4044],\n",
            "        [ 0.9872, -1.2626],\n",
            "        [ 1.0282, -1.1416],\n",
            "        [ 1.0437, -1.2668],\n",
            "        [-1.3917,  1.4429],\n",
            "        [-1.6724,  1.6202],\n",
            "        [-1.5361,  1.8209],\n",
            "        [-1.2998,  1.3293],\n",
            "        [-1.2595,  1.5711],\n",
            "        [ 0.0825, -0.0926],\n",
            "        [-1.2299,  1.2922],\n",
            "        [-1.6820,  1.5222],\n",
            "        [-1.6527,  1.6798],\n",
            "        [-1.3489,  1.1142],\n",
            "        [-1.5201,  1.4593],\n",
            "        [ 0.8970, -1.2971],\n",
            "        [ 0.2267, -0.5941],\n",
            "        [-0.9500,  0.7381],\n",
            "        [ 0.7773, -1.1629],\n",
            "        [ 0.8109, -1.0414],\n",
            "        [ 0.9385, -0.9810],\n",
            "        [-1.7936,  1.5584],\n",
            "        [-1.4489,  1.5901],\n",
            "        [ 0.7517, -0.9029],\n",
            "        [-1.4850,  1.5039],\n",
            "        [-0.8262,  1.4959],\n",
            "        [ 0.9545, -0.9973]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3433,  1.6790],\n",
            "        [ 0.6713, -0.8608],\n",
            "        [-1.4228,  1.2442],\n",
            "        [-1.2944,  1.5468],\n",
            "        [ 0.9914, -1.2004],\n",
            "        [-1.5680,  1.5996],\n",
            "        [ 0.8854, -1.1430],\n",
            "        [-0.8920,  0.7250],\n",
            "        [ 0.9544, -1.1485],\n",
            "        [ 0.9958, -1.1785],\n",
            "        [-0.1653,  0.1162],\n",
            "        [ 0.6643, -0.7088],\n",
            "        [-1.5838,  1.3740],\n",
            "        [ 1.1163, -1.1822],\n",
            "        [-0.2694,  0.0955],\n",
            "        [ 1.0771, -0.9731],\n",
            "        [-1.2139,  1.2398],\n",
            "        [-0.3098,  0.2373],\n",
            "        [-1.3829,  1.4805],\n",
            "        [ 0.5958, -0.8741],\n",
            "        [ 0.7226, -0.9909],\n",
            "        [ 1.1367, -0.8729],\n",
            "        [ 0.9471, -1.4291],\n",
            "        [ 0.9182, -1.1300],\n",
            "        [-0.4220,  0.4235],\n",
            "        [-1.6324,  1.5789],\n",
            "        [-1.3344,  1.2257],\n",
            "        [ 0.7785, -1.2470],\n",
            "        [ 0.5486, -0.6436],\n",
            "        [-1.1269,  1.3468],\n",
            "        [ 0.7811, -1.3661],\n",
            "        [ 1.1798, -0.9860]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3352, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4028, -0.7015],\n",
            "        [ 0.1110,  0.1009],\n",
            "        [ 0.7796, -0.9852],\n",
            "        [-1.3519,  1.8148],\n",
            "        [-1.4651,  1.5367],\n",
            "        [-1.2578,  1.2936],\n",
            "        [-1.0293,  1.2565],\n",
            "        [ 0.9342, -1.1409],\n",
            "        [-1.6745,  1.5889],\n",
            "        [ 0.6611, -1.2825],\n",
            "        [ 0.8954, -1.2211],\n",
            "        [ 0.9936, -1.0539],\n",
            "        [-1.5608,  1.6157],\n",
            "        [ 0.9036, -1.0147],\n",
            "        [-0.7411,  0.9019],\n",
            "        [ 1.0450, -0.9552],\n",
            "        [ 0.7355, -0.7525],\n",
            "        [-1.6611,  1.4833],\n",
            "        [ 0.7898, -0.9885],\n",
            "        [-1.3097,  1.3457],\n",
            "        [ 0.3601, -0.8002],\n",
            "        [-1.2177,  1.2003],\n",
            "        [ 0.5614, -0.8354],\n",
            "        [ 1.1226, -1.3064],\n",
            "        [ 0.8016, -0.6898],\n",
            "        [-1.5100,  1.4872],\n",
            "        [ 0.8240, -0.9408],\n",
            "        [ 0.2989, -0.5924],\n",
            "        [ 0.7757, -1.3455],\n",
            "        [ 0.4466, -0.9720],\n",
            "        [ 1.1022, -1.2854],\n",
            "        [ 1.0675, -1.1596]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7865,  0.5961],\n",
            "        [ 0.9173, -1.2028],\n",
            "        [-0.1615, -0.0295],\n",
            "        [-1.4809,  1.6635],\n",
            "        [-0.9580,  1.2028],\n",
            "        [-1.4227,  1.2968],\n",
            "        [-1.0317,  1.2549],\n",
            "        [ 0.7960, -1.0751],\n",
            "        [ 0.4816, -0.6682],\n",
            "        [ 0.3768, -0.9048],\n",
            "        [ 0.8496, -0.6491],\n",
            "        [-1.0266,  1.2079],\n",
            "        [ 0.7793, -1.0542],\n",
            "        [ 1.1408, -1.2636],\n",
            "        [ 0.5946, -0.5158],\n",
            "        [ 0.8277, -0.9096],\n",
            "        [-1.3005,  1.5795],\n",
            "        [ 1.1621, -1.1711],\n",
            "        [-0.2990,  0.3132],\n",
            "        [-0.6344,  0.6985],\n",
            "        [-0.2857,  0.6341],\n",
            "        [ 0.6166, -0.9070],\n",
            "        [ 1.2213, -1.3008],\n",
            "        [-1.6787,  1.7555],\n",
            "        [ 0.8090, -1.0424],\n",
            "        [ 0.8127, -1.3204],\n",
            "        [-0.8839,  0.9154],\n",
            "        [ 1.1971, -1.2050],\n",
            "        [-1.5020,  1.2954],\n",
            "        [ 0.1002, -0.2238],\n",
            "        [ 0.5869, -0.5885],\n",
            "        [-1.5609,  1.7014]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3656, -0.6484],\n",
            "        [ 0.8865, -1.1881],\n",
            "        [-0.4620,  0.0941],\n",
            "        [-1.3560,  1.3761],\n",
            "        [ 0.5656, -0.9545],\n",
            "        [-1.5297,  1.4793],\n",
            "        [ 0.8719, -1.3387],\n",
            "        [ 0.7555, -0.9510],\n",
            "        [ 0.7503, -1.1932],\n",
            "        [-1.3425,  1.5430],\n",
            "        [ 0.1408, -0.4230],\n",
            "        [ 1.1152, -1.1288],\n",
            "        [ 0.0903, -0.5209],\n",
            "        [ 0.7559, -1.1178],\n",
            "        [ 0.6689, -1.0644],\n",
            "        [ 0.9340, -1.2266],\n",
            "        [ 1.1457, -1.0789],\n",
            "        [ 0.5867, -0.6145],\n",
            "        [ 0.9316, -1.1870],\n",
            "        [-1.4411,  1.4946],\n",
            "        [ 0.8883, -1.4410],\n",
            "        [-1.5432,  1.2577],\n",
            "        [ 1.1271, -0.9724],\n",
            "        [-0.2553,  0.4814],\n",
            "        [-0.9843,  0.9237],\n",
            "        [ 0.6894, -1.3001],\n",
            "        [-1.4575,  1.6072],\n",
            "        [ 1.2093, -1.2165],\n",
            "        [ 0.4849, -0.8551],\n",
            "        [ 1.0230, -1.0601],\n",
            "        [ 0.8415, -1.1253],\n",
            "        [ 1.1930, -1.4183]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3303, -0.5751],\n",
            "        [ 1.0500, -1.2678],\n",
            "        [ 0.9773, -1.0658],\n",
            "        [ 0.0830, -0.3853],\n",
            "        [-1.4607,  1.6301],\n",
            "        [-1.7211,  1.4763],\n",
            "        [ 0.9765, -0.8506],\n",
            "        [ 1.0839, -1.3384],\n",
            "        [ 0.9865, -1.2085],\n",
            "        [-1.4330,  1.6306],\n",
            "        [ 0.9314, -1.1111],\n",
            "        [ 0.8344, -1.1397],\n",
            "        [-1.3271,  1.7302],\n",
            "        [ 0.2633, -0.7714],\n",
            "        [-0.1509, -0.0756],\n",
            "        [-1.6712,  1.6394],\n",
            "        [-0.6569,  0.8444],\n",
            "        [-1.0393,  1.0582],\n",
            "        [ 0.8276, -0.9983],\n",
            "        [ 0.9384, -0.9789],\n",
            "        [-1.6460,  1.6797],\n",
            "        [-0.8056,  0.5288],\n",
            "        [ 0.8149, -1.1993],\n",
            "        [-1.2998,  1.3615],\n",
            "        [ 0.6578, -0.8651],\n",
            "        [ 0.7306, -1.2812],\n",
            "        [ 0.1706, -0.2635],\n",
            "        [-1.4400,  1.4931],\n",
            "        [ 0.3576, -0.5142],\n",
            "        [ 0.9957, -1.1068],\n",
            "        [ 1.2423, -1.1329],\n",
            "        [ 0.8255, -1.1788]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0070, -1.3147],\n",
            "        [ 0.9340, -0.9668],\n",
            "        [-1.5273,  1.7091],\n",
            "        [-1.7338,  1.3963],\n",
            "        [ 0.0554, -0.1468],\n",
            "        [ 1.0417, -1.1068],\n",
            "        [-1.5159,  1.5661],\n",
            "        [-1.3505,  1.3756],\n",
            "        [ 0.5951, -0.9700],\n",
            "        [-1.0741,  1.3787],\n",
            "        [ 1.1056, -0.9000],\n",
            "        [ 0.7293, -1.1352],\n",
            "        [-1.6729,  1.8022],\n",
            "        [-1.6236,  1.5395],\n",
            "        [-1.3043,  1.5105],\n",
            "        [-1.1309,  1.0008],\n",
            "        [-0.9507,  0.9401],\n",
            "        [-1.5982,  1.8134],\n",
            "        [-0.3721, -0.0611],\n",
            "        [-1.1120,  1.1796],\n",
            "        [ 1.1246, -1.2511],\n",
            "        [ 0.9826, -1.2131],\n",
            "        [-1.4195,  1.3065],\n",
            "        [-1.5781,  1.5837],\n",
            "        [ 0.8059, -1.1350],\n",
            "        [-1.6138,  1.5003],\n",
            "        [ 0.8993, -1.4363],\n",
            "        [-1.3883,  1.4920],\n",
            "        [-1.4898,  1.6260],\n",
            "        [ 0.3280, -0.1953],\n",
            "        [ 0.4451, -0.8526],\n",
            "        [ 1.1357, -1.0570]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4841, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2305, -0.2675],\n",
            "        [-1.4316,  1.4179],\n",
            "        [-1.7895,  1.6490],\n",
            "        [ 1.0163, -1.2736],\n",
            "        [ 0.7227, -1.0120],\n",
            "        [ 1.2320, -1.0445],\n",
            "        [ 0.4701, -0.6698],\n",
            "        [ 0.2829, -0.8208],\n",
            "        [ 1.0038, -1.4814],\n",
            "        [-1.1136,  1.1819],\n",
            "        [ 0.9892, -1.2460],\n",
            "        [ 0.2467, -0.5599],\n",
            "        [ 0.8814, -1.3384],\n",
            "        [ 0.9720, -1.4024],\n",
            "        [-0.0182, -0.5102],\n",
            "        [-0.8221,  1.0118],\n",
            "        [ 0.7530, -1.1142],\n",
            "        [-1.1121,  1.1609],\n",
            "        [-1.3752,  1.6760],\n",
            "        [ 0.1427, -0.2238],\n",
            "        [ 0.6597, -1.1495],\n",
            "        [ 0.5297, -1.1491],\n",
            "        [ 1.2568, -1.1660],\n",
            "        [-1.4493,  1.6847],\n",
            "        [ 1.1080, -1.2534],\n",
            "        [ 0.0240, -0.3116],\n",
            "        [ 0.5634, -0.9635],\n",
            "        [ 1.1521, -1.1867],\n",
            "        [ 0.6959, -0.9684],\n",
            "        [ 0.3980, -1.0783],\n",
            "        [ 0.9226, -1.2737],\n",
            "        [-1.2729,  1.5425]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3558, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0420, -1.2010],\n",
            "        [ 0.7328, -0.8793],\n",
            "        [ 0.4705, -0.2246],\n",
            "        [-0.2428, -0.0220],\n",
            "        [-0.1594, -0.1371],\n",
            "        [ 0.6289, -0.5927],\n",
            "        [ 0.8323, -0.9843],\n",
            "        [-1.4685,  1.4357],\n",
            "        [ 0.9348, -1.1287],\n",
            "        [ 0.9591, -1.3574],\n",
            "        [ 1.0382, -1.4765],\n",
            "        [ 0.4768, -0.8173],\n",
            "        [ 1.1158, -1.2375],\n",
            "        [ 1.3415, -1.2376],\n",
            "        [-1.5873,  1.8068],\n",
            "        [ 0.9169, -1.1911],\n",
            "        [ 0.3226, -0.2341],\n",
            "        [-1.5076,  1.6698],\n",
            "        [-0.7845,  0.9253],\n",
            "        [ 0.9924, -1.1313],\n",
            "        [ 0.9436, -0.7763],\n",
            "        [-1.4052,  1.7781],\n",
            "        [-0.2740,  0.4714],\n",
            "        [-0.4228,  0.6388],\n",
            "        [-1.4626,  1.5054],\n",
            "        [ 0.7014, -1.0805],\n",
            "        [ 0.4138, -0.7269],\n",
            "        [-1.3454,  1.3938],\n",
            "        [ 0.9463, -1.0043],\n",
            "        [ 0.7486, -1.0434],\n",
            "        [ 0.8667, -1.1496],\n",
            "        [ 0.7987, -1.0409]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9861, -1.0884],\n",
            "        [ 0.8639, -1.2724],\n",
            "        [ 1.0312, -1.2137],\n",
            "        [-1.7261,  1.5335],\n",
            "        [ 0.8637, -0.7939],\n",
            "        [-1.5265,  1.4239],\n",
            "        [-0.8605,  1.1246],\n",
            "        [-1.5782,  1.4259],\n",
            "        [ 0.4217, -0.7099],\n",
            "        [-1.3508,  1.8043],\n",
            "        [-1.1666,  1.4159],\n",
            "        [ 0.9232, -1.2460],\n",
            "        [ 0.7285, -1.1883],\n",
            "        [-0.2489,  0.1551],\n",
            "        [ 1.1371, -1.0076],\n",
            "        [-1.7853,  1.6216],\n",
            "        [ 0.8483, -1.2117],\n",
            "        [ 1.0851, -1.1399],\n",
            "        [ 0.8036, -1.0704],\n",
            "        [ 0.8928, -1.2477],\n",
            "        [ 0.7921, -1.3200],\n",
            "        [-1.4786,  1.5512],\n",
            "        [-0.3839, -0.0281],\n",
            "        [ 0.5128, -0.6377],\n",
            "        [-1.2647,  1.0028],\n",
            "        [-1.1566,  1.6659],\n",
            "        [-0.1198,  0.1745],\n",
            "        [ 0.7573, -0.7985],\n",
            "        [-0.5139,  0.2817],\n",
            "        [-1.6606,  1.4095],\n",
            "        [-1.2437,  1.5871],\n",
            "        [-1.5713,  1.9345]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7577, -0.9885],\n",
            "        [ 0.9880, -1.0699],\n",
            "        [ 0.7494, -1.0138],\n",
            "        [-1.3759,  1.3401],\n",
            "        [ 0.9738, -1.2791],\n",
            "        [-0.8037,  0.8166],\n",
            "        [ 1.0460, -0.8805],\n",
            "        [ 0.6314, -1.2590],\n",
            "        [-1.2520,  1.4260],\n",
            "        [ 0.0996, -0.2856],\n",
            "        [ 0.5583, -0.8694],\n",
            "        [ 0.6442, -0.5825],\n",
            "        [ 1.0510, -1.2184],\n",
            "        [-1.5206,  1.5508],\n",
            "        [-0.7690,  0.8503],\n",
            "        [ 0.8582, -1.2487],\n",
            "        [-1.5289,  1.3565],\n",
            "        [ 1.0488, -1.1665],\n",
            "        [ 1.0665, -1.3989],\n",
            "        [ 0.9530, -1.3787],\n",
            "        [ 0.9267, -1.2465],\n",
            "        [ 0.8250, -0.8695],\n",
            "        [-1.5379,  1.5467],\n",
            "        [ 0.9512, -1.3279],\n",
            "        [ 1.0450, -1.1672],\n",
            "        [ 0.9920, -1.0356],\n",
            "        [ 1.0224, -1.1537],\n",
            "        [-1.7525,  1.5204],\n",
            "        [-1.1064,  1.4356],\n",
            "        [ 1.0644, -1.1408],\n",
            "        [-1.3657,  1.1349],\n",
            "        [-0.8327,  1.1625]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9125, -0.7940],\n",
            "        [ 0.8969, -1.5343],\n",
            "        [ 0.8716, -1.1882],\n",
            "        [ 0.6021, -1.0308],\n",
            "        [ 0.0981, -0.3822],\n",
            "        [ 0.4376, -0.8909],\n",
            "        [-1.7658,  1.8266],\n",
            "        [-1.3606,  1.6314],\n",
            "        [ 0.7355, -0.8445],\n",
            "        [ 0.5076, -0.9078],\n",
            "        [ 0.6815, -0.7474],\n",
            "        [ 1.0640, -1.3583],\n",
            "        [ 1.1277, -1.5645],\n",
            "        [-1.4639,  1.5050],\n",
            "        [-0.2461,  0.2143],\n",
            "        [-0.9295,  1.1547],\n",
            "        [ 0.8577, -1.2884],\n",
            "        [ 0.6604, -1.1593],\n",
            "        [-1.1724,  1.2399],\n",
            "        [ 0.8306, -1.3535],\n",
            "        [ 0.6817, -0.6536],\n",
            "        [ 0.8140, -1.3601],\n",
            "        [ 0.6467, -0.8019],\n",
            "        [ 0.8772, -1.2861],\n",
            "        [-1.6453,  1.7215],\n",
            "        [ 0.7920, -1.3283],\n",
            "        [ 0.3380, -0.5204],\n",
            "        [-0.6308,  0.5135],\n",
            "        [ 0.3892, -0.8981],\n",
            "        [ 0.5208, -0.4527],\n",
            "        [ 0.4235, -0.6388],\n",
            "        [-0.2111,  0.1287]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3109, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8207, -1.0616],\n",
            "        [-1.2703,  1.5030],\n",
            "        [ 1.0402, -0.9908],\n",
            "        [ 0.8095, -1.0292],\n",
            "        [-1.0152,  1.0920],\n",
            "        [ 0.4491, -0.5172],\n",
            "        [ 0.5780, -1.0561],\n",
            "        [-1.1604,  0.9284],\n",
            "        [ 0.8121, -1.1140],\n",
            "        [-0.7191,  0.8777],\n",
            "        [ 0.6717, -1.1936],\n",
            "        [-1.3427,  1.2155],\n",
            "        [ 0.8494, -1.3464],\n",
            "        [ 0.9452, -1.1535],\n",
            "        [ 0.7515, -1.1165],\n",
            "        [-1.1376,  0.9141],\n",
            "        [-1.6571,  1.5067],\n",
            "        [ 0.7198, -0.9446],\n",
            "        [ 1.0723, -1.2196],\n",
            "        [ 0.8813, -1.1640],\n",
            "        [-0.2563,  0.2492],\n",
            "        [ 1.2212, -1.2830],\n",
            "        [ 0.8340, -1.2173],\n",
            "        [ 0.9643, -1.0530],\n",
            "        [ 0.6710, -0.8976],\n",
            "        [ 0.6505, -0.8285],\n",
            "        [-1.0341,  1.0682],\n",
            "        [-1.6717,  1.5466],\n",
            "        [ 0.9861, -1.2012],\n",
            "        [-1.1324,  0.9151],\n",
            "        [-0.1735, -0.1158],\n",
            "        [ 0.9750, -1.1247]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1773, -1.3245],\n",
            "        [ 0.8155, -0.9486],\n",
            "        [ 0.8437, -1.1536],\n",
            "        [-0.9243,  0.6740],\n",
            "        [ 0.7672, -1.0579],\n",
            "        [ 0.8229, -1.0277],\n",
            "        [ 0.7416, -1.2557],\n",
            "        [ 1.0958, -1.3601],\n",
            "        [ 0.6884, -1.1370],\n",
            "        [-1.6212,  1.6682],\n",
            "        [-1.5340,  1.4037],\n",
            "        [ 0.3669, -0.6666],\n",
            "        [ 0.0667, -0.1030],\n",
            "        [ 0.6428, -1.0684],\n",
            "        [ 1.2618, -1.3363],\n",
            "        [ 1.1515, -1.3699],\n",
            "        [ 0.4707, -0.4771],\n",
            "        [-1.6370,  1.5463],\n",
            "        [ 1.0524, -0.8041],\n",
            "        [-1.6290,  1.5781],\n",
            "        [ 1.1281, -1.1306],\n",
            "        [ 0.6855, -0.6479],\n",
            "        [ 1.2376, -1.4694],\n",
            "        [ 1.0041, -1.1849],\n",
            "        [ 0.7698, -0.7209],\n",
            "        [-0.1914, -0.2996],\n",
            "        [ 0.2540,  0.0040],\n",
            "        [-1.5302,  1.4744],\n",
            "        [-0.2724,  0.1886],\n",
            "        [ 0.7778, -1.2530],\n",
            "        [ 0.8778, -1.0741],\n",
            "        [ 1.0334, -1.4186]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4555, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2822, -1.2578],\n",
            "        [ 0.9289, -1.0545],\n",
            "        [ 0.6674, -1.1402],\n",
            "        [-1.5507,  1.4873],\n",
            "        [ 0.5173, -1.2015],\n",
            "        [ 0.8288, -1.3156],\n",
            "        [-1.1842,  0.9871],\n",
            "        [-0.9294,  1.0102],\n",
            "        [ 0.4656, -0.9611],\n",
            "        [ 1.1782, -1.3772],\n",
            "        [ 0.9798, -1.0613],\n",
            "        [-1.6222,  1.5537],\n",
            "        [ 0.5640, -0.9962],\n",
            "        [ 1.0226, -1.1969],\n",
            "        [ 0.9309, -1.1662],\n",
            "        [ 0.7457, -0.8840],\n",
            "        [-1.6506,  1.7704],\n",
            "        [ 0.9327, -1.2658],\n",
            "        [ 0.5072, -0.8562],\n",
            "        [-0.2280,  0.0503],\n",
            "        [ 0.7193, -1.0082],\n",
            "        [ 0.5783, -0.6536],\n",
            "        [-1.4628,  1.3945],\n",
            "        [ 0.8405, -0.9236],\n",
            "        [-1.6799,  1.8588],\n",
            "        [-0.3203,  0.3839],\n",
            "        [ 0.9555, -1.3121],\n",
            "        [-1.1446,  1.4143],\n",
            "        [-0.0155,  0.0339],\n",
            "        [-0.1164, -0.2700],\n",
            "        [ 1.2666, -1.3122],\n",
            "        [-0.4062,  0.4142]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8176, -1.0802],\n",
            "        [-0.7190,  1.0385],\n",
            "        [ 1.0367, -1.0641],\n",
            "        [ 0.3697, -0.5619],\n",
            "        [-1.2761,  1.5154],\n",
            "        [-1.1519,  1.2345],\n",
            "        [ 0.9707, -1.1297],\n",
            "        [-1.1563,  1.1929],\n",
            "        [-0.6747,  0.5784],\n",
            "        [ 0.8955, -1.1410],\n",
            "        [ 0.6449, -0.9080],\n",
            "        [-0.3580,  0.1839],\n",
            "        [-0.9295,  1.1298],\n",
            "        [ 0.7025, -0.5558],\n",
            "        [-1.5822,  1.5622],\n",
            "        [ 0.6430, -0.7219],\n",
            "        [ 1.0629, -1.0145],\n",
            "        [-1.3509,  1.3170],\n",
            "        [-0.4715,  0.2760],\n",
            "        [-1.0070,  1.3983],\n",
            "        [ 0.0531, -0.1851],\n",
            "        [ 0.7689, -1.1413],\n",
            "        [ 1.1312, -1.3172],\n",
            "        [-1.7372,  1.5765],\n",
            "        [-0.9771,  1.1289],\n",
            "        [ 0.9001, -1.2074],\n",
            "        [-0.4634,  0.1062],\n",
            "        [ 0.7783, -1.0253],\n",
            "        [ 1.0052, -1.1359],\n",
            "        [-1.0263,  1.0817],\n",
            "        [ 1.1611, -1.2480],\n",
            "        [-0.5948,  0.5701]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4099, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4135, -0.8616],\n",
            "        [-0.1669,  0.5297],\n",
            "        [-1.1377,  1.0542],\n",
            "        [ 0.4783, -0.9143],\n",
            "        [ 0.8909, -1.1392],\n",
            "        [-1.3621,  1.2852],\n",
            "        [ 1.1362, -1.0795],\n",
            "        [-1.2080,  1.0772],\n",
            "        [ 0.9158, -1.0299],\n",
            "        [ 1.0890, -1.3175],\n",
            "        [ 0.5146, -0.4393],\n",
            "        [-1.3781,  1.4582],\n",
            "        [-0.1534, -0.4013],\n",
            "        [ 0.8929, -1.0224],\n",
            "        [-0.7046,  0.5795],\n",
            "        [-1.4434,  1.5857],\n",
            "        [-1.3929,  1.6318],\n",
            "        [ 1.0677, -1.1963],\n",
            "        [-0.1298,  0.2892],\n",
            "        [-0.0743, -0.4334],\n",
            "        [ 0.9035, -0.8870],\n",
            "        [-1.2390,  1.4446],\n",
            "        [ 0.3601, -0.4833],\n",
            "        [-0.0495, -0.2801],\n",
            "        [ 0.0047, -0.4910],\n",
            "        [ 0.8618, -0.9286],\n",
            "        [-1.4242,  1.5119],\n",
            "        [ 0.8940, -1.0670],\n",
            "        [ 0.8575, -1.3096],\n",
            "        [-0.0281, -0.4720],\n",
            "        [ 0.8475, -1.0630],\n",
            "        [-1.5604,  1.6343]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1613, -0.6495],\n",
            "        [ 1.0694, -1.0627],\n",
            "        [ 0.6492, -1.1201],\n",
            "        [ 0.3277, -0.6984],\n",
            "        [ 0.9706, -1.4141],\n",
            "        [ 1.0742, -1.1151],\n",
            "        [-1.1039,  0.9263],\n",
            "        [ 1.0538, -1.4504],\n",
            "        [ 1.1181, -1.3083],\n",
            "        [-1.5738,  1.8228],\n",
            "        [-0.7199,  0.8117],\n",
            "        [ 0.0180, -0.1922],\n",
            "        [-1.6725,  1.7927],\n",
            "        [-0.2449, -0.2350],\n",
            "        [-1.1749,  1.2679],\n",
            "        [ 0.6982, -1.1010],\n",
            "        [-1.2023,  1.1388],\n",
            "        [ 0.8385, -1.1267],\n",
            "        [ 0.8759, -1.1467],\n",
            "        [ 0.7537, -1.1656],\n",
            "        [ 0.8967, -1.2219],\n",
            "        [ 0.8042, -1.2072],\n",
            "        [ 0.9573, -1.2106],\n",
            "        [ 0.9715, -1.2467],\n",
            "        [-1.5958,  1.5145],\n",
            "        [ 0.6156, -1.1806],\n",
            "        [-1.4693,  1.6798],\n",
            "        [-1.7356,  1.6670],\n",
            "        [-1.3543,  1.5514],\n",
            "        [-1.0794,  1.1858],\n",
            "        [ 1.1769, -1.2942],\n",
            "        [ 0.8060, -1.2934]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4780, -0.9628],\n",
            "        [-1.2272,  1.2271],\n",
            "        [ 0.6560, -1.2232],\n",
            "        [ 0.9388, -0.9199],\n",
            "        [-1.4159,  1.5119],\n",
            "        [ 0.7689, -0.9126],\n",
            "        [ 0.2866, -0.0922],\n",
            "        [-0.7861,  0.6836],\n",
            "        [-1.7084,  1.5868],\n",
            "        [ 1.2597, -1.2811],\n",
            "        [ 0.9370, -1.1473],\n",
            "        [-1.2147,  1.4683],\n",
            "        [ 0.9569, -1.1393],\n",
            "        [-1.5982,  1.6439],\n",
            "        [-1.3626,  1.2094],\n",
            "        [ 0.8012, -0.9741],\n",
            "        [ 0.3405, -0.6631],\n",
            "        [-1.1196,  1.4611],\n",
            "        [-1.4147,  1.3394],\n",
            "        [ 0.8326, -1.2820],\n",
            "        [-1.6665,  1.6017],\n",
            "        [ 0.6803, -0.9174],\n",
            "        [ 0.9038, -1.1249],\n",
            "        [ 0.8646, -0.8765],\n",
            "        [ 0.8790, -0.7750],\n",
            "        [ 0.5682, -1.0879],\n",
            "        [ 0.8528, -1.2136],\n",
            "        [-1.2515,  1.8175],\n",
            "        [ 0.2870, -0.1949],\n",
            "        [ 0.3904, -0.5903],\n",
            "        [ 0.8712, -1.2808],\n",
            "        [ 0.5358, -0.9055]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6378, -0.9117],\n",
            "        [-1.1395,  1.1676],\n",
            "        [ 0.0931, -0.4115],\n",
            "        [ 1.0308, -1.0755],\n",
            "        [-1.3763,  1.6644],\n",
            "        [ 0.9670, -1.2551],\n",
            "        [-1.3926,  1.7046],\n",
            "        [-1.2938,  1.2346],\n",
            "        [-1.4322,  1.4376],\n",
            "        [ 0.6489, -0.9196],\n",
            "        [ 0.7991, -0.8051],\n",
            "        [ 1.2299, -1.3702],\n",
            "        [-1.2900,  1.1719],\n",
            "        [-0.9828,  0.7530],\n",
            "        [ 0.3742, -1.1239],\n",
            "        [-1.4349,  1.5445],\n",
            "        [-0.8668,  0.9370],\n",
            "        [ 1.0228, -1.1431],\n",
            "        [ 0.4863, -0.7507],\n",
            "        [-1.1360,  1.1162],\n",
            "        [-1.2386,  1.5093],\n",
            "        [ 0.5869, -0.8154],\n",
            "        [-1.4710,  1.4056],\n",
            "        [ 0.9856, -1.2050],\n",
            "        [ 0.6482, -1.1666],\n",
            "        [ 0.7496, -1.3313],\n",
            "        [-1.6016,  1.8443],\n",
            "        [ 0.8984, -1.2606],\n",
            "        [ 0.5115, -0.6820],\n",
            "        [ 0.8185, -0.8694],\n",
            "        [-1.3876,  1.4944],\n",
            "        [ 0.8073, -1.1075]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.4868,  1.7962],\n",
            "        [-1.0990,  1.0156],\n",
            "        [-1.1924,  1.4289],\n",
            "        [ 0.7412, -0.7643],\n",
            "        [-1.5968,  1.5447],\n",
            "        [ 0.8955, -1.2115],\n",
            "        [ 0.6378, -0.7265],\n",
            "        [ 0.1527, -0.3417],\n",
            "        [ 0.5796, -0.5581],\n",
            "        [ 0.7542, -1.1424],\n",
            "        [-1.1203,  1.4165],\n",
            "        [-1.4888,  1.6512],\n",
            "        [ 1.0398, -1.0849],\n",
            "        [ 0.9385, -1.1899],\n",
            "        [-0.1843,  0.0274],\n",
            "        [-0.2932,  0.4801],\n",
            "        [-0.5972,  0.6853],\n",
            "        [ 0.9959, -1.0623],\n",
            "        [-1.4317,  1.4660],\n",
            "        [ 1.0360, -1.1194],\n",
            "        [-1.3473,  1.5973],\n",
            "        [ 1.0292, -1.2274],\n",
            "        [ 0.4118, -0.5171],\n",
            "        [ 0.8101, -0.7613],\n",
            "        [-1.4769,  1.5247],\n",
            "        [-1.6401,  1.8191],\n",
            "        [ 1.0433, -1.2783],\n",
            "        [-0.8534,  1.3317],\n",
            "        [-0.5700,  0.2640],\n",
            "        [ 0.9674, -0.7950],\n",
            "        [ 0.4876, -0.6530],\n",
            "        [ 0.2493, -0.3226]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6397, -0.8908],\n",
            "        [ 0.3638, -0.4931],\n",
            "        [-1.4602,  1.5412],\n",
            "        [ 0.9538, -1.1511],\n",
            "        [-1.1555,  1.2719],\n",
            "        [ 0.8704, -1.1228],\n",
            "        [ 0.8104, -1.0909],\n",
            "        [ 1.0424, -1.1038],\n",
            "        [ 0.6947, -0.7121],\n",
            "        [ 0.8911, -1.2676],\n",
            "        [-0.5736,  0.5455],\n",
            "        [-1.0621,  1.1674],\n",
            "        [ 0.6998, -0.9907],\n",
            "        [ 0.1361, -0.1939],\n",
            "        [-1.0310,  1.2782],\n",
            "        [ 1.2351, -1.1144],\n",
            "        [ 0.2745, -0.6187],\n",
            "        [ 0.9445, -1.3971],\n",
            "        [ 0.7195, -0.8288],\n",
            "        [ 0.9978, -1.1190],\n",
            "        [-1.2923,  1.6037],\n",
            "        [ 0.1958, -0.6345],\n",
            "        [-1.3469,  1.5202],\n",
            "        [ 0.7988, -0.9663],\n",
            "        [-1.3380,  1.7031],\n",
            "        [ 0.9545, -0.9587],\n",
            "        [ 1.1016, -1.1814],\n",
            "        [-0.7165,  1.1014],\n",
            "        [ 0.1823, -0.2138],\n",
            "        [ 0.9011, -1.1449],\n",
            "        [-1.3108,  1.5116],\n",
            "        [-1.2919,  1.4450]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7357, -0.8900],\n",
            "        [ 0.8591, -0.7621],\n",
            "        [ 1.2619, -1.5930],\n",
            "        [ 1.0792, -1.4217],\n",
            "        [-0.4401,  0.5826],\n",
            "        [ 0.8523, -1.1700],\n",
            "        [ 0.6398, -0.6935],\n",
            "        [ 0.1062, -0.1539],\n",
            "        [ 0.7686, -1.0803],\n",
            "        [ 0.6543, -0.9609],\n",
            "        [ 0.7956, -1.3115],\n",
            "        [ 0.9417, -1.2467],\n",
            "        [-1.4697,  1.4749],\n",
            "        [ 0.8219, -1.4348],\n",
            "        [ 0.8413, -1.3014],\n",
            "        [-0.2314,  0.1213],\n",
            "        [ 0.7318, -1.0253],\n",
            "        [ 0.1358, -0.7298],\n",
            "        [-1.5773,  1.7797],\n",
            "        [-1.6625,  1.6376],\n",
            "        [ 0.8846, -1.1491],\n",
            "        [-1.2310,  1.4210],\n",
            "        [-1.0115,  1.1552],\n",
            "        [ 0.9383, -1.2129],\n",
            "        [ 1.0787, -0.9868],\n",
            "        [ 1.1631, -1.1319],\n",
            "        [ 1.2151, -1.3567],\n",
            "        [ 0.5901, -0.9014],\n",
            "        [-0.4315,  0.4818],\n",
            "        [ 0.7317, -0.6837],\n",
            "        [ 0.5204, -0.7441],\n",
            "        [ 0.7000, -0.6375]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1574, -1.2110],\n",
            "        [ 0.6736, -1.0661],\n",
            "        [ 0.7877, -0.9945],\n",
            "        [-1.2729,  1.3356],\n",
            "        [ 1.0337, -1.1654],\n",
            "        [-0.4473,  0.6021],\n",
            "        [ 1.1191, -1.0271],\n",
            "        [-0.3446,  0.4138],\n",
            "        [ 0.7352, -0.9072],\n",
            "        [-1.2770,  1.1629],\n",
            "        [ 0.9430, -0.7347],\n",
            "        [ 0.5188, -1.1572],\n",
            "        [ 0.8723, -1.2569],\n",
            "        [-1.0981,  0.9711],\n",
            "        [-1.2236,  1.1366],\n",
            "        [-1.4849,  1.5867],\n",
            "        [-1.1352,  1.1122],\n",
            "        [ 0.8913, -0.7925],\n",
            "        [ 1.0846, -1.2592],\n",
            "        [ 0.1999, -0.8584],\n",
            "        [-1.6789,  1.7116],\n",
            "        [-1.2877,  1.3648],\n",
            "        [-0.4469,  0.4459],\n",
            "        [-0.4545,  0.4068],\n",
            "        [-0.7306,  0.4155],\n",
            "        [-1.1784,  1.0833],\n",
            "        [ 0.8252, -1.0222],\n",
            "        [-1.5449,  1.5267],\n",
            "        [ 0.7317, -1.2140],\n",
            "        [-1.2923,  1.4543],\n",
            "        [ 0.3173, -0.2277],\n",
            "        [-0.8643,  0.8556]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2708, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0665, -1.1130],\n",
            "        [-1.5606,  1.6141],\n",
            "        [ 1.0050, -1.2351],\n",
            "        [ 0.7579, -0.9362],\n",
            "        [-1.7113,  1.6913],\n",
            "        [ 0.3967, -0.3506],\n",
            "        [-1.6867,  1.5598],\n",
            "        [-1.3494,  1.3092],\n",
            "        [ 0.8476, -1.0115],\n",
            "        [ 1.0856, -1.4437],\n",
            "        [ 0.1593, -0.2458],\n",
            "        [-0.4485,  0.6483],\n",
            "        [ 0.1485, -0.2827],\n",
            "        [-0.3655, -0.0841],\n",
            "        [-0.5144,  0.7268],\n",
            "        [ 0.4389, -0.7305],\n",
            "        [ 0.8801, -1.0168],\n",
            "        [-1.4447,  1.7305],\n",
            "        [ 0.8006, -1.1618],\n",
            "        [-1.6827,  1.4939],\n",
            "        [-1.3252,  1.3806],\n",
            "        [ 1.1159, -1.3478],\n",
            "        [-0.9807,  0.8545],\n",
            "        [ 0.6554, -0.6513],\n",
            "        [-0.6005,  0.7573],\n",
            "        [-1.9613,  1.7371],\n",
            "        [ 0.6383, -0.9954],\n",
            "        [-1.5380,  1.8244],\n",
            "        [-1.6473,  1.7588],\n",
            "        [-0.9712,  0.8194],\n",
            "        [-1.4508,  1.4384],\n",
            "        [-1.3113,  1.4352]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6859,  1.0502],\n",
            "        [ 0.1505,  0.0485],\n",
            "        [-0.4097,  0.2225],\n",
            "        [-1.1645,  1.1120],\n",
            "        [-1.3110,  1.1948],\n",
            "        [ 0.7974, -1.0192],\n",
            "        [ 0.8027, -1.0683],\n",
            "        [ 0.8938, -1.0930],\n",
            "        [ 1.0517, -1.2786],\n",
            "        [ 0.6470, -0.1953],\n",
            "        [-1.5284,  1.6263],\n",
            "        [ 0.8781, -1.3787],\n",
            "        [ 1.1241, -1.3276],\n",
            "        [ 0.3674, -0.9206],\n",
            "        [ 0.5238, -0.9212],\n",
            "        [ 0.2574, -0.5338],\n",
            "        [-1.1063,  1.1494],\n",
            "        [ 0.4776, -0.9461],\n",
            "        [-0.2357,  0.0884],\n",
            "        [-0.8997,  0.9065],\n",
            "        [-1.2695,  1.4173],\n",
            "        [ 0.9176, -1.2082],\n",
            "        [ 0.1651, -0.2195],\n",
            "        [ 0.7734, -0.8372],\n",
            "        [ 1.1702, -1.4536],\n",
            "        [ 0.5639, -0.6681],\n",
            "        [-1.6599,  1.4913],\n",
            "        [ 0.9099, -1.1655],\n",
            "        [ 0.8345, -0.9645],\n",
            "        [-1.7779,  1.7213],\n",
            "        [-0.6507,  0.8456],\n",
            "        [ 1.2886, -1.2641]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3634, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5971, -0.9744],\n",
            "        [ 1.1284, -1.2155],\n",
            "        [ 0.7943, -1.1995],\n",
            "        [ 0.7069, -1.2944],\n",
            "        [-0.4396,  0.6501],\n",
            "        [-1.4562,  1.2474],\n",
            "        [ 0.3769, -0.7590],\n",
            "        [ 0.8562, -1.0523],\n",
            "        [ 0.3891, -0.5677],\n",
            "        [ 1.0840, -1.1773],\n",
            "        [-1.3001,  1.5647],\n",
            "        [-1.3022,  1.3528],\n",
            "        [ 0.7837, -0.8112],\n",
            "        [ 1.1274, -0.9792],\n",
            "        [ 1.0513, -1.2068],\n",
            "        [ 0.8498, -1.1914],\n",
            "        [-1.4937,  1.5193],\n",
            "        [-1.2991,  1.4187],\n",
            "        [ 0.8891, -1.1387],\n",
            "        [ 0.8151, -1.0152],\n",
            "        [ 0.9575, -1.0273],\n",
            "        [ 0.9835, -1.4208],\n",
            "        [ 0.7989, -1.4108],\n",
            "        [-0.9801,  1.0020],\n",
            "        [-0.1945,  0.3260],\n",
            "        [ 0.7905, -1.0039],\n",
            "        [ 0.7412, -0.8898],\n",
            "        [ 0.9868, -0.9480],\n",
            "        [ 1.1772, -1.2806],\n",
            "        [ 0.4463, -0.6919],\n",
            "        [ 0.5279, -0.6937],\n",
            "        [ 0.9349, -1.2483]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.6535,  1.7175],\n",
            "        [ 1.0594, -1.0845],\n",
            "        [ 1.1373, -1.3190],\n",
            "        [-0.8450,  0.6239],\n",
            "        [ 0.7868, -1.1739],\n",
            "        [ 0.9110, -1.1468],\n",
            "        [-1.2713,  1.3505],\n",
            "        [ 0.6951, -0.9472],\n",
            "        [ 0.4889, -1.0401],\n",
            "        [-1.1689,  1.3226],\n",
            "        [ 0.8547, -1.0703],\n",
            "        [-1.3061,  1.3583],\n",
            "        [ 0.9649, -1.1136],\n",
            "        [ 0.2359, -0.4296],\n",
            "        [ 0.1991, -0.5103],\n",
            "        [ 0.8889, -0.8046],\n",
            "        [ 1.0615, -1.0194],\n",
            "        [-1.4828,  1.4864],\n",
            "        [-1.2529,  1.3946],\n",
            "        [ 0.4418, -1.1325],\n",
            "        [ 0.2570, -0.7755],\n",
            "        [-1.2378,  1.4665],\n",
            "        [ 0.0051,  0.1060],\n",
            "        [ 0.9933, -1.4908],\n",
            "        [ 0.7708, -1.4209],\n",
            "        [-0.4498,  0.4022],\n",
            "        [ 0.7725, -1.2358],\n",
            "        [ 0.6177, -0.8149],\n",
            "        [ 0.6903, -0.9161],\n",
            "        [ 1.0329, -0.9710],\n",
            "        [-0.2567, -0.0233],\n",
            "        [-1.3439,  1.5155]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6722, -1.0978],\n",
            "        [-1.1760,  1.7651],\n",
            "        [-1.4409,  1.8952],\n",
            "        [ 0.2088, -0.4112],\n",
            "        [ 1.1481, -1.2322],\n",
            "        [-0.4501,  0.5415],\n",
            "        [-1.4929,  1.3332],\n",
            "        [-0.5640,  0.4527],\n",
            "        [ 0.6694, -0.8101],\n",
            "        [ 0.8188, -1.0020],\n",
            "        [-0.1255, -0.2438],\n",
            "        [-0.5415,  0.6826],\n",
            "        [-1.4236,  1.6537],\n",
            "        [ 1.1630, -1.0845],\n",
            "        [ 0.4644, -0.5077],\n",
            "        [-1.2682,  1.7158],\n",
            "        [-1.1303,  1.1849],\n",
            "        [-1.6375,  1.6558],\n",
            "        [-1.6688,  1.5208],\n",
            "        [ 0.9566, -1.3640],\n",
            "        [ 1.0903, -1.2562],\n",
            "        [ 1.1194, -1.3209],\n",
            "        [ 0.7708, -0.9920],\n",
            "        [-0.5798,  0.8291],\n",
            "        [-1.3894,  1.4933],\n",
            "        [ 0.7995, -1.3046],\n",
            "        [-1.5940,  1.8843],\n",
            "        [ 0.3271, -0.7882],\n",
            "        [ 0.2165, -0.4344],\n",
            "        [ 0.8832, -1.0825],\n",
            "        [-1.1784,  1.2372],\n",
            "        [ 1.2033, -1.3812]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5769,  0.7738],\n",
            "        [-0.8651,  0.7964],\n",
            "        [-0.7270,  0.7645],\n",
            "        [-1.2574,  1.2453],\n",
            "        [ 1.1699, -1.0776],\n",
            "        [-0.6402,  0.6643],\n",
            "        [-1.4236,  1.5682],\n",
            "        [ 0.9327, -1.3871],\n",
            "        [-1.4077,  1.8053],\n",
            "        [-1.4687,  1.4402],\n",
            "        [ 1.0740, -1.1298],\n",
            "        [ 0.8971, -1.1163],\n",
            "        [ 1.0837, -1.1883],\n",
            "        [-1.3531,  1.4422],\n",
            "        [ 0.0438, -0.5222],\n",
            "        [ 0.8325, -1.1861],\n",
            "        [-0.4771,  0.5780],\n",
            "        [ 1.0022, -1.3905],\n",
            "        [-0.7778,  0.6058],\n",
            "        [ 0.6657, -0.9321],\n",
            "        [ 0.7107, -0.7569],\n",
            "        [ 0.9503, -1.2345],\n",
            "        [-1.2298,  1.1911],\n",
            "        [ 1.0860, -1.1463],\n",
            "        [ 1.0649, -1.0798],\n",
            "        [-1.4071,  1.5031],\n",
            "        [ 0.8217, -1.2412],\n",
            "        [-1.2990,  1.1532],\n",
            "        [ 0.5491, -0.7256],\n",
            "        [ 0.5361, -0.9328],\n",
            "        [-0.1716,  0.0264],\n",
            "        [-1.5910,  1.6837]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9152, -1.1531],\n",
            "        [-1.3402,  1.5181],\n",
            "        [-1.0543,  0.9646],\n",
            "        [-0.2302, -0.2510],\n",
            "        [ 0.6002, -0.9910],\n",
            "        [-1.4586,  1.4708],\n",
            "        [ 1.2122, -1.3011],\n",
            "        [-1.4999,  1.5767],\n",
            "        [-1.0861,  1.4988],\n",
            "        [ 0.5768, -1.0236],\n",
            "        [ 0.8967, -0.9174],\n",
            "        [-1.5585,  1.7076],\n",
            "        [ 0.8334, -0.8953],\n",
            "        [ 0.8993, -0.9530],\n",
            "        [-1.3631,  1.5130],\n",
            "        [-0.7086,  0.7891],\n",
            "        [ 0.9723, -1.2943],\n",
            "        [-0.6248,  0.5740],\n",
            "        [ 0.8603, -1.3383],\n",
            "        [ 1.3850, -1.2148],\n",
            "        [ 0.7908, -1.0734],\n",
            "        [-1.3492,  1.6837],\n",
            "        [ 0.8078, -1.1442],\n",
            "        [-1.6363,  1.2882],\n",
            "        [-0.0277,  0.0214],\n",
            "        [-0.5445,  0.6588],\n",
            "        [-1.4038,  1.4181],\n",
            "        [-1.4460,  1.5949],\n",
            "        [ 1.0201, -1.4189],\n",
            "        [-1.0747,  1.2798],\n",
            "        [ 0.0594, -0.4181],\n",
            "        [ 0.3438, -0.4077]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8728, -1.1700],\n",
            "        [ 0.7395, -0.8067],\n",
            "        [ 0.2895, -0.6644],\n",
            "        [-0.4674,  0.2133],\n",
            "        [ 0.9065, -1.1569],\n",
            "        [ 0.8998, -1.2029],\n",
            "        [-0.9240,  1.0191],\n",
            "        [ 0.9554, -1.4704],\n",
            "        [-0.2242,  0.3955],\n",
            "        [ 0.8326, -1.1805],\n",
            "        [ 0.8475, -1.0755],\n",
            "        [ 0.8277, -1.2342],\n",
            "        [ 0.8566, -0.9402],\n",
            "        [ 0.0232, -0.3028],\n",
            "        [-1.7568,  1.7791],\n",
            "        [ 0.0988, -0.3927],\n",
            "        [-1.5057,  1.6445],\n",
            "        [ 0.4320, -0.3457],\n",
            "        [ 0.7371, -1.1772],\n",
            "        [ 1.0172, -1.2042],\n",
            "        [-1.2555,  1.2016],\n",
            "        [ 1.1057, -1.0217],\n",
            "        [ 0.7682, -1.2014],\n",
            "        [-1.6540,  1.6851],\n",
            "        [-1.5589,  1.6398],\n",
            "        [ 0.8395, -1.2833],\n",
            "        [-1.3044,  1.6996],\n",
            "        [ 0.4983, -0.5856],\n",
            "        [ 0.9349, -1.0227],\n",
            "        [ 1.0287, -1.2394],\n",
            "        [ 1.0673, -0.9481],\n",
            "        [-1.4551,  1.8107]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0577, -1.0712],\n",
            "        [-0.6430,  0.4392],\n",
            "        [-1.5337,  1.5155],\n",
            "        [-1.3874,  1.5010],\n",
            "        [ 1.2284, -1.2563],\n",
            "        [ 1.1315, -1.4332],\n",
            "        [-1.0409,  1.0377],\n",
            "        [ 0.5279, -0.8703],\n",
            "        [ 0.7650, -0.6154],\n",
            "        [-0.6047,  0.3226],\n",
            "        [-0.5511,  0.4296],\n",
            "        [ 0.8318, -1.2874],\n",
            "        [ 0.7086, -0.7791],\n",
            "        [ 0.3182, -0.7283],\n",
            "        [ 1.0265, -1.0900],\n",
            "        [-1.2944,  1.4565],\n",
            "        [-1.5598,  1.7130],\n",
            "        [-1.6918,  1.7970],\n",
            "        [-0.3467,  0.0398],\n",
            "        [ 0.4175, -0.7148],\n",
            "        [-0.8295,  1.1403],\n",
            "        [ 0.9080, -1.2684],\n",
            "        [-0.6383,  0.6931],\n",
            "        [-0.5301,  0.3426],\n",
            "        [-0.8139,  1.0476],\n",
            "        [ 0.3255, -0.2143],\n",
            "        [ 0.9207, -1.1480],\n",
            "        [ 1.1603, -1.3333],\n",
            "        [ 0.7531, -1.0967],\n",
            "        [ 0.3309, -0.3074],\n",
            "        [ 1.2023, -1.2410],\n",
            "        [-1.4625,  1.4756]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5717,  1.5722],\n",
            "        [ 1.0793, -1.0406],\n",
            "        [ 0.0766, -0.2648],\n",
            "        [-1.4736,  1.6519],\n",
            "        [ 1.0622, -1.5378],\n",
            "        [ 0.8819, -0.9743],\n",
            "        [-0.1430, -0.2686],\n",
            "        [-0.2714,  0.2597],\n",
            "        [ 0.9065, -1.3152],\n",
            "        [ 0.9736, -1.4221],\n",
            "        [-1.4161,  1.6487],\n",
            "        [ 0.9697, -1.2654],\n",
            "        [ 0.5707, -0.7023],\n",
            "        [-1.2492,  1.5464],\n",
            "        [ 0.5767, -0.9008],\n",
            "        [-0.4380,  0.6448],\n",
            "        [ 1.1133, -1.0330],\n",
            "        [ 1.3141, -1.1729],\n",
            "        [-0.5404,  0.4732],\n",
            "        [ 0.4529, -0.8577],\n",
            "        [-1.5898,  1.7703],\n",
            "        [-1.5676,  1.2296],\n",
            "        [-0.3676,  0.3820],\n",
            "        [-1.3488,  1.5290],\n",
            "        [ 1.1139, -1.2609],\n",
            "        [ 0.6883, -1.1388],\n",
            "        [-0.9338,  1.0077],\n",
            "        [ 0.7850, -0.8805],\n",
            "        [ 0.4277, -0.7492],\n",
            "        [ 0.2793, -0.6636],\n",
            "        [ 0.4767, -1.0034],\n",
            "        [-0.5327,  0.6285]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Output: SequenceClassifierOutput(loss=tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5267,  0.5595],\n",
            "        [ 0.7476, -1.0491],\n",
            "        [-1.5433,  1.7262],\n",
            "        [ 0.7540, -1.1480],\n",
            "        [-0.3956,  0.3532],\n",
            "        [ 0.5816, -0.6856],\n",
            "        [-0.3627,  0.4640],\n",
            "        [-0.2671,  0.0839],\n",
            "        [ 0.7238, -1.2756],\n",
            "        [ 1.0947, -1.1744]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0:04:22\n",
            "\n",
            "Running Validation...\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6957, device='cuda:0'), logits=tensor([[ 0.6707, -0.9085],\n",
            "        [-1.4423,  1.5747],\n",
            "        [-1.3038,  1.4296],\n",
            "        [ 0.4389, -0.8842],\n",
            "        [ 0.1545, -0.4480],\n",
            "        [-1.3975,  1.4456],\n",
            "        [-0.9582,  1.1344],\n",
            "        [ 1.0379, -1.1306],\n",
            "        [ 1.0497, -1.2756],\n",
            "        [ 1.1388, -1.2397],\n",
            "        [-1.4658,  1.6304],\n",
            "        [-1.5570,  1.5559],\n",
            "        [-1.4686,  1.6276],\n",
            "        [-1.3986,  1.4041],\n",
            "        [-1.5907,  1.7429],\n",
            "        [ 0.9064, -1.0568],\n",
            "        [ 0.4973, -0.7779],\n",
            "        [ 1.0428, -1.2233],\n",
            "        [ 0.4285, -0.4435],\n",
            "        [ 1.1355, -1.2755],\n",
            "        [-1.3892,  1.6391],\n",
            "        [-0.8828,  0.9211],\n",
            "        [-1.6058,  1.6891],\n",
            "        [ 1.0465, -1.2767],\n",
            "        [-1.2718,  1.3511],\n",
            "        [ 0.9957, -1.2161],\n",
            "        [-1.6544,  1.7742],\n",
            "        [-0.8979,  0.9959],\n",
            "        [-1.5664,  1.7314],\n",
            "        [-0.4770,  0.6083],\n",
            "        [-1.0930,  1.0515],\n",
            "        [ 1.1446, -1.2282]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3212, device='cuda:0'), logits=tensor([[-1.2633,  1.4279],\n",
            "        [ 0.8150, -1.1823],\n",
            "        [-0.2623,  0.2151],\n",
            "        [ 0.9213, -0.9710],\n",
            "        [-1.4851,  1.5603],\n",
            "        [ 0.9950, -1.1521],\n",
            "        [ 0.6668, -0.9526],\n",
            "        [ 0.6158, -0.8870],\n",
            "        [ 0.8813, -1.1807],\n",
            "        [ 1.0210, -1.2663],\n",
            "        [ 0.8673, -1.0913],\n",
            "        [ 0.2852, -0.3288],\n",
            "        [ 1.0198, -1.1642],\n",
            "        [ 0.8765, -1.1674],\n",
            "        [-1.2252,  1.3106],\n",
            "        [ 1.0397, -1.1716],\n",
            "        [ 0.8369, -1.0172],\n",
            "        [-1.4377,  1.5851],\n",
            "        [-1.3129,  1.3673],\n",
            "        [ 0.9505, -1.2482],\n",
            "        [-0.8057,  0.8625],\n",
            "        [-0.7372,  0.5196],\n",
            "        [-0.8451,  0.9426],\n",
            "        [ 0.8384, -1.1377],\n",
            "        [-1.1883,  1.4382],\n",
            "        [ 0.0554, -0.0033],\n",
            "        [ 1.1493, -1.3304],\n",
            "        [ 0.6581, -0.9693],\n",
            "        [ 0.8363, -1.0038],\n",
            "        [-0.7040,  0.6696],\n",
            "        [ 0.9738, -1.2497],\n",
            "        [-0.9635,  1.2720]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3611, device='cuda:0'), logits=tensor([[ 1.0493, -1.1957],\n",
            "        [ 1.0093, -1.1719],\n",
            "        [ 0.4316, -0.7251],\n",
            "        [-1.5306,  1.5852],\n",
            "        [-1.5134,  1.6133],\n",
            "        [-1.6712,  1.7326],\n",
            "        [ 0.8185, -0.9411],\n",
            "        [ 1.0290, -1.2413],\n",
            "        [ 1.0472, -1.0988],\n",
            "        [ 0.4630, -0.6779],\n",
            "        [-0.8861,  0.7369],\n",
            "        [ 0.8099, -1.1372],\n",
            "        [ 0.8117, -1.0476],\n",
            "        [ 1.1193, -1.2322],\n",
            "        [-1.5653,  1.5192],\n",
            "        [-0.6344,  0.6646],\n",
            "        [-1.5349,  1.5930],\n",
            "        [ 0.8760, -0.9825],\n",
            "        [-0.2991,  0.2320],\n",
            "        [ 1.0548, -1.2546],\n",
            "        [ 0.9406, -1.1775],\n",
            "        [-1.5089,  1.7274],\n",
            "        [ 0.9920, -1.2543],\n",
            "        [ 0.9732, -1.1416],\n",
            "        [ 1.0057, -1.1837],\n",
            "        [ 0.1385, -0.3148],\n",
            "        [ 1.0633, -1.1767],\n",
            "        [ 0.8019, -1.0371],\n",
            "        [-1.6951,  1.7432],\n",
            "        [-1.6280,  1.7391],\n",
            "        [ 0.9317, -1.1238],\n",
            "        [-1.4766,  1.5997]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3749, device='cuda:0'), logits=tensor([[ 0.4699, -0.5896],\n",
            "        [-0.6461,  0.6899],\n",
            "        [-0.6693,  0.3819],\n",
            "        [ 1.0226, -1.2955],\n",
            "        [ 1.0224, -1.2744],\n",
            "        [ 0.9389, -1.1516],\n",
            "        [-1.6634,  1.7361],\n",
            "        [-1.0918,  1.3736],\n",
            "        [ 0.9782, -1.1785],\n",
            "        [ 0.8832, -1.0571],\n",
            "        [ 0.8399, -1.0378],\n",
            "        [-1.5377,  1.6945],\n",
            "        [-1.5265,  1.5064],\n",
            "        [ 0.7839, -0.9313],\n",
            "        [ 0.4414, -0.6470],\n",
            "        [-1.1819,  1.3621],\n",
            "        [-1.5338,  1.6765],\n",
            "        [ 1.0915, -1.2519],\n",
            "        [-1.5873,  1.7547],\n",
            "        [-1.6405,  1.7136],\n",
            "        [-0.1239,  0.0509],\n",
            "        [-0.4237,  0.3525],\n",
            "        [ 1.0260, -1.0922],\n",
            "        [-0.2810,  0.2164],\n",
            "        [ 0.3673, -0.4992],\n",
            "        [-0.0587, -0.0756],\n",
            "        [ 0.9192, -1.0589],\n",
            "        [-0.7045,  0.9769],\n",
            "        [-0.3677,  0.2873],\n",
            "        [ 0.9176, -1.0221],\n",
            "        [-1.7224,  1.6948],\n",
            "        [-1.5396,  1.7094]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.1703, device='cuda:0'), logits=tensor([[ 0.8089, -1.2078],\n",
            "        [ 0.3918, -0.7627],\n",
            "        [-1.5960,  1.7848],\n",
            "        [-1.6936,  1.7554],\n",
            "        [ 1.0291, -1.2198],\n",
            "        [ 0.1675, -0.4186],\n",
            "        [ 0.6156, -0.7799],\n",
            "        [-1.6704,  1.7660],\n",
            "        [-1.6604,  1.6667],\n",
            "        [-0.8856,  1.0330],\n",
            "        [ 0.6029, -0.7465],\n",
            "        [-1.5524,  1.7240],\n",
            "        [-1.6299,  1.5665],\n",
            "        [ 1.0905, -1.2733],\n",
            "        [-1.1605,  1.3520],\n",
            "        [-1.5045,  1.6490],\n",
            "        [ 1.0111, -1.1002],\n",
            "        [-1.3186,  1.4153],\n",
            "        [ 1.0676, -1.2379],\n",
            "        [ 1.0504, -1.0946],\n",
            "        [ 0.2560, -0.2839],\n",
            "        [-0.9053,  1.0117],\n",
            "        [-0.8012,  0.8428],\n",
            "        [ 0.9365, -1.0795],\n",
            "        [ 1.0449, -1.2047],\n",
            "        [ 1.0560, -1.1367],\n",
            "        [ 1.0061, -1.2465],\n",
            "        [ 0.4776, -0.7635],\n",
            "        [ 0.8363, -1.1188],\n",
            "        [ 0.9332, -1.1892],\n",
            "        [ 0.9132, -1.1442],\n",
            "        [ 0.8436, -1.0522]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3679, device='cuda:0'), logits=tensor([[-1.6905,  1.7243],\n",
            "        [-1.5126,  1.5881],\n",
            "        [-0.0209, -0.0505],\n",
            "        [ 0.3721, -0.6584],\n",
            "        [ 0.6850, -0.9435],\n",
            "        [ 0.9892, -1.0686],\n",
            "        [-1.7019,  1.7445],\n",
            "        [-1.4689,  1.6585],\n",
            "        [-1.5667,  1.7207],\n",
            "        [ 0.9930, -1.1871],\n",
            "        [-1.6113,  1.6148],\n",
            "        [ 1.0071, -1.2311],\n",
            "        [-0.6185,  0.4430],\n",
            "        [-1.5020,  1.6686],\n",
            "        [-0.4641,  0.4419],\n",
            "        [ 0.8731, -1.0154],\n",
            "        [-0.4275,  0.6720],\n",
            "        [-0.1559,  0.0019],\n",
            "        [-1.5266,  1.6226],\n",
            "        [ 0.4024, -0.6759],\n",
            "        [ 0.9875, -1.1762],\n",
            "        [ 0.2789, -0.5637],\n",
            "        [-1.0821,  1.2073],\n",
            "        [-1.5171,  1.5598],\n",
            "        [ 0.8701, -1.0816],\n",
            "        [-1.6713,  1.6537],\n",
            "        [ 1.1676, -1.3376],\n",
            "        [ 0.8315, -1.0255],\n",
            "        [-1.3023,  1.5345],\n",
            "        [ 0.9956, -1.2096],\n",
            "        [ 0.9147, -0.9941],\n",
            "        [ 0.1948, -0.5547]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4319, device='cuda:0'), logits=tensor([[ 0.7739, -1.0107],\n",
            "        [-1.4972,  1.6902],\n",
            "        [ 1.0948, -1.1287],\n",
            "        [-0.9221,  1.1254],\n",
            "        [ 1.0422, -1.2876],\n",
            "        [-1.5763,  1.5874],\n",
            "        [-1.7206,  1.6966],\n",
            "        [-1.7244,  1.7003],\n",
            "        [ 0.5976, -0.9141],\n",
            "        [-0.4125,  0.4212],\n",
            "        [ 1.1349, -1.3560],\n",
            "        [ 0.8815, -1.0030],\n",
            "        [-1.5093,  1.6125],\n",
            "        [ 0.1579, -0.5415],\n",
            "        [ 0.9397, -1.2471],\n",
            "        [ 0.7006, -1.0156],\n",
            "        [-1.4420,  1.5794],\n",
            "        [-1.4877,  1.5617],\n",
            "        [ 0.8423, -1.1412],\n",
            "        [-1.6420,  1.7388],\n",
            "        [ 0.2687, -0.6390],\n",
            "        [ 0.4778, -0.8116],\n",
            "        [ 1.0738, -1.2350],\n",
            "        [ 0.0082, -0.2916],\n",
            "        [-1.3867,  1.6107],\n",
            "        [-1.4340,  1.5614],\n",
            "        [ 1.0416, -1.1347],\n",
            "        [ 0.9697, -1.1502],\n",
            "        [ 0.5561, -0.2537],\n",
            "        [ 0.9117, -1.0423],\n",
            "        [ 0.9661, -1.1559],\n",
            "        [ 0.8734, -1.0489]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5582, device='cuda:0'), logits=tensor([[ 0.7557, -0.9771],\n",
            "        [ 0.9732, -1.2210],\n",
            "        [-1.5340,  1.6402],\n",
            "        [-1.6463,  1.7406],\n",
            "        [ 0.9190, -1.1387],\n",
            "        [ 0.9078, -1.1041],\n",
            "        [ 0.6815, -0.9078],\n",
            "        [ 0.5451, -0.8642],\n",
            "        [ 0.1792, -0.4417],\n",
            "        [-0.9866,  1.2069],\n",
            "        [ 0.7520, -0.9150],\n",
            "        [ 1.0850, -1.0681],\n",
            "        [ 0.9633, -1.2452],\n",
            "        [-1.6926,  1.6775],\n",
            "        [-1.5741,  1.7364],\n",
            "        [ 1.1310, -1.2952],\n",
            "        [-0.5598,  0.9026],\n",
            "        [ 0.7338, -1.0305],\n",
            "        [ 0.2744, -0.7033],\n",
            "        [ 0.8567, -1.0989],\n",
            "        [-1.6466,  1.6995],\n",
            "        [-1.2715,  1.3576],\n",
            "        [-0.6229,  0.5392],\n",
            "        [ 0.9958, -1.1739],\n",
            "        [-1.6843,  1.7517],\n",
            "        [ 1.0427, -1.1936],\n",
            "        [ 1.0873, -1.2955],\n",
            "        [ 0.9721, -1.2866],\n",
            "        [-1.4526,  1.5984],\n",
            "        [-1.1571,  1.1342],\n",
            "        [ 0.1669, -0.3952],\n",
            "        [ 0.9467, -1.2567]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3488, device='cuda:0'), logits=tensor([[ 0.0078, -0.2921],\n",
            "        [ 1.0102, -1.1426],\n",
            "        [ 0.9317, -1.1821],\n",
            "        [ 0.9708, -1.2655],\n",
            "        [-1.3649,  1.4573],\n",
            "        [ 0.7918, -0.9168],\n",
            "        [-0.5642,  0.7220],\n",
            "        [-1.2624,  1.4360],\n",
            "        [-1.7002,  1.7716],\n",
            "        [-1.5277,  1.5790],\n",
            "        [ 1.0778, -1.2149],\n",
            "        [ 0.6656, -1.0364],\n",
            "        [ 0.1052, -0.5345],\n",
            "        [ 0.7824, -1.1038],\n",
            "        [ 0.0649, -0.4750],\n",
            "        [ 0.8723, -1.0876],\n",
            "        [ 0.5645, -0.8339],\n",
            "        [ 0.1499, -0.1229],\n",
            "        [-0.0384,  0.0826],\n",
            "        [ 0.9385, -1.0789],\n",
            "        [-1.3433,  1.3437],\n",
            "        [-1.5032,  1.7498],\n",
            "        [-1.6681,  1.6709],\n",
            "        [-1.1714,  1.1020],\n",
            "        [ 0.4472, -0.6655],\n",
            "        [-0.5201,  0.6346],\n",
            "        [-1.3395,  1.4505],\n",
            "        [ 0.9831, -1.2260],\n",
            "        [-1.1436,  1.4246],\n",
            "        [ 0.8688, -1.2478],\n",
            "        [ 0.1401, -0.3927],\n",
            "        [ 0.6416, -0.8961]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4410, device='cuda:0'), logits=tensor([[ 0.3561, -0.6474],\n",
            "        [-1.5160,  1.6403],\n",
            "        [-1.6697,  1.7605],\n",
            "        [ 1.0068, -1.2234],\n",
            "        [-0.7808,  1.1662],\n",
            "        [-1.6694,  1.7383],\n",
            "        [-1.4858,  1.4753],\n",
            "        [-1.7327,  1.7813],\n",
            "        [-0.3708,  0.4149],\n",
            "        [-1.5579,  1.6235],\n",
            "        [-0.8874,  0.8465],\n",
            "        [-0.0456,  0.0351],\n",
            "        [ 0.1614, -0.5854],\n",
            "        [-1.5869,  1.6458],\n",
            "        [ 0.7185, -0.6034],\n",
            "        [ 0.8691, -1.0079],\n",
            "        [-1.5584,  1.5431],\n",
            "        [ 0.9460, -1.1194],\n",
            "        [-0.8929,  0.8703],\n",
            "        [-1.6181,  1.5473],\n",
            "        [-1.0223,  1.0120],\n",
            "        [ 1.0225, -1.1969],\n",
            "        [-0.5705,  0.6073],\n",
            "        [-1.5434,  1.5134],\n",
            "        [-1.1021,  1.1353],\n",
            "        [-1.6359,  1.7630],\n",
            "        [ 0.7519, -1.1322],\n",
            "        [ 0.8381, -1.0274],\n",
            "        [-1.5440,  1.6664],\n",
            "        [ 0.5824, -0.7798],\n",
            "        [ 0.9529, -1.0983],\n",
            "        [ 0.7955, -1.0026]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4567, device='cuda:0'), logits=tensor([[-1.5764,  1.6575],\n",
            "        [ 0.6784, -0.8906],\n",
            "        [ 0.9262, -1.0992],\n",
            "        [ 0.9487, -1.0562],\n",
            "        [-1.5511,  1.6983],\n",
            "        [-0.3939,  0.0540],\n",
            "        [-1.6829,  1.7836],\n",
            "        [ 0.4418, -0.7938],\n",
            "        [ 1.0267, -1.2421],\n",
            "        [ 1.0160, -1.1648],\n",
            "        [-0.2012,  0.2543],\n",
            "        [-1.6407,  1.6959],\n",
            "        [ 0.9006, -1.1078],\n",
            "        [ 0.7172, -0.8033],\n",
            "        [ 0.2386, -0.5012],\n",
            "        [ 0.9569, -1.2237],\n",
            "        [ 0.8204, -1.1617],\n",
            "        [-1.1518,  1.2762],\n",
            "        [-1.4874,  1.7639],\n",
            "        [ 0.1090, -0.2209],\n",
            "        [-1.6698,  1.7420],\n",
            "        [ 0.8666, -0.9063],\n",
            "        [-1.5742,  1.7011],\n",
            "        [ 0.4853, -0.7243],\n",
            "        [-1.2745,  1.4921],\n",
            "        [ 0.9787, -1.2429],\n",
            "        [ 1.1080, -1.0639],\n",
            "        [-1.6028,  1.7553],\n",
            "        [-0.8775,  1.0905],\n",
            "        [ 0.7088, -0.9136],\n",
            "        [ 0.8149, -1.0208],\n",
            "        [-0.5610,  0.6945]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4711, device='cuda:0'), logits=tensor([[ 0.7081, -0.9690],\n",
            "        [-1.2878,  1.1307],\n",
            "        [-1.0110,  0.8526],\n",
            "        [ 0.6392, -0.7831],\n",
            "        [-1.4937,  1.6053],\n",
            "        [-0.6569,  0.7584],\n",
            "        [-0.9436,  1.0457],\n",
            "        [ 1.0390, -1.2842],\n",
            "        [ 0.4696, -0.7629],\n",
            "        [-1.5893,  1.6651],\n",
            "        [-0.3186,  0.0637],\n",
            "        [-1.6072,  1.6788],\n",
            "        [-1.5048,  1.6314],\n",
            "        [ 1.0154, -1.2059],\n",
            "        [ 0.0595, -0.1918],\n",
            "        [-1.5296,  1.6239],\n",
            "        [ 1.0746, -1.2517],\n",
            "        [ 0.8528, -1.0295],\n",
            "        [ 1.0272, -1.2956],\n",
            "        [ 0.9633, -1.1377],\n",
            "        [-1.5587,  1.6821],\n",
            "        [ 1.1028, -1.2884],\n",
            "        [ 0.9967, -1.2840],\n",
            "        [ 0.9688, -1.1157],\n",
            "        [-1.6396,  1.6517],\n",
            "        [-0.6634,  0.8375],\n",
            "        [ 1.1321, -1.2817],\n",
            "        [ 1.0148, -1.2265],\n",
            "        [ 0.8922, -1.1574],\n",
            "        [-0.5043,  0.5407],\n",
            "        [ 1.0858, -1.1723],\n",
            "        [-1.6556,  1.7754]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4649, device='cuda:0'), logits=tensor([[-0.4087,  0.3850],\n",
            "        [-0.2819,  0.1778],\n",
            "        [-1.3642,  1.5639],\n",
            "        [ 1.0178, -1.2234],\n",
            "        [-0.6323,  0.7122],\n",
            "        [ 0.9202, -1.1189],\n",
            "        [ 1.0707, -1.2027],\n",
            "        [ 0.7016, -0.9981],\n",
            "        [ 0.8977, -1.1396],\n",
            "        [-1.6361,  1.7527],\n",
            "        [ 0.7046, -1.0013],\n",
            "        [-1.5445,  1.7275],\n",
            "        [ 0.7922, -0.9686],\n",
            "        [ 0.8194, -1.1900],\n",
            "        [ 0.8728, -1.0686],\n",
            "        [-1.5219,  1.6328],\n",
            "        [ 1.0110, -1.0777],\n",
            "        [ 1.0881, -1.2007],\n",
            "        [ 1.0430, -1.1791],\n",
            "        [ 1.0161, -1.3482],\n",
            "        [ 1.0191, -1.3214],\n",
            "        [-1.2938,  1.4018],\n",
            "        [ 0.8119, -1.1069],\n",
            "        [ 0.5402, -0.8198],\n",
            "        [-1.2848,  1.3950],\n",
            "        [ 1.0561, -1.2235],\n",
            "        [ 0.8715, -1.1961],\n",
            "        [-1.2738,  1.1354],\n",
            "        [ 1.1038, -1.2722],\n",
            "        [ 0.8234, -1.0900],\n",
            "        [ 0.6821, -1.0312],\n",
            "        [ 0.1541, -0.1789]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4686, device='cuda:0'), logits=tensor([[-1.6071,  1.6974],\n",
            "        [-0.4397,  0.3358],\n",
            "        [ 0.1527, -0.4300],\n",
            "        [-1.6117,  1.6734],\n",
            "        [ 0.8560, -1.1461],\n",
            "        [ 0.8253, -1.1020],\n",
            "        [-1.4669,  1.4714],\n",
            "        [-1.5875,  1.7219],\n",
            "        [ 0.9586, -1.2402],\n",
            "        [-1.6319,  1.7348],\n",
            "        [-0.3434,  0.2492],\n",
            "        [ 0.4141, -0.7822],\n",
            "        [ 1.0145, -1.1495],\n",
            "        [ 0.9190, -1.2219],\n",
            "        [ 0.5982, -0.4383],\n",
            "        [-1.5112,  1.6383],\n",
            "        [ 0.3627, -0.7542],\n",
            "        [-1.2854,  1.5594],\n",
            "        [-0.6115,  0.7345],\n",
            "        [-0.4079,  0.3023],\n",
            "        [-1.2387,  1.2665],\n",
            "        [ 1.0873, -1.2216],\n",
            "        [-1.5544,  1.5930],\n",
            "        [ 0.6119, -1.0007],\n",
            "        [ 0.6818, -0.9043],\n",
            "        [ 0.8517, -1.0744],\n",
            "        [-1.5689,  1.7031],\n",
            "        [ 0.9576, -1.0948],\n",
            "        [ 1.0519, -1.2159],\n",
            "        [ 0.8600, -0.9964],\n",
            "        [ 0.3655, -0.6035],\n",
            "        [ 0.8565, -1.0391]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4633, device='cuda:0'), logits=tensor([[-1.4492,  1.6030],\n",
            "        [-1.3084,  1.3577],\n",
            "        [ 0.9917, -1.2510],\n",
            "        [ 0.9960, -1.0626],\n",
            "        [ 0.6413, -0.7393],\n",
            "        [ 0.7808, -0.8940],\n",
            "        [-1.2514,  1.3344],\n",
            "        [-1.3162,  1.5940],\n",
            "        [-0.9306,  0.9083],\n",
            "        [ 0.7301, -0.8040],\n",
            "        [ 0.1851, -0.1558],\n",
            "        [-1.5512,  1.6804],\n",
            "        [ 0.7714, -0.8204],\n",
            "        [ 1.0424, -1.1870],\n",
            "        [ 0.7011, -1.0215],\n",
            "        [-1.5451,  1.6957],\n",
            "        [-0.3697,  0.5948],\n",
            "        [ 1.0072, -1.0390],\n",
            "        [-1.6648,  1.6771],\n",
            "        [ 0.9393, -1.1197],\n",
            "        [-1.6357,  1.7835],\n",
            "        [-1.5754,  1.5322],\n",
            "        [ 0.9179, -1.1159],\n",
            "        [ 0.9894, -1.1340],\n",
            "        [ 0.9317, -1.2631],\n",
            "        [-1.2165,  1.4083],\n",
            "        [-1.5045,  1.5870],\n",
            "        [-1.1105,  1.3514],\n",
            "        [ 0.8911, -1.1292],\n",
            "        [ 0.9856, -1.2100],\n",
            "        [-1.5548,  1.5320],\n",
            "        [ 0.8524, -1.0678]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6057, device='cuda:0'), logits=tensor([[ 0.8451, -1.1332],\n",
            "        [-1.3273,  1.4665],\n",
            "        [-1.7317,  1.7025],\n",
            "        [ 0.8738, -1.0699],\n",
            "        [ 0.8755, -1.0311],\n",
            "        [-0.6314,  0.6690],\n",
            "        [-0.0064, -0.2168],\n",
            "        [ 0.9246, -1.2831],\n",
            "        [-0.6602,  0.7113],\n",
            "        [-0.2212,  0.0472],\n",
            "        [ 0.5966, -0.9382],\n",
            "        [ 0.7275, -1.0047],\n",
            "        [ 0.9897, -1.2142],\n",
            "        [ 0.5876, -0.8883],\n",
            "        [-1.6070,  1.7842],\n",
            "        [-1.5744,  1.7003],\n",
            "        [ 0.6995, -0.6448],\n",
            "        [-0.4921,  0.3982],\n",
            "        [-1.6821,  1.6986],\n",
            "        [ 1.0109, -1.1237],\n",
            "        [ 0.9416, -0.9113],\n",
            "        [ 1.1088, -1.2654],\n",
            "        [ 1.0095, -1.2398],\n",
            "        [-1.0497,  1.0798],\n",
            "        [-1.2489,  1.3629],\n",
            "        [ 0.3531, -0.5960],\n",
            "        [-0.6629,  0.7329],\n",
            "        [ 0.4798, -0.5640],\n",
            "        [ 0.7298, -0.9106],\n",
            "        [-0.1524, -0.1989],\n",
            "        [ 0.8088, -1.0647],\n",
            "        [ 1.1696, -1.3266]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3872, device='cuda:0'), logits=tensor([[-0.8212,  0.9371],\n",
            "        [-1.5944,  1.6667],\n",
            "        [-1.4872,  1.6305],\n",
            "        [ 0.2716, -0.2518],\n",
            "        [-1.5708,  1.6680],\n",
            "        [-1.5406,  1.5242],\n",
            "        [-1.6499,  1.7205],\n",
            "        [ 1.0777, -1.2617],\n",
            "        [ 0.3569, -0.4538],\n",
            "        [ 0.8704, -1.1297],\n",
            "        [-1.5915,  1.7434],\n",
            "        [-0.8520,  0.9369],\n",
            "        [-1.0506,  0.9469],\n",
            "        [ 0.7594, -0.9296],\n",
            "        [ 1.0081, -1.0123],\n",
            "        [ 1.0066, -1.1784],\n",
            "        [ 0.1226, -0.3230],\n",
            "        [-0.6538,  0.3936],\n",
            "        [ 0.9923, -1.1994],\n",
            "        [-0.5885,  0.7081],\n",
            "        [ 0.8045, -1.0839],\n",
            "        [ 1.0413, -1.2910],\n",
            "        [-1.5302,  1.4740],\n",
            "        [ 0.6400, -0.6667],\n",
            "        [ 1.0316, -1.1921],\n",
            "        [ 0.8320, -0.8455],\n",
            "        [-1.3619,  1.6948],\n",
            "        [ 0.9080, -0.9562],\n",
            "        [ 0.9819, -1.1197],\n",
            "        [ 0.2740, -0.4310],\n",
            "        [-1.4567,  1.6711],\n",
            "        [ 0.9429, -1.2646]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4054, device='cuda:0'), logits=tensor([[-1.4530,  1.3689],\n",
            "        [ 1.0167, -1.2730],\n",
            "        [-1.5664,  1.6448],\n",
            "        [ 1.0261, -1.1835],\n",
            "        [ 0.3964, -0.6818],\n",
            "        [-1.6222,  1.6040],\n",
            "        [ 1.0009, -1.2661],\n",
            "        [-0.8986,  0.8883],\n",
            "        [ 1.0717, -1.2904],\n",
            "        [-1.6195,  1.6993],\n",
            "        [ 0.9208, -1.0253],\n",
            "        [-0.1174, -0.1860],\n",
            "        [ 1.0907, -1.2413],\n",
            "        [ 0.7175, -1.0481],\n",
            "        [-1.6390,  1.7163],\n",
            "        [ 0.9432, -1.1601],\n",
            "        [ 0.5823, -0.8699],\n",
            "        [-1.6669,  1.7271],\n",
            "        [-1.2002,  0.9658],\n",
            "        [ 1.0139, -1.2456],\n",
            "        [ 0.8333, -1.0163],\n",
            "        [ 0.5322, -0.7676],\n",
            "        [ 0.7629, -0.9811],\n",
            "        [-1.4121,  1.6093],\n",
            "        [ 0.6406, -0.8790],\n",
            "        [ 0.9345, -0.9603],\n",
            "        [ 0.8021, -0.8827],\n",
            "        [ 0.8419, -0.8667],\n",
            "        [ 1.0049, -1.2088],\n",
            "        [ 0.9192, -1.0957],\n",
            "        [ 0.9511, -1.1863],\n",
            "        [-1.5175,  1.7327]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3809, device='cuda:0'), logits=tensor([[ 0.9123, -1.0364],\n",
            "        [-0.6060,  0.6193],\n",
            "        [ 0.9283, -1.1480],\n",
            "        [ 0.9843, -1.2591],\n",
            "        [-0.5507,  0.3732],\n",
            "        [-1.4987,  1.7319],\n",
            "        [ 0.1743, -0.5327],\n",
            "        [-0.2897,  0.1334],\n",
            "        [-0.9948,  1.3913],\n",
            "        [-1.5444,  1.6665],\n",
            "        [ 0.9407, -1.0532],\n",
            "        [ 0.8910, -1.0305],\n",
            "        [-1.4318,  1.6658],\n",
            "        [ 0.9583, -1.1609],\n",
            "        [ 0.5624, -0.8276],\n",
            "        [ 0.6788, -0.9413],\n",
            "        [-0.9991,  0.9864],\n",
            "        [-1.5830,  1.6429],\n",
            "        [ 0.9822, -1.2514],\n",
            "        [-1.5454,  1.5923],\n",
            "        [-1.6022,  1.5988],\n",
            "        [ 0.8922, -1.0511],\n",
            "        [-0.9347,  0.8001],\n",
            "        [-1.5249,  1.7797],\n",
            "        [ 0.0815, -0.2600],\n",
            "        [ 0.7311, -1.0222],\n",
            "        [ 0.1718, -0.1414],\n",
            "        [ 0.8567, -1.1120],\n",
            "        [-1.6849,  1.6961],\n",
            "        [ 0.7067, -1.0053],\n",
            "        [-0.1437,  0.1291],\n",
            "        [-0.8245,  0.8280]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3457, device='cuda:0'), logits=tensor([[-1.5984,  1.7102],\n",
            "        [ 0.4931, -0.8596],\n",
            "        [ 0.6135, -0.8548],\n",
            "        [ 1.0799, -1.2625],\n",
            "        [-1.6397,  1.7959],\n",
            "        [-0.0693, -0.1644],\n",
            "        [-1.6413,  1.8006],\n",
            "        [ 1.0459, -1.1683],\n",
            "        [ 1.1131, -1.2047],\n",
            "        [ 0.4025, -0.6897],\n",
            "        [ 0.8008, -1.0256],\n",
            "        [ 0.7747, -1.0274],\n",
            "        [-1.6576,  1.7616],\n",
            "        [ 0.3507, -0.6193],\n",
            "        [ 0.7581, -1.0179],\n",
            "        [ 0.2614, -0.4733],\n",
            "        [ 0.9969, -1.0660],\n",
            "        [-1.5758,  1.6214],\n",
            "        [ 0.0869, -0.1802],\n",
            "        [-1.5867,  1.6276],\n",
            "        [-1.3353,  1.3611],\n",
            "        [ 1.0048, -1.1711],\n",
            "        [ 0.9402, -1.1967],\n",
            "        [ 0.9725, -1.1522],\n",
            "        [-0.4861,  0.5365],\n",
            "        [ 0.6167, -0.9058],\n",
            "        [ 0.7598, -0.8066],\n",
            "        [-0.9383,  1.1112],\n",
            "        [-0.0810,  0.2045],\n",
            "        [-0.3934,  0.4711],\n",
            "        [-1.6000,  1.7614],\n",
            "        [-1.5105,  1.6811]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3681, device='cuda:0'), logits=tensor([[-1.5899,  1.5417],\n",
            "        [-0.5403,  0.6747],\n",
            "        [ 1.0327, -1.2038],\n",
            "        [-1.5741,  1.5893],\n",
            "        [ 0.0574, -0.2698],\n",
            "        [ 1.1476, -1.2368],\n",
            "        [-1.1342,  1.1438],\n",
            "        [ 1.0253, -1.2146],\n",
            "        [ 0.8109, -1.0398],\n",
            "        [ 0.2659, -0.4840],\n",
            "        [ 0.9571, -1.1993],\n",
            "        [ 0.7767, -0.9052],\n",
            "        [ 0.5362, -0.8620],\n",
            "        [-1.5097,  1.5921],\n",
            "        [ 0.9788, -1.2325],\n",
            "        [ 1.0704, -1.1923],\n",
            "        [ 0.8672, -1.1207],\n",
            "        [ 0.4321, -0.7178],\n",
            "        [ 1.1232, -1.2944],\n",
            "        [-1.3396,  1.4449],\n",
            "        [-1.1721,  1.0757],\n",
            "        [-1.5846,  1.6468],\n",
            "        [-1.6752,  1.7207],\n",
            "        [-0.7746,  0.5121],\n",
            "        [ 0.7481, -0.9901],\n",
            "        [-1.4881,  1.5955],\n",
            "        [ 0.3405, -0.6440],\n",
            "        [ 0.9615, -1.1884],\n",
            "        [ 0.2506, -0.3496],\n",
            "        [ 0.3943, -0.3585],\n",
            "        [ 0.8808, -1.1506],\n",
            "        [ 0.5281, -0.7007]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3900, device='cuda:0'), logits=tensor([[-1.6822,  1.8043],\n",
            "        [ 0.8964, -1.0364],\n",
            "        [ 0.8816, -1.0732],\n",
            "        [ 0.9064, -0.9400],\n",
            "        [ 1.0327, -1.1460],\n",
            "        [-1.5242,  1.5555],\n",
            "        [ 0.3234, -0.5324],\n",
            "        [ 0.9456, -1.1765],\n",
            "        [-1.5115,  1.6894],\n",
            "        [ 0.9790, -1.2373],\n",
            "        [-1.1276,  1.1402],\n",
            "        [ 0.0433, -0.2448],\n",
            "        [-1.4418,  1.5868],\n",
            "        [-0.6711,  1.0510],\n",
            "        [-0.7288,  0.7855],\n",
            "        [-1.6517,  1.7743],\n",
            "        [-1.6219,  1.6507],\n",
            "        [-0.6392,  0.5807],\n",
            "        [ 1.0130, -1.2476],\n",
            "        [ 0.8278, -1.1477],\n",
            "        [ 0.8362, -1.1490],\n",
            "        [-1.5488,  1.6842],\n",
            "        [ 0.9912, -1.2347],\n",
            "        [-1.5190,  1.4087],\n",
            "        [-1.3498,  1.5471],\n",
            "        [-1.7076,  1.8196],\n",
            "        [-1.1563,  1.2563],\n",
            "        [-1.1947,  1.3788],\n",
            "        [-1.6273,  1.6573],\n",
            "        [ 0.4158, -0.7951],\n",
            "        [-1.4408,  1.5544],\n",
            "        [ 0.7070, -1.0665]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.7229, device='cuda:0'), logits=tensor([[ 1.0331, -1.2797],\n",
            "        [-1.0523,  0.9348],\n",
            "        [ 0.8678, -1.1923],\n",
            "        [-0.6780,  0.6414],\n",
            "        [-1.6127,  1.7571],\n",
            "        [-1.2047,  1.4578],\n",
            "        [ 0.8390, -1.0890],\n",
            "        [-0.7066,  0.7169],\n",
            "        [-1.6129,  1.5113],\n",
            "        [-1.2352,  1.2861],\n",
            "        [ 0.5092, -0.9049],\n",
            "        [ 0.7070, -0.8983],\n",
            "        [-1.0061,  1.1001],\n",
            "        [-1.0883,  0.8458],\n",
            "        [ 1.1755, -1.3027],\n",
            "        [-1.5994,  1.6668],\n",
            "        [ 0.9369, -1.0502],\n",
            "        [ 0.9813, -1.3147],\n",
            "        [-1.6525,  1.6434],\n",
            "        [-1.5811,  1.5910],\n",
            "        [-1.3795,  1.3869],\n",
            "        [ 1.0661, -1.1887],\n",
            "        [ 0.4604, -0.7054],\n",
            "        [-1.2266,  1.2330],\n",
            "        [ 0.7171, -1.0359],\n",
            "        [-1.3424,  1.4731],\n",
            "        [-1.0567,  1.4411],\n",
            "        [ 0.9829, -1.1537],\n",
            "        [ 0.3185, -0.6242],\n",
            "        [ 1.1080, -1.2473],\n",
            "        [ 0.9466, -1.2099],\n",
            "        [ 0.4506, -0.7983]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2085, device='cuda:0'), logits=tensor([[-0.7330,  0.8887],\n",
            "        [ 0.9045, -1.0713],\n",
            "        [-1.5698,  1.6434],\n",
            "        [ 0.9955, -1.0452],\n",
            "        [-1.6067,  1.6274],\n",
            "        [ 0.9528, -1.1549],\n",
            "        [ 0.9434, -1.1477],\n",
            "        [ 0.6885, -0.8113],\n",
            "        [ 0.5758, -0.8485],\n",
            "        [-1.6815,  1.7670],\n",
            "        [ 1.0532, -1.2795],\n",
            "        [-1.3101,  1.5977],\n",
            "        [ 0.3284, -0.3849],\n",
            "        [-1.6418,  1.7001],\n",
            "        [-0.2314, -0.0678],\n",
            "        [ 0.9305, -1.0887],\n",
            "        [ 0.9036, -0.9851],\n",
            "        [-1.6436,  1.6252],\n",
            "        [-1.2680,  1.2559],\n",
            "        [-1.2357,  1.2642],\n",
            "        [ 1.0824, -1.1737],\n",
            "        [ 0.5278, -0.6770],\n",
            "        [ 0.8187, -1.1289],\n",
            "        [ 0.7703, -1.0682],\n",
            "        [ 0.6281, -0.7086],\n",
            "        [ 0.0047, -0.4409],\n",
            "        [ 0.0870, -0.3364],\n",
            "        [-1.7045,  1.6726],\n",
            "        [-1.5399,  1.6104],\n",
            "        [ 0.8737, -1.1161],\n",
            "        [ 0.8608, -1.0816],\n",
            "        [-1.6802,  1.8158]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3232, device='cuda:0'), logits=tensor([[-0.2949,  0.1383],\n",
            "        [ 0.9778, -1.2193],\n",
            "        [ 1.0751, -1.3447],\n",
            "        [-1.4621,  1.4592],\n",
            "        [ 0.9384, -1.2455],\n",
            "        [ 0.2907, -0.6378],\n",
            "        [ 0.7402, -0.9492],\n",
            "        [-1.0972,  1.3625],\n",
            "        [-1.5837,  1.7108],\n",
            "        [-1.6020,  1.6149],\n",
            "        [ 0.2864, -0.5118],\n",
            "        [-0.7924,  0.8923],\n",
            "        [-0.2278,  0.4253],\n",
            "        [ 0.9607, -1.1242],\n",
            "        [-0.5605,  0.4551],\n",
            "        [ 0.6580, -0.9813],\n",
            "        [-1.5342,  1.5313],\n",
            "        [ 1.0044, -1.2915],\n",
            "        [ 1.0086, -1.1733],\n",
            "        [-1.5044,  1.6414],\n",
            "        [-1.3893,  1.4517],\n",
            "        [ 0.9222, -1.1612],\n",
            "        [-1.5404,  1.6863],\n",
            "        [ 1.0248, -1.2035],\n",
            "        [-1.5596,  1.7194],\n",
            "        [ 0.5974, -0.8213],\n",
            "        [ 1.0019, -1.1182],\n",
            "        [ 1.0864, -1.2300],\n",
            "        [-0.4729,  0.3308],\n",
            "        [ 0.9009, -0.9944],\n",
            "        [ 0.6209, -0.8966],\n",
            "        [-1.5631,  1.7345]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3140, device='cuda:0'), logits=tensor([[ 0.8471, -1.0835],\n",
            "        [ 0.9645, -1.2850],\n",
            "        [ 0.7995, -1.0874],\n",
            "        [ 0.5722, -0.8590],\n",
            "        [ 0.9379, -1.1643],\n",
            "        [ 0.9409, -1.0888],\n",
            "        [ 1.1358, -1.3372],\n",
            "        [ 0.9637, -1.1396],\n",
            "        [ 0.8409, -1.1302],\n",
            "        [ 1.0117, -1.2596],\n",
            "        [ 0.7046, -0.8431],\n",
            "        [ 0.5522, -0.7282],\n",
            "        [ 1.1332, -1.2850],\n",
            "        [-1.7191,  1.7849],\n",
            "        [-1.4177,  1.5430],\n",
            "        [ 0.7952, -1.1989],\n",
            "        [ 0.5787, -0.8500],\n",
            "        [-1.6130,  1.7217],\n",
            "        [ 0.7652, -1.1030],\n",
            "        [-1.5688,  1.5274],\n",
            "        [ 1.0273, -1.1882],\n",
            "        [ 0.8410, -1.2009],\n",
            "        [-1.1465,  1.0954],\n",
            "        [-1.4763,  1.5432],\n",
            "        [-1.5600,  1.6568],\n",
            "        [-0.0985, -0.3311],\n",
            "        [ 0.7804, -1.0670],\n",
            "        [ 0.9719, -1.2368],\n",
            "        [-1.4840,  1.7424],\n",
            "        [ 0.2417, -0.5800],\n",
            "        [ 1.0222, -1.2434],\n",
            "        [-0.3332,  0.5680]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4349, device='cuda:0'), logits=tensor([[-1.4603,  1.6988],\n",
            "        [-1.2265,  1.2263],\n",
            "        [ 0.2179, -0.4050],\n",
            "        [ 1.0268, -1.0744],\n",
            "        [ 0.9941, -1.2365],\n",
            "        [ 0.8923, -1.2108],\n",
            "        [ 0.9867, -1.2402],\n",
            "        [ 0.9704, -1.2438],\n",
            "        [ 0.7788, -0.9140],\n",
            "        [-1.3547,  1.3423],\n",
            "        [ 1.0455, -1.2845],\n",
            "        [ 1.0845, -1.3006],\n",
            "        [ 0.6431, -0.9291],\n",
            "        [-1.3835,  1.5110],\n",
            "        [-0.0712, -0.0656],\n",
            "        [ 0.8866, -1.1840],\n",
            "        [-0.8023,  0.7581],\n",
            "        [-1.6301,  1.7291],\n",
            "        [-1.6695,  1.7531],\n",
            "        [ 0.3728, -0.4493],\n",
            "        [ 0.4221, -0.7868],\n",
            "        [-1.5717,  1.7257],\n",
            "        [ 0.9418, -1.1899],\n",
            "        [ 1.0882, -1.1632],\n",
            "        [ 1.0109, -1.2081],\n",
            "        [-1.6499,  1.7954],\n",
            "        [ 1.0021, -1.2681],\n",
            "        [ 0.7498, -1.0331],\n",
            "        [-1.7131,  1.7999],\n",
            "        [-0.4167,  0.6196],\n",
            "        [-0.0124,  0.0876],\n",
            "        [-0.2569,  0.4487]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4648, device='cuda:0'), logits=tensor([[ 1.0941, -1.3088],\n",
            "        [ 1.0125, -1.1907],\n",
            "        [-1.4611,  1.3747],\n",
            "        [ 0.9275, -1.1912],\n",
            "        [ 1.0381, -1.2196],\n",
            "        [ 1.0788, -1.1650],\n",
            "        [ 0.8656, -1.1338],\n",
            "        [-0.6326,  0.8102],\n",
            "        [-1.4437,  1.5357],\n",
            "        [ 0.2397, -0.4841],\n",
            "        [ 1.0051, -1.2814],\n",
            "        [ 0.9695, -1.2551],\n",
            "        [ 1.0749, -1.2127],\n",
            "        [-1.6342,  1.7115],\n",
            "        [ 0.1725, -0.4482],\n",
            "        [-1.5498,  1.6505],\n",
            "        [-0.1762, -0.0065],\n",
            "        [ 1.1456, -1.2306],\n",
            "        [ 0.9610, -1.1522],\n",
            "        [ 0.7761, -1.1327],\n",
            "        [-1.2667,  1.4100],\n",
            "        [ 1.0521, -1.2930],\n",
            "        [ 1.1188, -1.2857],\n",
            "        [-1.0749,  0.8863],\n",
            "        [ 0.3051, -0.6692],\n",
            "        [-0.2770,  0.1274],\n",
            "        [ 1.0033, -1.2803],\n",
            "        [-1.6169,  1.7184],\n",
            "        [-1.6642,  1.8221],\n",
            "        [-0.1031, -0.1618],\n",
            "        [ 0.3799, -0.6214],\n",
            "        [-1.4395,  1.6789]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4802, device='cuda:0'), logits=tensor([[-0.5297,  0.5900],\n",
            "        [ 0.9762, -1.1355],\n",
            "        [-1.5917,  1.7974],\n",
            "        [ 0.9644, -1.0533],\n",
            "        [ 0.7261, -1.0246],\n",
            "        [ 0.6331, -0.9191],\n",
            "        [-1.3959,  1.5362],\n",
            "        [ 1.0329, -1.1508],\n",
            "        [ 0.7752, -1.0410],\n",
            "        [ 1.1132, -1.2698],\n",
            "        [ 0.8592, -1.1562],\n",
            "        [-0.7743,  0.6350],\n",
            "        [-1.5711,  1.7179],\n",
            "        [-1.2000,  1.4049],\n",
            "        [-1.5305,  1.6840],\n",
            "        [ 0.9410, -1.0093],\n",
            "        [-1.5468,  1.7006],\n",
            "        [ 0.8577, -1.1272],\n",
            "        [ 0.5473, -0.9531],\n",
            "        [-1.4737,  1.6587],\n",
            "        [ 1.1206, -1.1770],\n",
            "        [ 1.1415, -1.1749],\n",
            "        [-1.5841,  1.5090],\n",
            "        [ 0.9497, -1.1751],\n",
            "        [ 0.4337, -0.7659],\n",
            "        [ 1.0897, -1.0275],\n",
            "        [-1.5947,  1.6679],\n",
            "        [ 0.2995, -0.4630],\n",
            "        [-1.0209,  0.9118],\n",
            "        [-1.5003,  1.7062],\n",
            "        [ 0.1904, -0.2857],\n",
            "        [-1.6112,  1.6541]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3143, device='cuda:0'), logits=tensor([[ 5.8916e-01, -9.5980e-01],\n",
            "        [-8.2715e-01,  1.0013e+00],\n",
            "        [ 1.0530e+00, -1.2616e+00],\n",
            "        [-1.5550e+00,  1.6869e+00],\n",
            "        [-3.3573e-01,  2.7982e-01],\n",
            "        [-1.7196e+00,  1.7963e+00],\n",
            "        [-3.1731e-01,  5.4264e-01],\n",
            "        [ 1.0173e+00, -1.1956e+00],\n",
            "        [-1.7198e+00,  1.7028e+00],\n",
            "        [ 5.4203e-01, -8.3321e-01],\n",
            "        [-2.1252e-01,  8.0051e-02],\n",
            "        [-4.0846e-04, -1.8294e-01],\n",
            "        [-1.4009e+00,  1.5814e+00],\n",
            "        [ 4.3146e-01, -7.3205e-01],\n",
            "        [ 4.0891e-01, -6.9974e-01],\n",
            "        [-1.1407e+00,  1.3346e+00],\n",
            "        [-1.1828e+00,  1.3305e+00],\n",
            "        [ 1.1080e+00, -1.2132e+00],\n",
            "        [ 1.0528e-01, -3.3232e-01],\n",
            "        [ 5.8406e-01, -9.6340e-01],\n",
            "        [ 7.6893e-01, -1.0130e+00],\n",
            "        [ 9.0589e-01, -1.0307e+00],\n",
            "        [ 9.3725e-01, -1.1076e+00],\n",
            "        [ 1.0313e+00, -1.3057e+00],\n",
            "        [-1.5536e+00,  1.7221e+00],\n",
            "        [ 9.6486e-01, -1.1285e+00],\n",
            "        [-9.0886e-01,  1.0033e+00],\n",
            "        [ 8.7145e-01, -1.0791e+00],\n",
            "        [ 4.9390e-01, -2.5601e-01],\n",
            "        [ 1.0347e+00, -1.2269e+00],\n",
            "        [ 1.0901e+00, -1.2587e+00],\n",
            "        [ 9.5993e-01, -1.0564e+00]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5105, device='cuda:0'), logits=tensor([[ 1.0856, -1.2800],\n",
            "        [ 0.9736, -1.2968],\n",
            "        [ 0.9646, -1.1743],\n",
            "        [ 0.1772, -0.5925],\n",
            "        [-0.3514,  0.4926],\n",
            "        [-0.5582,  0.6213],\n",
            "        [ 0.9741, -1.0963],\n",
            "        [ 0.1098, -0.2669],\n",
            "        [-1.5060,  1.6301],\n",
            "        [ 0.9818, -1.2825],\n",
            "        [ 0.6794, -0.9521],\n",
            "        [ 0.9721, -1.1074],\n",
            "        [-0.8910,  0.9765],\n",
            "        [ 0.9626, -1.0978],\n",
            "        [-0.6796,  0.5295],\n",
            "        [ 1.1425, -1.2150],\n",
            "        [ 0.6584, -0.9937],\n",
            "        [ 0.9058, -1.1721],\n",
            "        [ 1.0032, -1.1463],\n",
            "        [ 0.0425, -0.1194],\n",
            "        [ 0.6149, -0.7435],\n",
            "        [ 1.0912, -1.3083],\n",
            "        [ 0.9278, -1.0982],\n",
            "        [ 0.2215,  0.0638],\n",
            "        [ 1.1129, -1.2786],\n",
            "        [-1.2973,  1.1631],\n",
            "        [-1.6269,  1.7296],\n",
            "        [-1.5926,  1.6384],\n",
            "        [ 1.0407, -1.2195],\n",
            "        [ 0.8500, -1.1108],\n",
            "        [ 0.6481, -0.7823],\n",
            "        [ 1.0294, -1.1772]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3253, device='cuda:0'), logits=tensor([[ 0.6722, -0.9381],\n",
            "        [ 0.9187, -1.1739],\n",
            "        [ 1.0807, -1.2542],\n",
            "        [-1.6897,  1.7288],\n",
            "        [ 0.8695, -1.1580],\n",
            "        [-0.4545,  0.2832],\n",
            "        [ 1.0007, -1.1332],\n",
            "        [-1.7339,  1.7300],\n",
            "        [-0.8574,  0.8658],\n",
            "        [-1.0244,  1.1888],\n",
            "        [-0.4360,  0.2698],\n",
            "        [-0.6179,  0.6089],\n",
            "        [ 0.7298, -0.5593],\n",
            "        [ 0.9380, -1.1810],\n",
            "        [ 0.9978, -1.1603],\n",
            "        [-0.0876, -0.4183],\n",
            "        [ 1.0931, -1.2155],\n",
            "        [ 0.7057, -0.9195],\n",
            "        [-0.6605,  0.8898],\n",
            "        [ 0.4918, -0.5181],\n",
            "        [ 0.3608, -0.8450],\n",
            "        [-0.3028,  0.1889],\n",
            "        [ 1.0384, -1.3090],\n",
            "        [-1.5955,  1.6191],\n",
            "        [-1.3784,  1.5479],\n",
            "        [-1.5934,  1.6741],\n",
            "        [ 0.9689, -1.1378],\n",
            "        [ 1.0767, -1.2142],\n",
            "        [-1.5714,  1.5367],\n",
            "        [ 0.7044, -0.9890],\n",
            "        [-0.4171,  0.3541],\n",
            "        [ 1.0827, -1.2437]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6381, device='cuda:0'), logits=tensor([[ 0.7237, -0.8786],\n",
            "        [-1.6144,  1.6908],\n",
            "        [ 0.9815, -1.1001],\n",
            "        [-1.4694,  1.7588],\n",
            "        [ 0.8521, -1.0451],\n",
            "        [ 0.9358, -1.2180],\n",
            "        [ 0.6193, -1.0109],\n",
            "        [ 0.6206, -0.4319],\n",
            "        [ 0.8866, -1.1728],\n",
            "        [-1.0657,  0.9089],\n",
            "        [ 0.6775, -0.9391],\n",
            "        [ 0.5620, -0.8929],\n",
            "        [-1.6837,  1.7526],\n",
            "        [ 1.1945, -1.2527],\n",
            "        [-0.8353,  0.6852],\n",
            "        [ 1.0714, -1.2995],\n",
            "        [ 0.4707, -0.8507],\n",
            "        [-1.1626,  1.2341],\n",
            "        [ 0.9090, -1.2016],\n",
            "        [-0.3175,  0.1046],\n",
            "        [-1.4312,  1.5716],\n",
            "        [ 1.0110, -1.1853],\n",
            "        [-1.6377,  1.6278],\n",
            "        [-1.5757,  1.6156],\n",
            "        [ 0.7355, -0.9822],\n",
            "        [-1.3093,  1.2009],\n",
            "        [-0.2462,  0.3900],\n",
            "        [-1.4040,  1.6025],\n",
            "        [ 0.4850, -0.5243],\n",
            "        [-1.2529,  1.5151],\n",
            "        [ 0.9715, -1.1983],\n",
            "        [ 0.9078, -1.0619]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2137, device='cuda:0'), logits=tensor([[ 0.9874, -1.2052],\n",
            "        [-1.1417,  1.3586],\n",
            "        [ 0.5370, -0.7501],\n",
            "        [-0.7740,  0.6448],\n",
            "        [-1.0877,  1.2209],\n",
            "        [ 0.5839, -0.7266],\n",
            "        [ 0.2511, -0.5540],\n",
            "        [-1.6333,  1.6479],\n",
            "        [ 0.5170, -0.8299],\n",
            "        [ 0.8704, -1.1271],\n",
            "        [ 1.0593, -1.1913],\n",
            "        [ 0.8316, -0.9258],\n",
            "        [ 1.0845, -1.1971],\n",
            "        [ 0.4914, -0.8320],\n",
            "        [-1.5599,  1.6748],\n",
            "        [ 0.8237, -1.0116],\n",
            "        [ 0.1659, -0.5413],\n",
            "        [-1.7059,  1.7670],\n",
            "        [ 0.9814, -1.1936],\n",
            "        [ 0.8702, -1.1876],\n",
            "        [-0.9794,  1.0528],\n",
            "        [ 0.7512, -1.0216],\n",
            "        [ 0.5439, -0.7483],\n",
            "        [-1.4842,  1.6892],\n",
            "        [-1.4256,  1.4906],\n",
            "        [-1.5107,  1.5775],\n",
            "        [ 0.9685, -1.2855],\n",
            "        [ 0.7884, -1.1014],\n",
            "        [ 0.9816, -1.1828],\n",
            "        [-1.4986,  1.7238],\n",
            "        [-1.3030,  1.5705],\n",
            "        [ 0.6254, -1.0493]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3802, device='cuda:0'), logits=tensor([[ 1.1230, -1.3344],\n",
            "        [-0.0389, -0.0293],\n",
            "        [ 0.4160, -0.8049],\n",
            "        [ 0.8028, -1.1055],\n",
            "        [ 0.8527, -1.1865],\n",
            "        [-1.5626,  1.6411],\n",
            "        [-1.6388,  1.8162],\n",
            "        [ 0.6976, -0.9363],\n",
            "        [-1.5589,  1.5848],\n",
            "        [-0.5926,  0.7245],\n",
            "        [-1.6308,  1.7306],\n",
            "        [ 0.9224, -1.2431],\n",
            "        [-1.4971,  1.4971],\n",
            "        [ 0.6278, -0.8206],\n",
            "        [ 0.4366, -0.6042],\n",
            "        [ 1.1088, -1.2361],\n",
            "        [ 0.6085, -0.9016],\n",
            "        [-1.6477,  1.7122],\n",
            "        [-1.5397,  1.6533],\n",
            "        [ 0.1347, -0.2159],\n",
            "        [-1.3279,  1.3985],\n",
            "        [ 0.6745, -0.9709],\n",
            "        [-1.4402,  1.7069],\n",
            "        [ 0.8763, -1.1369],\n",
            "        [-1.3178,  1.5371],\n",
            "        [ 0.7237, -0.8391],\n",
            "        [-0.6479,  0.5981],\n",
            "        [ 1.0181, -1.2332],\n",
            "        [ 0.9177, -1.1980],\n",
            "        [ 0.7360, -0.9981],\n",
            "        [ 0.3480, -0.7007],\n",
            "        [ 0.9131, -1.0530]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3716, device='cuda:0'), logits=tensor([[ 1.1055, -1.2985],\n",
            "        [-1.3587,  1.4157],\n",
            "        [ 0.5629, -0.8614],\n",
            "        [ 1.0624, -1.2098],\n",
            "        [ 0.9970, -1.1199],\n",
            "        [ 0.7143, -0.9909],\n",
            "        [ 0.5561, -0.8887],\n",
            "        [-1.6654,  1.7076],\n",
            "        [-1.5266,  1.7203],\n",
            "        [-1.5474,  1.7227],\n",
            "        [-1.6481,  1.6999],\n",
            "        [-0.3974,  0.2970],\n",
            "        [-1.4838,  1.5868],\n",
            "        [-1.1997,  1.3755],\n",
            "        [ 1.0943, -1.2288],\n",
            "        [-1.6627,  1.6559],\n",
            "        [ 0.9664, -1.1133],\n",
            "        [-0.1347,  0.3227],\n",
            "        [ 0.7141, -0.9978],\n",
            "        [-0.1703, -0.1884],\n",
            "        [ 0.3372, -0.4914],\n",
            "        [-1.0704,  1.2762],\n",
            "        [ 0.5677, -0.9328],\n",
            "        [-0.6185,  0.6477],\n",
            "        [ 0.5154, -0.8719],\n",
            "        [-0.2629,  0.3174],\n",
            "        [ 0.9893, -1.0729],\n",
            "        [ 0.7090, -0.9400],\n",
            "        [-1.6883,  1.7918],\n",
            "        [-0.5876,  0.6124],\n",
            "        [-1.5816,  1.7904],\n",
            "        [ 0.9244, -1.2222]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2460, device='cuda:0'), logits=tensor([[ 0.6609, -0.8506],\n",
            "        [ 0.2638, -0.5744],\n",
            "        [-1.5011,  1.7687],\n",
            "        [-1.6603,  1.7281],\n",
            "        [ 0.9957, -1.1134],\n",
            "        [-1.4931,  1.5443],\n",
            "        [ 0.9936, -1.2640],\n",
            "        [ 1.0630, -1.2497],\n",
            "        [-1.5739,  1.7501],\n",
            "        [-0.7903,  0.8272],\n",
            "        [ 1.1101, -1.2993],\n",
            "        [-0.0278, -0.2550],\n",
            "        [-1.6802,  1.7211],\n",
            "        [ 0.4730, -0.7719],\n",
            "        [ 0.9123, -1.1583],\n",
            "        [ 0.9623, -1.1304],\n",
            "        [-1.6725,  1.7768],\n",
            "        [ 0.4444, -0.7380],\n",
            "        [ 0.7237, -1.0169],\n",
            "        [-1.5590,  1.7361],\n",
            "        [-1.5748,  1.7868],\n",
            "        [-1.5974,  1.7064],\n",
            "        [ 0.8305, -1.1965],\n",
            "        [ 0.8845, -1.2042],\n",
            "        [ 1.0119, -1.0449],\n",
            "        [ 0.4690, -0.8084],\n",
            "        [-1.6212,  1.5659],\n",
            "        [-1.5780,  1.6855],\n",
            "        [ 1.0146, -1.2097],\n",
            "        [-1.2944,  1.4217],\n",
            "        [-1.6979,  1.7971],\n",
            "        [ 0.1336, -0.4569]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.6420, device='cuda:0'), logits=tensor([[ 1.0408, -1.2686],\n",
            "        [-0.0064, -0.2900],\n",
            "        [ 0.7320, -0.9557],\n",
            "        [-1.4995,  1.6349],\n",
            "        [ 0.7236, -0.6971],\n",
            "        [ 0.4938, -0.7370],\n",
            "        [ 1.0906, -1.1559],\n",
            "        [ 0.6016, -0.7517],\n",
            "        [-1.5720,  1.6779],\n",
            "        [ 0.9619, -1.0340],\n",
            "        [-0.3492,  0.4321],\n",
            "        [-1.5074,  1.5989],\n",
            "        [ 0.8794, -1.2225],\n",
            "        [ 1.0364, -1.2324],\n",
            "        [ 0.3627, -0.7542],\n",
            "        [-1.6301,  1.7241],\n",
            "        [-1.0037,  1.0244],\n",
            "        [ 0.7523, -0.7323],\n",
            "        [ 0.6961, -1.0167],\n",
            "        [-0.6906,  0.8968],\n",
            "        [ 0.3665, -0.7223],\n",
            "        [ 0.4697, -0.8793],\n",
            "        [-1.4525,  1.5768],\n",
            "        [-1.2461,  1.3914],\n",
            "        [ 1.1235, -1.2021],\n",
            "        [ 0.6348, -0.9095],\n",
            "        [ 0.7551, -0.9707],\n",
            "        [ 1.0163, -1.3279],\n",
            "        [-1.4940,  1.5731],\n",
            "        [-0.9422,  1.0278],\n",
            "        [ 0.4195, -0.6794],\n",
            "        [ 0.3786, -0.7592]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4514, device='cuda:0'), logits=tensor([[ 0.8728, -1.0215],\n",
            "        [-1.2845,  1.5839],\n",
            "        [-0.3825,  0.2011],\n",
            "        [ 0.8193, -1.0067],\n",
            "        [ 0.9426, -1.2636],\n",
            "        [ 1.0881, -1.3258],\n",
            "        [ 0.9664, -0.9579],\n",
            "        [ 0.8235, -0.9026],\n",
            "        [-1.0089,  1.0862],\n",
            "        [ 0.3594, -0.5079],\n",
            "        [ 0.9612, -1.1333],\n",
            "        [-1.3606,  1.4564],\n",
            "        [ 0.8669, -1.1332],\n",
            "        [ 0.4319, -0.7122],\n",
            "        [-1.5821,  1.6815],\n",
            "        [ 0.8978, -0.8396],\n",
            "        [ 0.3697, -0.2677],\n",
            "        [ 0.5278, -0.8353],\n",
            "        [-1.1384,  1.1562],\n",
            "        [ 0.9089, -1.1959],\n",
            "        [-1.5619,  1.7229],\n",
            "        [-0.4168,  0.2498],\n",
            "        [-1.1989,  1.3476],\n",
            "        [ 0.7854, -0.9619],\n",
            "        [ 1.0460, -1.2552],\n",
            "        [ 0.8820, -1.1307],\n",
            "        [-0.0594, -0.1662],\n",
            "        [-1.5008,  1.6860],\n",
            "        [-0.0232,  0.1195],\n",
            "        [ 0.4334, -0.6569],\n",
            "        [ 0.6862, -0.9580],\n",
            "        [ 1.0438, -1.1985]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3983, device='cuda:0'), logits=tensor([[-0.0877, -0.0755],\n",
            "        [-1.5451,  1.6957],\n",
            "        [-1.3584,  1.4167],\n",
            "        [ 0.9321, -1.0126],\n",
            "        [ 0.9547, -1.1334],\n",
            "        [ 0.5539, -0.9573],\n",
            "        [ 0.6238, -0.8997],\n",
            "        [-1.5822,  1.7250],\n",
            "        [ 0.7755, -0.9174],\n",
            "        [ 0.6172, -0.8421],\n",
            "        [ 1.0279, -1.2091],\n",
            "        [ 0.8500, -0.9778],\n",
            "        [ 0.8151, -0.8664],\n",
            "        [-0.3029, -0.0752],\n",
            "        [ 0.3594, -0.6818],\n",
            "        [-0.6266,  0.8229],\n",
            "        [-1.6087,  1.7632],\n",
            "        [ 0.7751, -1.0595],\n",
            "        [-0.1357, -0.0601],\n",
            "        [ 0.9707, -1.1656],\n",
            "        [ 0.2611, -0.5940],\n",
            "        [-1.4783,  1.6253],\n",
            "        [ 0.9167, -1.2101],\n",
            "        [ 0.5588, -0.9136],\n",
            "        [ 1.0672, -1.2520],\n",
            "        [-0.3695,  0.2744],\n",
            "        [-0.5600,  0.4498],\n",
            "        [ 0.8878, -1.2048],\n",
            "        [-1.2594,  1.1069],\n",
            "        [ 1.0426, -1.2516],\n",
            "        [-1.6895,  1.7638],\n",
            "        [ 0.9053, -1.0080]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5430, device='cuda:0'), logits=tensor([[ 1.0351, -1.3110],\n",
            "        [ 0.4421, -0.6799],\n",
            "        [ 1.0820, -1.3135],\n",
            "        [-1.5711,  1.7894],\n",
            "        [ 0.7933, -1.1554],\n",
            "        [ 0.5681, -0.9615],\n",
            "        [ 0.6895, -0.9501],\n",
            "        [ 1.0032, -1.1781],\n",
            "        [ 0.3998, -0.3207],\n",
            "        [-1.6698,  1.7759],\n",
            "        [ 1.0637, -1.2053],\n",
            "        [ 0.8334, -1.1714],\n",
            "        [-0.9325,  1.1321],\n",
            "        [ 0.6056, -0.9613],\n",
            "        [-1.3243,  1.4165],\n",
            "        [-1.5862,  1.7068],\n",
            "        [-1.5140,  1.5168],\n",
            "        [-1.4915,  1.6535],\n",
            "        [-1.5687,  1.6861],\n",
            "        [-0.8979,  0.9959],\n",
            "        [-1.5749,  1.5945],\n",
            "        [ 0.8771, -1.1205],\n",
            "        [ 0.8646, -1.1836],\n",
            "        [ 1.0484, -1.1833],\n",
            "        [ 0.7572, -1.0092],\n",
            "        [ 0.9344, -1.1300],\n",
            "        [ 0.4085, -0.6417],\n",
            "        [ 0.8754, -1.1093],\n",
            "        [-1.2374,  1.3000],\n",
            "        [ 0.6581, -0.9960],\n",
            "        [ 0.9534, -1.1795],\n",
            "        [ 0.6513, -0.9509]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3341, device='cuda:0'), logits=tensor([[-1.3643,  1.4580],\n",
            "        [-1.2610,  1.2861],\n",
            "        [-0.0276,  0.0971],\n",
            "        [-1.6268,  1.6846],\n",
            "        [ 0.5630, -0.8795],\n",
            "        [ 0.4885, -0.7261],\n",
            "        [ 0.3116, -0.4481],\n",
            "        [ 1.0583, -1.1409],\n",
            "        [ 0.4153, -0.6974],\n",
            "        [-1.4032,  1.2919],\n",
            "        [ 0.5103, -0.8443],\n",
            "        [ 1.1355, -1.2882],\n",
            "        [-1.5886,  1.7595],\n",
            "        [ 1.1039, -1.2172],\n",
            "        [-1.5825,  1.6212],\n",
            "        [-1.2310,  1.2357],\n",
            "        [ 0.8338, -1.1393],\n",
            "        [-1.4284,  1.6052],\n",
            "        [ 0.7847, -1.1176],\n",
            "        [ 0.7722, -1.0936],\n",
            "        [ 0.4274, -0.4387],\n",
            "        [ 0.9988, -1.2179],\n",
            "        [-0.3088,  0.2891],\n",
            "        [-0.2712,  0.3075],\n",
            "        [ 0.0158, -0.1311],\n",
            "        [-1.0365,  1.1489],\n",
            "        [-1.6314,  1.7098],\n",
            "        [-1.6021,  1.7108],\n",
            "        [ 1.0229, -1.2269],\n",
            "        [ 0.5858, -0.7973],\n",
            "        [-1.5103,  1.5604],\n",
            "        [-1.5097,  1.6051]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.3316, device='cuda:0'), logits=tensor([[ 1.1334, -1.3380],\n",
            "        [ 1.0757, -1.2999],\n",
            "        [ 0.2426, -0.2935],\n",
            "        [ 1.0677, -1.2886],\n",
            "        [-1.0024,  1.0936],\n",
            "        [ 0.9844, -1.2145],\n",
            "        [ 1.0284, -1.2029],\n",
            "        [ 0.1454, -0.4026],\n",
            "        [-1.5013,  1.7769],\n",
            "        [ 0.6613, -0.8724],\n",
            "        [ 0.5390, -0.8102],\n",
            "        [ 0.5926, -0.7017],\n",
            "        [-1.3671,  1.5767],\n",
            "        [ 0.8239, -1.1007],\n",
            "        [-1.3693,  1.4236],\n",
            "        [-1.6632,  1.7882],\n",
            "        [ 0.6776, -0.7790],\n",
            "        [ 1.0072, -1.1412],\n",
            "        [ 0.9291, -1.0870],\n",
            "        [ 0.9825, -1.2435],\n",
            "        [ 0.7192, -1.0827],\n",
            "        [-0.3088,  0.1560],\n",
            "        [ 0.8999, -1.1919],\n",
            "        [-0.1183, -0.0798],\n",
            "        [-0.8758,  0.9106],\n",
            "        [ 0.6195, -0.9061],\n",
            "        [-0.3277,  0.4998],\n",
            "        [-1.2293,  1.4567],\n",
            "        [ 1.0328, -1.3114],\n",
            "        [-1.6453,  1.7277],\n",
            "        [ 1.0353, -1.2446],\n",
            "        [ 0.8915, -1.1519]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5264, device='cuda:0'), logits=tensor([[-1.5869,  1.7387],\n",
            "        [ 0.8459, -0.9813],\n",
            "        [ 0.8580, -1.0348],\n",
            "        [-1.7047,  1.7335],\n",
            "        [ 0.4866, -0.9359],\n",
            "        [ 1.0181, -1.0675],\n",
            "        [ 0.8142, -0.9529],\n",
            "        [ 0.6834, -0.8658],\n",
            "        [ 0.5960, -0.9172],\n",
            "        [ 1.0824, -1.2990],\n",
            "        [-0.3466,  0.1776],\n",
            "        [ 0.3815, -0.6492],\n",
            "        [ 0.7510, -0.6716],\n",
            "        [ 0.6674, -0.9268],\n",
            "        [ 0.7097, -0.9464],\n",
            "        [ 0.3164, -0.6124],\n",
            "        [ 0.3320, -0.5902],\n",
            "        [-0.0873, -0.0349],\n",
            "        [-1.5365,  1.6802],\n",
            "        [-1.5418,  1.6047],\n",
            "        [ 0.7276, -0.9798],\n",
            "        [ 0.5409, -0.7856],\n",
            "        [ 0.8324, -1.0452],\n",
            "        [-0.1890,  0.2702],\n",
            "        [-0.1595, -0.0918],\n",
            "        [-0.9360,  0.8884],\n",
            "        [ 1.0045, -1.2332],\n",
            "        [ 0.8220, -1.0344],\n",
            "        [-1.1532,  1.2945],\n",
            "        [-0.6981,  0.5607],\n",
            "        [-1.4171,  1.4152],\n",
            "        [-1.5960,  1.7808]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.2696, device='cuda:0'), logits=tensor([[-1.5660,  1.7245],\n",
            "        [ 1.0854, -1.2839],\n",
            "        [ 0.6283, -0.7485],\n",
            "        [ 1.1266, -1.2497],\n",
            "        [ 0.9692, -1.2361],\n",
            "        [-1.3066,  1.5636],\n",
            "        [ 1.0664, -1.2338],\n",
            "        [ 0.9934, -1.3209],\n",
            "        [-1.4761,  1.5185],\n",
            "        [-0.7180,  0.6231],\n",
            "        [ 0.7303, -0.8535],\n",
            "        [ 0.7827, -1.0867],\n",
            "        [-1.3212,  1.4642],\n",
            "        [ 0.8761, -1.1641],\n",
            "        [ 0.9419, -0.9629],\n",
            "        [-1.3762,  1.4329],\n",
            "        [ 1.1174, -1.3112],\n",
            "        [ 0.3656, -0.6075],\n",
            "        [ 0.9911, -1.1346],\n",
            "        [-0.3092,  0.2612],\n",
            "        [-1.4761,  1.5185],\n",
            "        [ 0.0816, -0.0648],\n",
            "        [ 1.0143, -1.2756],\n",
            "        [-1.5503,  1.5223],\n",
            "        [-1.2341,  1.3906],\n",
            "        [-1.3729,  1.3046],\n",
            "        [-1.5130,  1.4645],\n",
            "        [-1.3223,  1.4768],\n",
            "        [ 0.7282, -0.9318],\n",
            "        [ 1.0554, -1.2815],\n",
            "        [-1.5400,  1.5841],\n",
            "        [ 0.8231, -0.8329]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.4706, device='cuda:0'), logits=tensor([[-0.3018,  0.4527],\n",
            "        [ 1.0305, -1.3385],\n",
            "        [-1.6338,  1.7559],\n",
            "        [ 0.8596, -1.1628],\n",
            "        [ 0.5075, -0.9535],\n",
            "        [ 1.1070, -1.1995],\n",
            "        [-1.5651,  1.7118],\n",
            "        [ 0.8190, -1.1622],\n",
            "        [-0.6753,  0.6628],\n",
            "        [-0.8296,  0.7494],\n",
            "        [ 0.8521, -0.8767],\n",
            "        [ 0.5233, -0.7561],\n",
            "        [ 0.7081, -1.0159],\n",
            "        [-0.0151, -0.1380],\n",
            "        [ 0.8261, -1.1952],\n",
            "        [ 0.8481, -1.0793],\n",
            "        [ 0.8041, -1.0216],\n",
            "        [-1.1248,  1.1708],\n",
            "        [ 1.0348, -1.2006],\n",
            "        [ 0.7296, -0.6776],\n",
            "        [ 1.0352, -1.2144],\n",
            "        [ 0.9222, -1.2141],\n",
            "        [ 0.9550, -1.1157],\n",
            "        [ 0.0198, -0.1136],\n",
            "        [ 1.0519, -1.2744],\n",
            "        [ 0.8099, -1.1196],\n",
            "        [-0.5355,  0.4634],\n",
            "        [ 0.4519, -0.7114],\n",
            "        [ 0.3696, -0.6763],\n",
            "        [ 1.1278, -1.1861],\n",
            "        [ 0.9184, -1.1868],\n",
            "        [ 0.3251, -0.6677]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5500, device='cuda:0'), logits=tensor([[ 0.0398, -0.2266],\n",
            "        [-1.5494,  1.7961],\n",
            "        [-1.4638,  1.5206],\n",
            "        [-0.8853,  1.1186],\n",
            "        [ 0.4839, -0.9162],\n",
            "        [-1.0189,  1.0717],\n",
            "        [ 0.2560, -0.5955],\n",
            "        [ 0.5911, -0.9447],\n",
            "        [ 0.7304, -0.8900],\n",
            "        [-0.2879,  0.4118],\n",
            "        [-1.5473,  1.7421],\n",
            "        [ 0.8876, -1.0403],\n",
            "        [ 0.7041, -0.8117],\n",
            "        [ 1.0956, -1.2277],\n",
            "        [-1.0958,  1.1699],\n",
            "        [ 0.9462, -1.1592],\n",
            "        [ 0.9934, -1.2549],\n",
            "        [ 0.6346, -0.9359],\n",
            "        [ 0.9940, -1.1626],\n",
            "        [ 0.9209, -1.1690],\n",
            "        [ 1.0413, -1.2501],\n",
            "        [-1.6391,  1.7879],\n",
            "        [-0.2669,  0.1472],\n",
            "        [-1.6844,  1.7245],\n",
            "        [-1.6635,  1.7243],\n",
            "        [ 1.0366, -1.0920],\n",
            "        [ 0.8681, -1.1403],\n",
            "        [-0.7220,  0.4745],\n",
            "        [ 0.4673, -0.8413],\n",
            "        [ 0.8761, -1.2089],\n",
            "        [ 0.7252, -1.0196],\n",
            "        [ 1.0222, -1.1893]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Output: SequenceClassifierOutput(loss=tensor(0.5272, device='cuda:0'), logits=tensor([[-0.1067, -0.1057],\n",
            "        [ 0.5422, -0.6612],\n",
            "        [ 0.9748, -0.9550],\n",
            "        [ 0.4850, -0.7603],\n",
            "        [ 0.9371, -0.9456],\n",
            "        [ 1.0912, -1.1192],\n",
            "        [ 0.9398, -1.1832],\n",
            "        [ 0.9474, -1.2527],\n",
            "        [ 0.1150, -0.4930],\n",
            "        [ 1.0695, -1.1875],\n",
            "        [ 0.8308, -1.0714],\n",
            "        [-1.5836,  1.7667],\n",
            "        [ 0.5474, -0.6405],\n",
            "        [ 0.8314, -0.9952],\n",
            "        [ 1.0684, -1.2961],\n",
            "        [ 0.2469, -0.2951],\n",
            "        [-0.6981,  0.5607],\n",
            "        [ 0.8647, -1.0161],\n",
            "        [ 1.0217, -1.2382]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "  Accuracy: 0.83\n",
            "  F1: 0.78\n",
            "  Validation Loss: 0.42\n",
            "  Validation took: 0:00:20\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:14:12 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "    print('')\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes:\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
        "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
        "        b_labels = batch[2].to(device).to(torch.int64)\n",
        "\n",
        "        # Clear previously calculated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass\n",
        "        outputs = model(b_input_ids,\n",
        "                         token_type_ids=None,\n",
        "                         attention_mask=b_input_mask,\n",
        "                         labels=b_labels)\n",
        "\n",
        "        # Check the output structure\n",
        "        print(\"Output:\", outputs)  # Debugging line\n",
        "\n",
        "        # Extract the loss from the output (it's stored as `loss` in `SequenceClassifierOutput`)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the training loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to help prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print('')\n",
        "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
        "    print('  Training epoch took: {:}'.format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on our validation set.\n",
        "    print('')\n",
        "    print('Running Validation...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    total_eval_f1 = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # No need to compute gradients\n",
        "        with torch.no_grad():\n",
        "             outputs = model(b_input_ids,\n",
        "                              token_type_ids=None,\n",
        "                              attention_mask=b_input_mask,\n",
        "                              labels=b_labels)\n",
        "\n",
        "             # Check the output structure\n",
        "             print(\"Validation Output:\", outputs)  # Debugging line\n",
        "\n",
        "             # Extract the loss and logits from the output\n",
        "             loss = outputs.loss\n",
        "             logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        total_eval_f1 += flat_f1(logits, label_ids)\n",
        "\n",
        "    # Report the final accuracy for this validation run\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n",
        "\n",
        "    # Report the final f1 score for this validation run\n",
        "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
        "    print('  F1: {0:.2f}'.format(avg_val_f1))\n",
        "\n",
        "    # Calculate the average loss over all of the batches\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n",
        "    print('  Validation took: {:}'.format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Val_F1' : avg_val_f1,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print('')\n",
        "print('Training complete!')\n",
        "print('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU10XeVz6r_V",
        "outputId": "85620a0b-c516-401b-e699-f4979b5440ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Val_F1</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0:04:28</td>\n",
              "      <td>0:00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.39</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:04:22</td>\n",
              "      <td>0:00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0:04:22</td>\n",
              "      <td>0:00:20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur.  Val_F1 Training Time  \\\n",
              "epoch                                                                    \n",
              "1               0.50         0.44           0.82    0.78       0:04:28   \n",
              "2               0.39         0.42           0.82    0.77       0:04:22   \n",
              "3               0.36         0.42           0.83    0.78       0:04:22   \n",
              "\n",
              "      Validation Time  \n",
              "epoch                  \n",
              "1             0:00:20  \n",
              "2             0:00:20  \n",
              "3             0:00:20  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set the display precision for floating-point numbers\n",
        "pd.set_option('display.precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "display(df_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vybyY0tk6r_Y",
        "outputId": "29278bd4-e8e1-4253-8808-d756056658b5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Increase the plot size and font size:\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12,8))\n",
        "\n",
        "# Plot the learning curve:\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label='Training')\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label='Validation')\n",
        "\n",
        "# Label the plot:\n",
        "\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3])\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}